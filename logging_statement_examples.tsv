logging_code_labels
"<line3>        LOG.debug(""Initializing XRay tracer"");"
"<line5>      LOGGER.warn(""Unable to transform geometry"", e);"
"<line1>    log.debug(""client [container: {}] disconnected"", con.getRemoteContainer());"
"<line2>    log.info(""get ["" + keys + ""]"");"
"<line6>        LOGGER.debug(""Configuration {} detected and added"", location);<line11>        LOGGER.warn(""Configuration {} not readable; ignored"", location);"
"<line9>        log.warn(""Missing />"");"
"<line2>    LOG.info(""Memory Difference is: "" + differenceMemoryCache);"
"<line16>        LOG.info(""Received from Zeppelin LoginRestApi : "" + content);<line18>        LOG.error(""Failed Zeppelin login {}, status code {}"", endpoint, code);<line21>      LOG.error(""Cannot login into Zeppelin"", e);"
"<line4>        log.trace(""seenModules.size: "" + seenModules.size());<line5>      log.trace(""name: "" + name);<line6>        log.trace(""url: "" + url);<line16>      log.debug(""Resource is not visible"");"
"<line9>      log.info(""Second holdcc() threw IllegalStateException as expected."");"
"<line7>            log.debug(""Exception in closing "" + c, e);"
<line26>      log.error(systemException, systemException);
"<line13>      logger.warn("" Exception while processing: ""+ belongsToClass+ "" Line: ""+ typeType.start.getLine()+ "" ""+ e.getMessage());"
<line1>    logger.info(msg);
"<line5>      logger.error(""Failed to create ssl tofu rest template"", e);"
"<line13>      LOG.debug(""MIME types loaded: {}"", mimeTypesMap);<line17>      LOG.error(""Error reading file MIMETypes from resources"", e);"
"<line19>      logger.debug(""Since creation of parent template: ""+ templateInfo.getId()+ "" failed, delete Datadisk templates that were created as part of parent""+ "" template download"");"
<line21>      log.error(systemException, systemException);
"<line3>      LOGGER.info(""===== Executing workflow "" + config.inputWorkflow + "" ====="");<line5>        LOGGER.info(""========= Workflow did not execute sucessfully ============"");<line10>        LOGGER.info(""============= Workflow executed sucessfully ==============="");"
"<line35>      logger.trace(""Optimized query model:\n{}"", tupleExpr);"
"<line2>    log.trace(""#{} queue {} completed"", session.uniqueId(), var);"
"<line10>    log.info(""Finished request: ""+ tm+ ""ms  for ""+ request.getAbsolutePath()+ "" method=""+ request.getMethod());"
"<line7>    LOGGER.info(""doStopSelfTest() for device: {}."", deviceRequest.getDeviceIdentification());"
"<line3>      LOGGER.debug(format(""Generating schema for udt of type %s"",udtClassProperty.udtClass.getCanonicalName()));<line12>      ACHILLES_DMLlogger.debug(udtSchema + ""\n"");"
<line12>        log.debug(sb.toString());
"<line3>      LOG.info(""no message log replay snapshot, return empty state."");"
<line8>          log.error(e.getMessage(), e);
"<line5>    LOGGER.debug(""MessageLength: "" + message.getMessageLength().getValue());"
"<line1>    logger.info(""~~~~ Initial State ~~~~~"");<line6>    logger.info(_currentMapping.toString());<line8>      logger.info(""~~~~ Iteration "" + i + "" ~~~~~"");<line12>        logger.info(mapResult.toString());<line12>        logger.info(listResult.toString());"
"<line2>    logger.debug(""Find all assets for repositoryId: {}, path: {}, deleted: {}, virtual: {}, branchId: {}"",repositoryId,path,deleted,virtual,branchId);"
"<line16>          log.debug(remoteSessionObject.toString());<line17>          log.debug(""jsonString {}"", xmlString);"
"<line5>        log.info(""Before acquiring and wait lock '{}'"", lock);<line7>        log.info(""Lock-wait acquired to object '{}'"", lock);"
<line16>      LOGGER.error(e);
"<line6>      log.info(""exceotuib reseting stats"", e);"
"<line2>    LOGGER.debug(""Extensions: "" + ArrayConverter.bytesToHexString(pair.getExtensions().getValue()));"
"<line24>      logger.error(""Exception occurred in SDNCDeactivateTasks deactivateVfModule"", ex);"
"<line2>      LOG.debug(""==> reorderEvaluators()"");<line13>      LOG.debug(""<== reorderEvaluators()"");"
"<line13>      log.error(""Problem de-serializing object."", e);"
"<line1>    LOG.debug(""thread {}: connection stop"", Thread.currentThread().getId());"
"<line2>    logger.info(""Testing DB2 schema creation"");"
<line2>    logger.debug(Messages.STARTING_APPLICATION_0, applicationName);
"<line3>    LOG.debug(""Calling OpenstackNeutronNetworkResource.getAllShouldSucceed()"");"
"<line12>        LOGGER.info(""Encountered a subsequent failure. Giving up."");<line14>        LOGGER.info(""User requested restart."");<line19>          LOGGER.info(""Unexpected exit. Restarting..."");<line20>          LOGGER.info(""Exiting with status "" + code);"
"<line8>      logger.error(""Decode RQNT Response failed"", e);"
"<line2>    LOGGER.info(""Kinited user: "" + user + "" keytab: "" + KEYTAB_LOCATION + ""/"" + user + "".keytab"");"
"<line2>    logger.info(name.getMethodName() + "" - callback - error"");"
"<line2>    logger.debug(""Jumping from old track position {} ms to new position {} ms"",currentPosition,currentPosition + positionOffsetInMs);"
"<line15>          LOGGER.error("""", e);"
"<line11>      log.error(""Error while trying to load registry config for C-App : ""+ parentAppName+ "" artifact : ""+ artifactName,e);"
"<line5>      logger.debug(""Ping command port, "" + privateIp + "":"" + cmdPort);<line9>        logger.error(""Can not ping System vm "" + vmName + ""due to:"" + result);<line12>      logger.error(""Can not ping System vm "" + vmName + ""due to exception"");<line15>      logger.debug(""Ping command port succeeded for vm "" + vmName);<line18>        logger.debug(""Execute network usage setup command on "" + vmName);"
"<line8>      log.warn(""Inknown object invoked in surf filter ui"");"
"<line4>      LOG.error(""closing piped input stream failed"", e);<line8>      LOG.error(""cancelling write task failed"", e);"
"<line25>              LOG.warn(""Too busy to snap, skipping"");<line31>                    LOG.warn(""Unexpected exception"", e);<line58>    LOG.info(""SyncRequestProcessor exited!"");"
"<line3>      LOG.error(I18n.err(I18n.ERR_01308_ZERO_LENGTH_TLV));<line8>      LOG.debug(""Authenticator created"");"
"<line5>      logger.debug(""Handling virtual datapoint '{}' on gateway with id '{}'"", dp.getName(), id);<line8>        logger.debug(""Executing script '{}' on gateway with id '{}'"", dp.getInfo(), id);<line11>      logger.debug(""Sending variable '{}' with value '{}' to gateway with id '{}'"",dp.getInfo(),newValue,id);<line13>      logger.debug(""Sending datapoint '{}' with value '{}' to gateway with id '{}' using rxMode '{}'"",dpInfo,newValue,id,rxMode == null ? ""DEFAULT"" : rxMode);"
"<line19>      logger.debug(""got "" + messages.size() + "" message(s): "" + messages);"
<line49>      log.error(e.getMessage(), e);<line50>      log.info(Color.MAGENTA + monitor.stop() + Color.NORMAL);
"<line6>          logger.debug(""Couldn't resolve ""+ an.desc+ "" annotation type whilst searching for hints on ""+ getName());"
"<line1>    logger.info(""Before Starting application"");<line2>    logger.debug(""Starting my application in debug with {} arguments"", args.length);<line2>    logger.info(""Starting my application with {} arguments."", args.length);"
"<line10>    log.info(""Starting Spark server, ignoring port and host"");"
<line7>      log.error(exception, exception);
"<line2>    LOGGER.trace(""ENTERING: Before filter"");<line11>            LOGGER.debug(""Unable to extract date from Before filter {}. Ignoring filter."", literal);<line14>    LOGGER.trace(""EXITING: Before filter"");"
"<line34>          LOG.info(""Found fresh table, ignoring: "" + tableName);<line43>                      LOG.info(""Test-setup deleting table: %s"", tableName.getNameAsString());"
"<line6>      LOG.error(ex.getLocalizedMessage());<line17>      LOG.info(""Zibase gets relay status from file "" + statusFileURL);<line31>      LOG.error(Freedomotic.getStackTraceInfo(ex));"
"<line3>    log.debug(this.getName() + "" flushed."");"
"<line5>    LOGGER.debug(""Merging tables"");"
"<line10>          LOG.error("""", e);"
"<line4>      logger.info(""Apache NiFi is not currently running"");<line16>      logger.debug(""Connecting to NiFi instance"");<line18>      logger.debug(""Established connection to NiFi instance."");<line19>      logger.debug(""Sending DECOMMISSION Command to port {}"", port);<line25>        logger.debug(""Received response to DECOMMISSION command: {}"", response);<line30>        logger.error(""When sending DECOMMISSION command to NiFi, got unexpected response {}"", response);<line34>        logger.error(""Failed to delete lock file {}; this file should be cleaned up manually"", lockFile);"
"<line8>    LOGGER.debug(""Reconstruct consumer code :""+ bill.getConsumerId()+ "", with bill reference number: ""+ billReferenceNumber+ "", for Amount Paid :""+ actualAmountPaid);"
"<line11>      LOG.info(""Can't create admin log and detail for logo replacement."", e);"
"<line5>        LOGGER.error(""[loadConfig][{}] Exception occurred, cause: {}"", sourceName, throwable.toString());<line6>        LOGGER.debug(""[loadConfig][{}] Loaded config properties"", sourceName);"
<line5>      logger.error(e, e);
<line12>              LOGGER.error(t.toString(), t);<line16>                LOGGER.error(e.toString(), e);
"<line2>    log.debug(""exists: "" + exists);"
"<line1>    logger.warn(""Skipping a URL: {} which was bigger ( {} ) than max allowed size"", urlStr, pageSize);"
"<line3>        logger.info(""Override expected test files (instead of checking)"");<line16>      logger.debug(msg, e);"
"<line12>            logger.info(context,""ShadowUserProcessor:getSyncCallback:SUCCESS:SYNC CALLBACK SUCCESSFULLY PROCESSED""+ "" for Shadow user: ""+ singleShadowUser.toString());<line13>          logger.info(context,""ShadowUserProcessor:getSyncCallback:SUCCESS:SYNC CALLBACK SUCCESSFULLY MIGRATED  ALL""+ "" Shadow user"");<line14>          logger.error(context,""ShadowUserProcessor:getSyncCallback:SUCCESS:ERROR OCCURRED WHILE GETTING SYNC""+ "" CALLBACKS"",e);<line19>        logger.error(""ShadowUserProcessor:getSyncCallback:FAILURE:ERROR OCCURRED WHILE GETTING SYNC""+ "" CALLBACKS"",t);"
"<line4>    logger.debug(""Transaction started for window {}"", windowId);"
<line9>          log.error(e, e);
"<line5>    log.debug(""getGroups query for "" + entityId);"
"<line2>    LOG.debug(""Deregistering stream consumer - {}"", stream);<line11>    LOG.debug(""Deregistered stream consumer - {}"", streamConsumerArn);"
"<line13>      logger.error("""", e);"
"<line2>    LOG.debug(""begin loadExpirationConfiguration()"");<line3>    LOG.debug(""Property Directory: "" + propertyDir);"
"<line4>    logger.debug(""Registering queue on renderer {}"", thing.getLabel());<line12>        logger.trace(""Renderer {} still playing, set new queue as next entry"", thing.getLabel());"
<line24>      LOGGER.debug(e.getMessage(), e);
<line5>    log.info(message);
"<line1>    LOGGER.debug(action + "" feature list("" + featureList.size() + ""), into Feature table: "" + serviceUrl);<line15>    LOGGER.debug(""Response code: "" + response.getResponseCode() + ""\n\t"" + response.getBody());"
"<line11>        LOG.error(""Unknown column "" + columnIndex);"
"<line3>      logger.debug(""Successfully connected to "" + context.getDescription());"
"<line2>    LOG.info(""doing async send..."");<line5>      LOG.info(""got send exception: "", e);<line7>    LOG.info(""done async send"");"
"<line6>    log.debug(""login:: {} users were found"", users.size());<line7>      log.debug(""No users was found: {}"", userOrEmail);<line11>      log.debug(""Password does not match: {}"", u);<line14>      log.debug(""Not activated: {}"", u);<line16>    log.debug(""login user groups {}"", u.getGroupUsers());<line17>      log.debug(""No Group assigned: {}"", u);"
"<line11>        LOG.debug(""Resource has no File reference: {}"", resource);<line16>        LOG.debug(""Resource has no URI reference: {}"", resource);"
"<line6>      LOGGER.info(""test role doesn't exist, but it's ok"");"
"<line4>      LOG.warn(""ClosedChannelException caught. Cause: "", cause);<line6>    LOG.warn(""Unexpected exception. Closing channel {}"", ctx.channel(), cause);"
"<line13>        log.error(""Unable to iterate on HDFS directories "" + e.getMessage());"
"<line4>    LOG.debug(""determine record resources for data model '{}'"", finalDataModel.getUuid());<line9>    LOG.debug(""determined record resources for data model '{}'"", finalDataModel.getUuid());"
"<line20>      LOGGER.debug(""Launching: {}"", args);<line25>          LOGGER.debug(""Process Error Out: {}"", errOutput);<line25>          LOGGER.debug(""Process Out: {}"", processReader.getOutput());<line38>        LOGGER.debug(""Audit Request: {}"", auditRequest);"
"<line17>      logger.error(""error tagging efs - > "" + resourceId, ase);"
"<line15>            log.warn(""Cannot detect a charset for the zip file"");<line28>      log.error(e.getMessage(), e);"
"<line13>    logger.info(String.format(""Connected to cluster: %s\n"", metadata.getClusterName()));<line14>      logger.info(String.format(""Datacenter: %s; Host: %s; Rack: %s\n"",host.getDatacenter(), host.getAddress(), host.getRack()));"
"<line3>    LOGGER.debug(""Registering: "" + processID);<line7>      LOGGER.debug(""ProcessDescription is not valid. Removing "" + processID + "" from Repository."");"
<line2>    log.trace(XTCE_SEQUENCE_CONTAINER);
"<line2>      log.debug(""persisting cachedRunningQuery ""+ crq.getQueryId()+ "" to cache with status ""+ crq.getStatus());<line8>      log.debug(""persisting cachedRunningQuery ""+ crq.getQueryId()+ "" to database with status ""+ crq.getStatus());"
"<line13>      log.debug(String.format(""Use default network interface to bind %s"", address));"
<line5>        LOGGER.error(t.toString(), t);
"<line5>      LOG.info(""Creating new pool for "" + builder.getName());"
"<line13>      LOGGER.error("""", exc);"
"<line33>              LOG.warn(""Failed to delete temp file[%s]"", zippedFile);"
"<line5>    log.info(""Extracting "" + fileName + "" to "" + target);"
"<line6>      LOGGER.info(""none of config center source enabled."");<line8>    LOGGER.info(""use config center source {}"", configCenterConfigurationSource.getClass().getName());"
"<line3>    logger.info(""Ready to deploy"");<line4>      logger.info(""Deploying on delegate."");"
"<line3>    log.trace("">> addOrder() bidOrder={}"", order);"
"<line8>        log.info(""Successfully set the configured order number generator"");<line10>        log.info(""Setting default order number generator"");"
"<line18>          logger.warn(String.format(""unhandled exception happened when calling %s"", task.getClass().getName()),t);"
"<line10>        LOG.info(""asyn call time "" + c);"
"<line2>    logger.debug(""Adding NZB for {} to NZB with category {}"", title, category);<line5>      logger.info(""Error while trying to add link {} for NZB \""{}\"" to NZBGet queue: {}"",content,title,throwable.getMessage());"
"<line6>      logger.debug(""Got response string from player: "" + getId());"
"<line1>    logger.debug(""Searching index using series query '{}'"", query);"
"<line2>      logger.info(""Listening on port {}"", StructrLicenseManager.ServerPort);<line6>          logger.info(""##### New connection from {}"", socket.getInetAddress().getHostAddress());<line37>            logger.info(""License verification failed."");<line40>          logger.warn(""Unable to verify license: {}"", t.getMessage());<line43>      logger.warn(""Unable to verify license: {}"", t.getMessage());"
"<line16>      LOG.info(""BOLT ack TASK: {} TIME: {} TUPLE: {}"", taskId, delta, input);"
"<line17>        log.info(this,""HttpServiceComponent[""+ this.cName+ ""] for feature[""+ this.feature+ ""] started SUCCESS: port=""+ this.port);"
"<line7>        LOG.warn(""Failed to do initial update, will retry in [{}]ms, error: "",new Object[] {retryWaitMillisec, ex.getMessage(), ex});"
"<line15>            log.trace(""Using existing connection to {}"", serverAddr);<line34>        log.debug(""Failed to connect to {}"", servers.get(index), tte);"
"<line6>        logger.debug(""Batch ignored, flush operation not scheduled and queue is not full"");<line12>      logger.debug(""Batch ignored, no elements left in queue"");<line15>    logger.info(""Batch #{} - Preparing {} entries, scheduled={}"", batchNumber, batchSize, scheduled);<line22>    logger.info(""Batch #{} - Finished"", batchNumber, batchSize);"
"<line3>      LOGGER.debug(""Get last commit which are given to a param map with {} elements"", param.size());"
"<line7>              LOGGER.info(""Create dir {} in hdfs"", dir);"
<line10>      LOG.info(detail.getEndpointUri());
"<line1>    log.trace(""permissionDenied"");"
"<line2>    logger.debug(""Edit tools"");"
<line28>      log.error(systemException, systemException);
"<line2>      LOG.info(""Attaching Jira {} with snapshot"", basicIssue.getKey());"
"<line33>      logger.error(""StudyController - deleteComprehensionTestQuestion - ERROR"", e);"
"<line3>    LOG.info(""Request to get notebook info"");<line13>      LOG.error(""Exception in NotebookRestApi while get notebook info"", e);"
"<line10>      LOG.info(""Found plugin: {}"", plugin.getDescriptor().getPluginId());"
"<line3>      LOGGER.info(""setLight() successful for device : {}"", deviceResponse.getDeviceIdentification());"
"<line7>          log.trace(""Message "" + msg.getSrc() + "" -> "" + msg.getDest() + "" with workerIndex -1"");<line9>            log.trace(""Discarding message ""+ msg.getSrc()+ "" -> ""+ msg.getDest()+ "" with workerIndex ""+ header.getIndex());"
"<line9>      logger.error(""Upon plug-in service registration - "" + "" can not create a PluginInfo object!"");<line11>      logger.info(""Plug-in service ("" + pluginInfo.getPluginName() + "")"" + "" was registered."");"
"<line25>      LOG.debug(""acceptReducedValues: Accepted one set with "" + numReducers + "" aggregated values"");"
"<line6>        log.debug(""Failed to getMatching."");"
"<line15>      LOG.error(""While trying to send task completed mail for {}"", taskId, t);<line17>      LOG.debug(""Task {} mail sent with status {} (delete result {})"", taskId, shouldSendState, result);"
"<line1>    logger.debug(""Received deployment changed event, processing..."");<line9>            logger.debug(""New deployment {} has been discovered and will be deployed"", name);<line13>            logger.debug(""Deployment {} deployed successfully"", name);<line14>            logger.warn(""Deployment {} failed to deploy due to {}"", name, e.getMessage(), e);<line22>            logger.debug(""New deployment {} has been discovered and will be deployed"", identifier);<line25>            logger.debug(""Deployment {} undeployed successfully"", identifier);<line26>            logger.warn(""Undeployment {} failed to deploy due to {}"", identifier, e.getMessage(), e);"
"<line2>    LOG.debug(""Stop counting..."");"
"<line2>    logger.info(""Reading configuration for 1.2"");<line38>          logger.debug(""Found unexpected entry"");"
"<line1>    LOGGER.info(""Bind handler {} into jetty server {}:{}"",handler.getClass().getSimpleName(),jettyServerConfig.getHost(),jettyServerConfig.getPort());"
"<line19>    log.debug(""Number of components:""+ server.getStorageManager().getJournalSequentialFileFactory().getCriticalAnalyzer().getNumberOfComponents());"
"<line2>      log.info(""dimensions bad in dot()"");"
"<line7>      log.trace(""AncestorQueryIterator init()"");"
"<line9>        LOGGER.debug(""Token was successfully renewed (new TTL = {} seconds), response: {}"",ttl,bodyAsString(response.getRestResponse()));<line15>          LOGGER.warn(""Token TTL ({}) is not enough for scheduling"", ttl);<line18>        LOGGER.warn(""Vault token is not renewable now"");<line21>        LOGGER.warn(""Could not renew the Vault token"", e);"
"<line5>      LOG.error(""Unable to cleanup access tracker"", e);"
"<line4>      LOG.info(""Unregistering webhook for endpoint: {}"", delegateEndpoint);"
"<line7>    LOG.debug(""invokeRpc: uriPath: {}, input: {}"", uriPath, actualInput);<line15>        LOG.debug(""Parsed YangInstanceIdentifier: {}"",inputContext.getInstanceIdentifierContext().getInstanceIdentifier());<line15>        LOG.debug(""Parsed NormalizedNode: {}"", inputContext.getData());"
"<line14>      LOG.error(""Insert obervations thread was interrupted!"", e);"
"<line8>      LOG.error(""There was an error in Git {} operation"", operation);"
<line6>        log.debug(invalidFileVersionException, invalidFileVersionException);
"<line9>    log.warn(String.format(""Unexpected value type for serialized key %s"", key));"
"<line7>        logger.debug(""checkServerTrusted for {} succeeded"", tm);<line11>        logger.debug(""checkServerTrusted for {} failed"", tm);"
"<line13>      log.warn(""decodePrivateKey({}) non-standard RSA exponent found: {}"", keyType, e);<line21>      log.warn(""decodePrivateKey({}) mismatched modulus values: encoded={}, calculated={}"",keyType,n,modulus);"
"<line21>          log.warn(""Failed to transfer ""+ urlToInstall+ "" to ""+ machine+ "", retryable error, attempt ""+ attemptNum+ ""/""+ numAttempts+ "": ""+ e);<line23>        log.warn(""Failed to transfer ""+ urlToInstall+ "" to ""+ machine+ "", not a retryable error so failing: ""+ e);"
"<line6>      LOGGER.debug(""Access token: {}"", decodedJwt);<line7>      LOGGER.debug(""Access token can not be logged. {}"", e.getMessage());"
"<line15>        log.error(""Error while sending "" + name + "" packet"", e1);<line27>        log.error(""Error while receiving packet on "" + name + "" socket"", e1);"
"<line14>        log.error(""Cannot set autoCommit=true"", e);"
"<line8>    LOG.trace(""resolveArtifacts({}) returns {}"", coords, result);"
"<line12>        LOGGER.error(""Bestimmung der Installationspfade fÃ¼r das WollMux-Paket fehlgeschlagen."");<line20>      LOGGER.error("""", e);"
"<line15>      logger.debug(""to be created, intake common"");<line15>      logger.debug(objectAsXmlString(intake, IntakesCommon.class));"
"<line1>    log.debug(""getting MbZielobjTypTxt instance with id: "" + id);<line6>        log.debug(""get successful, no instance found"");<line7>        log.debug(""get successful, instance found"");<line10>      log.error(""get failed"", re);"
"<line9>      logger.warn(""Exception loading public key from PEM"", e);"
"<line8>    log.info(""setCredential {} status for {} to {}"", credential, entityId, status);"
"<line5>    LOGGER.info(""WFS GetFeature&typename=gsml:MappedFeature response:\n"" + prettyString(doc));"
"<line1>    LOG.info(""Setting up HTTP connector for web server"");"
"<line2>    LOG.debug(""List status of {}"", f.toString());"
"<line2>    LOGGER.trace(""checking whether Bundle {} is a domain"", bundle);<line5>      LOGGER.trace(""Bundle {} is not a domain, ignoring"", bundle);"
"<line21>      logger.error(""TTransportException inside handler"", e);<line24>      logger.error(""TApplicationException inside handler"", e);<line27>      logger.error(""Exception inside handler"", e);<line35>      logger.error(""Exception writing to internal frame buffer"", ex);"
"<line9>          logger.debug(""MapQuest statuscode: "" + status);<line9>          logger.error(""MapQuest messages "" + jsonNode.get(""info"").get(""messages""));<line18>            logger.warn(""Error retrieving GeocodedAddress from MapQuest response "" + json, ex);<line24>      logger.error(""Malformed MapQuest url!"", ex);<line25>      logger.error(""UTF-8 Unsupported?!"", ex);<line26>      logger.error(""Error opening API resource! "" + ex.toString() + "" Response: "" + json);<line27>      logger.error(""MapQuest response was not formatted correctly. Response: "" + json, ex);<line28>      logger.error("""" + ex);"
"<line55>      LOGGER.warn(""You can use @Route#order() to ensure the routes are not executed in random order"");"
"<line40>          LOGGER.debug(""Error validating standard format DMS string."", e);<line43>          LOGGER.debug(""Error validating standard format DMS pattern."", e);"
"<line5>        logger.trace(""Downloading montage image {} for channel '{}'"", imageNumber, channelId);<line16>              logger.debug(""IOException creating BufferedImage from downloaded image: {}"", e.getMessage());<line21>        logger.debug(""Some images could not be downloaded: wanted={}, actual={}"",numberOfImages,images.size());"
"<line21>        LOGGER.error(""URL.setURLStreamHandlerFactory() can only be called once per JVM instance, and""+ "" currently something has set it to;  additionally unable to discover type of""+ "" Factory"",e);<line24>        LOGGER.info(""setURLStreamHandlerFactory already set on this JVM to FsUrlStreamHandlerFactory. ""+ "" Nothing to do"");<line28>      LOGGER.error(""URL.setURLStreamHandlerFactory() can only be called once per JVM instance, and currently""+ "" something has set it to: ""+ type);"
"<line2>    logger.debug(""Checking if the atom container set empty: "", atomContainerCount == 0);"
"<line12>            logger.debug(""{} is a {} which is not interruptable as the thread running the session has not ""+ ""been set - please check the implementation if this is not desirable"",sessionId,this.getClass().getSimpleName());"
"<line2>    LOG.info(""Twilight plugin stopped "");"
"<line6>    log.info(""Executing command: "" + command);"
"<line3>      logger.info(""Processing schedule MatchManagerTask...""+ matchManager.processEvent(null, IMatchManager.Event.SCHEDULED_EVENT));<line4>      logger.error(""Error while processing scheduled MatchManager scheduled task.."");<line4>      logger.error(""error"", e);"
"<line7>      LOG.error(""While deleting {}"", model.getObject().getKey(), e);"
"<line12>    log.info(""About to run: {}"", jobName);"
"<line2>    log.debug("""");<line2>    log.debug(""Starting wiser on port "" + smtpPort);"
"<line2>    logger.debug(""getInterDirectPingMeasurementByMeasurement started ..."");<line8>      logger.debug(ex.getMessage(), ex);"
"<line9>        LOG.debug(""release of transaction member '{}'. "", mem);<line11>        LOG.error(""release member {}"", mem, t);"
"<line11>                      log.debug(""Configured Hibernate Dialect:\t"" + getHibernateDialect());<line25>                      log.warn(""Unsupported dialect (""+ getHibernateDialect()+ "") configured.\n""+ ""Please use one of the supported (Oracle, Postgresql or Derby)"");<line44>                        log.warn(""table not created yet, trying again"", e);"
"<line16>        log.error(""fieldType {} is illegal."", fieldType.toString());"
"<line1>    logger.debug(""writeCommand: {}"", HexUtils.bytesToHex(message));<line12>      logger.debug(""writeCommand failed: {}"", e.getMessage());"
"<line19>          log.trace(""Failed to send nameservice request for "" + name.name, ioe);<line22>        log.info(""Failed to send nameservice request for "" + name.name, ioe);"
"<line3>      Log.trace(""Session {} Request ID {}, event complete: {}"", streamID, rid, asyncEvent);"
"<line2>    LOG.info(""START TaskPersistDataStage.process()"");<line8>    LOG.info(""END TaskPersistDataStage.process() for cluster {} took {} ms"",cache.getClusterName(),(endTime - startTime));"
"<line18>          log.info(Bundle.getMessage(""ConfigMigratedToProfile""));<line20>        log.error(""Profiles not configurable. Using fallback per-application configuration. Error: {}"",ex.getMessage());<line27>          log.info(""Starting with profile {}"", profile.getId());<line28>          log.info(""Starting without a profile"");<line30>        log.error(""Specify profile to use as command line argument."");<line30>        log.error(""If starting with saved profile configuration, ensure the autoStart property is set to""+ "" \""true\"""");<line30>        log.error(""Profiles not configurable. Using fallback per-application configuration."");<line32>      log.info(""Profiles not configurable. Using fallback per-application configuration. Error: {}"",ex.getMessage());"
<line1>    Log.info(string);
"<line1>    log.debug(""attaching clean StgMBstnStatus instance"");<line3>      log.debug(""attach successful"");<line4>      log.error(""attach failed"", re);"
"<line4>      logger.trace(""Opening port {}"", serialPortName);<line8>      logger.trace(""Configure serial port parameters: {}"", portSettings);<line13>      logger.trace(""SerialPort opened successful on {}"", serialPortName);<line23>        logger.debug(""Enable receive threshold is unsupported"");<line27>        logger.debug(""Enable receive timeout is unsupported"");<line30>        logger.warn(""Possible bug because a new serial port value was set during opening new port."");<line33>      logger.debug(""Failed to get inputstream for serialPort"", ioe);<line35>      logger.warn(""Possible bug because a listener was added while one already set."", tmle);<line37>      logger.debug(""Port already in use: {}"", serialPortName, piue);<line39>      logger.debug(""Port does not support requested port settings (invalid dsmr:portsettings parameter?):""+ "" {}"",serialPortName,ucoe);"
"<line11>        logger.warn(""Slow request: {} {} ({}ms)"", req.getMethod(), getFullUrl(req), elapsedMS);"
"<line4>      logger.info(""Will parse {}"", serializedData);<line5>      logger.debug(""Template parsed {}"", template);"
<line34>      log.error(systemException, systemException);
"<line16>              LOG.trace(""ReplyTo is disabled on endpoint: {}"", endpoint);<line28>                LOG.debug(""Disabling JMSReplyTo: {} for destination: {}. Use preserveMessageQos=true to""+ "" force Camel to keep the JMSReplyTo on endpoint: {}"",new Object[] {jmsReplyTo, to, endpoint});<line43>              LOG.debug(""Using JMSReplyTo destination: {}"", replyTo);<line45>              LOG.trace(""Not using JMSReplyTo"");<line47>            LOG.trace(""Created javax.jms.Message: {}"", answer);"
"<line28>      LOGGER.debug(""Invalid spatial query type specified.  Will not apply spatial filter."");"
"<line8>      log.error(""Unable to get asset entries"", exception);"
"<line8>      LOGGER.warn(""Could not load class"", e);"
"<line2>    log.debug(""append [{}]"", path);"
"<line2>    log.debug(""Sending message to role "" + role);<line2>    log.debug(""User Service : "" + Context.getUserService());<line5>    log.debug(""Sending message "" + message + "" to "" + users);"
"<line5>      LOGGER.error(""appendMessageLog schedule log error,log:{} {},code:{}"",event.getSubject(),event.getMessageId(),code);"
"<line5>    log.info(""=======================Testing the order: ExP, EP, ER, ES-1, ES-2=======================""+ "" "");<line10>    log.info(""=======================Adding an execution plan ======================= "");<line15>    log.info(""=======================Adding an event receiver ======================= "");<line20>    log.info(""=======================Adding an event publisher ======================= "");<line25>    log.info(""=======================Adding a stream definition===================="");<line31>    log.info(""=======================Adding another stream definition===================="");"
"<line17>        log.error(""Can't watch env.properties file for changes"", e);"
"<line7>          log.debug(""Gauge {} is null-valued, defaulting to 0."", gauge);<line18>        log.debug(""Invalid type for Gauge {}: {}, only number types and booleans are supported by this""+ "" reporter."",gauge,value.getClass().getName());"
"<line1>    logger.info(""Requesting that Node be offloaded"");<line10>        logger.error(""Failed when attempting to disconnect node from cluster"", cause);"
"<line3>    logger.debug(""Doorbird action 'getRingTimeLimit' called"");<line6>      logger.info(""Doorbird Action service ThingHandler is null!"");"
"<line10>            log.info(""Re-indexing of ""+ size+ "" objects done after updating ""+ obj.getClass().getName()+ "":""+ obj.getId());"
"<line4>      log.error(""Error in printing AST."", e);"
"<line4>    LOGGER.info(""Time range between : "" + startTime + "" and "" + endTime);"
"<line8>      logger.info(""ElasticsearchPlugin config:{}"", elasticsearchPluginConfig);"
"<line13>          LOGGER.error(""Number of IO operations cannot be negative for dataset: "" + this);"
"<line31>      log.warn(""[{}] Failed to fetch device state data"", device.getId(), e);"
"<line9>      logger.info(""creating schema '{}'"", schemaName);"
"<line4>    log.debug(""{}"", t);"
"<line3>      LOGGER.info(""RTree "" + getTestOpName() + "" Test With Two Dimensions With Integer Keys."");"
"<line4>      LOGGER.warn(""could not close ZooKeeper client due to interrupt"", e);"
"<line5>          LOGGER.info(""UDFs can't be loaded as as dir {} doesn't exist or is not a directory"", pluginDir);<line12>        LOGGER.error(""Failed to load UDFs from location {}"", pluginDir, e);"
"<line42>        application.error(this.getClass(), infoLog, soaException);<line43>        application.info(this.getClass(), infoLog);<line45>        LOGGER.debug(getClass() + "" "" + infoLog + "", payload:\n"" + soaException.getMessage());<line47>      LOGGER.error(e.getMessage(), e);"
"<line4>    LOGGER.trace(ENTERING_STR, methodName);<line4>    LOGGER.trace(""subscriptionId = {}"", logSanitizedId);<line10>      LOGGER.debug(""Removing (unregistering) subscription: {}"", logSanitizedId);<line14>        LOGGER.debug(""No ServiceRegistration found for subscription: {}"", logSanitizedId);<line18>          LOGGER.debug(""Deleting subscription for subscriptionId = {}"", logSanitizedId);<line20>          LOGGER.debug(""subscriptionConfig is NULL for ID = {}"", logSanitizedId);<line22>        LOGGER.debug(""IOException trying to delete subscription's configuration for subscription ID {}"",subscriptionId,e);<line23>      LOGGER.debug(""Subscription removal complete"");<line24>      LOGGER.debug(""Could not delete subscription for {}"", logSanitizedId, e);<line25>    LOGGER.trace(""EXITING: {}    (status = {})"", methodName, false);"
"<line5>    LOG.trace(""routine: {}.{} "", schemaName, routineName);"
"<line2>    LOG.info(""Adding ESB job factory for job "" + name + ""."");"
"<line15>        logger.error(""Error during HBase clean up"", e);"
"<line2>      log.info(""Skipping ""+ fixtureId+ "" tests. Fixture file ""+ fixtureFile.getCanonicalPath()+ "" not found."");<line3>      log.debug(e);"
"<line8>          log.warn(""SQL driver deregistration failed"", e);<line38>                    log.debug(""Set field "" + field.getName() + "" to null in class "" + clazz.getName());<line42>                  log.debug(""Could not set field ""+ field.getName()+ "" to null in class ""+ clazz.getName(),t);<line48>            log.debug(""Could not clean fields for class "" + clazz.getName(), t);"
"<line3>    logger.debug(""consumer "" + id + "" got m: "" + m);<line7>      logger.debug("" consumer ""+ id+ "" acked ""+ m.getClass().getName()+ ""now total received: ""+ receivedMessages.size());"
"<line2>    log.debug(""getModuleTypes()"");"
"<line19>      logger.debug(""No bridge connected or selected. Cannot set sensor state."");"
"<line7>        LOGGER.trace(StringUtilities.formatTimingMessage(""Time to paint layer "" + layer.getClass().getSimpleName() + "": "",System.nanoTime() - t0));"
"<line6>        log.warn(""The JDBC driver does not appear to support ResultSet.absolute(). Consider""+ "" reverting to the default behavior setting the driverSupportsAbsolute to false"",e);"
"<line3>      ActiveMQRALogger.LOGGER.trace(""destroy()"");<line11>      logger.debug(""Error unsetting the exception listener "" + this, e);<line18>      logger.debug(e.getMessage(), e);<line30>        ActiveMQRALogger.LOGGER.debug(""Error closing session "" + this, e);"
"<line1>    LOG.error(""Exception calling mesos ({} so far)"", failedMesosCalls.incrementAndGet(), t);"
"<line3>    log.info(""Test output for "" + getName());<line3>    log.info(""----------------------------------------"");<line4>    log.info(testOut.toString());"
<line5>        LOGGER.error(e);
"<line4>      log.warn(""moveEyelids - I have a null head"");"
"<line14>      LOGGER.error(""DashboardMetaDataDao - getStatisticsType() :: ERROR"", e);"
"<line6>      LOG.debug(MessageFormat.format(""Start Direct I/O output job abort: job={0} ({1}), state={2}"",jobContext.getJobID(), jobContext.getJobName(), state));<line13>      LOG.info(MessageFormat.format(""aborted Direct I/O output: job={0} ({1}), state={2}, elapsed={3}ms"",jobContext.getJobID(), jobContext.getJobName(), state, t1 - t0));"
"<line3>    logger.info(""Testing: "" + filename);"
"<line5>      log.warn(""Error loading {}, possibly jar was not compiled with maven."", GIT_PROPS, e);"
"<line7>      LOGGER.info(""Running queries {}."", queries.stream().collect(Collectors.joining("", "")));<line31>      LOGGER.error(e.getMessage(), e);"
"<line5>      logger.error(""[getAlertSystemRecovertIME]"", e);"
"<line2>    log.info(""Creating k8s namespace: {}"", testNamespace);"
<line9>      log.error(e);
"<line2>    LOG.debug(""Removing session {} from SSO client session store"", se.getSession().getId());<line5>      LOG.error(""Unable to remove session from SSO Store. Session store is not configured in servlet""+ "" context."");<line10>          LOG.debug(""Logout by session timeout. Notify sso server about client {} - {} - {} logout"",principal.getName(),principal.getClientUrl(),principal.getToken());"
"<line14>      LOG.debug(""Returned containers from MBean: {}"", containers);<line21>      LOG.debug(""Extracted allowlist: {}"", list);<line22>      LOG.error(""Invocation to allowlist MBean failed: "" + e.getMessage(), e);"
"<line2>    LOGGER.debug(""Salt: "" + ArrayConverter.bytesToHexString(msg.getSalt()));"
"<line6>      log.warn(""Could not find session for writable event,maybe it is closed"");"
"<line1>    logger.info(""Daten Schreiben nach: {}"", xmlFilePath.toString());"
"<line5>      logger.debug(""Bridge not discovered: ip is null"");<line8>      logger.debug(""Bridge not discovered: id is null"");<line11>      logger.debug(""Bridge not discovered: id {} is shorter then 10."", id);<line14>      logger.debug(""Bridge not discovered: id {} does not contain bridge indicator {} or its at the wrong""+ "" position."",id,BRIDGE_INDICATOR);<line19>      logger.debug(""Bridge not discovered: Failure accessing description file for ip: {}"", host);<line22>      logger.debug(""Bridge not discovered: Description does not containing the model name: {}"", description);"
"<line3>      log.warn(""Adding domain when one already exists"");"
"<line5>    LOG.debug(""Ask for singleton '{}'"", typeRef.getSimpleName());<line16>      LOG.debug(""Singleton '{}' has been deleted"", typeRef.getSimpleName());"
"<line13>        logger.trace(""Posting over http is successful. Status code: {}"", statusCode);<line17>        logger.error(""Did not receive a valid response from Flux core. Status code: {}, message: {}"",statusCode,EntityUtils.toString(httpResponse.getEntity()));<line34>      logger.error(""Posting over http errored. Message: {}"", e.getMessage(), e);"
"<line4>    log.info(String.format(""Synchronizing page %d with page size %d"", page, pageSize));<line5>      log.debug(""Events that are going to be synchronized are: "" + events);<line9>      log.info(""The lastSynchronized flag of these Events will be updated: "" + eventsUIDs);"
<line12>    LOGGER.debug(jsonText);
"<line9>      logger.error(""ERROR: AppUtil - httpResponseForInternalServerError()"", e);"
"<line24>                    log.info("">>> Performs put [node=""+ ((IgniteKernal) ignite).localNode()+ "", tx=""+ tx+ "", key=""+ key1+ "", cache=""+ cache1.getName()+ ']');<line27>                    log.info("">>> Performs put [node=""+ ((IgniteKernal) ignite).localNode()+ "", tx=""+ tx+ "", key=""+ key2+ "", cache=""+ cache2.getName()+ ']');<line33>                        U.error(log,""At least one stack trace should contain ""+ TransactionDeadlockException.class.getSimpleName(),e);"
"<line14>      logger.warn("""", fex);"
"<line4>      LOGGER.debug(""unable to convert WKT to a Geometry object: wkt={}"", wkt, e);"
"<line46>        log.warn(""Erro no Wait Element"");<line46>        log.warn(e);"
"<line2>    logger.debug(""Clearing expired approved sites"");<line4>      logger.info(""Found "" + expiredSites.size() + "" expired approved sites."");"
"<line6>    LOG.debug(""Remove task manager {}."", instanceId);"
"<line7>      logger.debug(""WebDAV create folder at: "" + parentUri);<line8>      logger.debug(""WebDAV create folder at: "" + resolvedParent);"
"<line10>      logger.info(""Hook unregistered: {}"", hookId);"
"<line3>    logger.debug(""This is a test of the root logger"");"
"<line6>    log.info(""Add a standby session: ""+ SystemUtils.getRawAddress(remoteSocketAddress)+ "":""+ remoteSocketAddress.getPort()+ "" for ""+ SystemUtils.getRawAddress(mainNodeAddress)+ "":""+ mainNodeAddress.getPort());<line7>      log.warn(""Memcached node {} is resolved into {}."", lastResolvedMainAddr, remoteSocketAddress);"
<line4>      LOGGER.error(e.getMessage(), e);
<line9>      log.error(exception, exception);
"<line3>      LOG.warn(""RangerDefaultPolicyResourceMatcher is already initialized. init() must be done again""+ "" after updating serviceDef"");"
"<line28>      logger.error(""Error creating user filter"", t);"
"<line5>        LOG.error(String.format(""Cannot find transform from %s to %s"", fromSrsId, toSrsId));<line10>      LOG.error(""Error converting: x="" + x + "" y="" + y + "" srs="" + fromSrsId, t);"
"<line3>      logger.warn(String.format(""delete image [%s] failed after management node restarted"", msg.getResourceUuid()));"
"<line5>        log.debug(""missing time {} in [{}] --> {}"", timeField.getField(), bundle.getCount(), bundle);"
"<line3>      logger.trace(""xasuspend on "" + this.tx);"
"<line3>      LOGGER.warn(""preventing access to {}"", repo);"
"<line4>      logger.info(""Error committing offsets."", e);<line5>      logger.trace(""About to clear offsets map."");"
"<line12>      LOG.error(""Plan ID is null! Unable to create PlanInstance!"");<line18>      LOG.error(""Plan with ID {} does not exist in CSAR {}!"", planId, csar.id().csarName());"
"<line8>        log.info(""failed to parse heat parameters "");"
"<line6>      log.error(""Unable get content : "" + e.getMessage());<line8>      log.error(""GetTextContent Error : "" + e.getMessage());"
"<line1>    log.debug(""merging StgNmbZusatz instance"");<line4>      log.debug(""merge successful"");<line6>      log.error(""merge failed"", re);"
<line31>              logger.warn(reply.getError().getDetails());
"<line6>    LOG.info(""Completed configuration of {}"", clusterSpec.getClusterName());<line6>    LOG.info(""Solr Hosts: {}"", getHosts(cluster.getInstancesMatching(role(SOLR_ROLE)), jettyPort));"
<line16>                LOG.info(null, getUpdateNotification(config));
"<line3>    logger.info(""[Sedona-Viz][SaveVectormageAsLocalFile][Start]"");<line12>    logger.info(""[Sedona-Viz][SaveVectormageAsLocalFile][Stop]"");"
"<line7>        logger.info(""Updated digest file to "" + digest);"
"<line6>            logger.info(""processing message: "" + message);"
"<line5>      log.info(""Test eternalPolicy, key: "" + key);"
"<line30>    logger.trace(""ThingAction 'sendMessageToDevice' called with value(s): device='{}', message='{}',""+ "" title='{}'"",device,message,title);"
"<line36>      LOG.info(""patch skipped: typeName={}; applyToVersion={}; updateToVersion={}"",patch.getTypeName(),patch.getApplyToVersion(),patch.getUpdateToVersion());"
"<line13>    LOGGER.info(""JSON content\n{}"", specContent);"
"<line1>    log.debug(""persisting StgSysExportItv instance"");<line3>      log.debug(""persist successful"");<line4>      log.error(""persist failed"", re);"
"<line6>        LOGGER.warn(MessageFormat.format(""Could not parse date string: \""{0}\"""", dateString), e);"
"<line20>          logger.info(""Could not drop table before staging"");<line48>        logger.error(e.getMessage());"
"<line4>    log.info(""Stopping node "" + GRIDS_COUNT);<line10>        log.info(""Wait data check."");<line11>        log.info(""Finished wait data check."");"
"<line8>      log.debug(""Failed to get the component with id {} for component instance {} creation. "",componentInstance.getComponentUid(),componentInstance.getName());"
"<line10>      LOG.error(""While changing password for {}"",SyncopeEnduserSession.get().getSelfTO().getUsername(),e);"
"<line3>      log.debug(""Calling ConnectionsActionSetService"");"
<line3>    LOGGER.debug(String.format(Messages.Log.MAPPING_USER_OP_S, GET_ALL_SECURITY_RULES_OPERATION, order));<line4>    LOGGER.debug(String.format(Messages.Log.MAPPED_USER_S, cloudUser));<line8>      LOGGER.debug(String.format(Messages.Log.RESPONSE_RECEIVED_S, securityRuleInstances));<line10>      LOGGER.debug(String.format(Messages.Exception.GENERIC_EXCEPTION_S, e + e.getMessage()));
"<line2>    LOG.info(""Start sorting measure points."");<line39>    LOG.info(""Finish sorting measure points."");"
"<line6>      LOG.error(""Could not find method parseFrom in class "" + protoClass, e);<line8>      LOG.error(""Could not access method parseFrom in class "" + protoClass, e);<line10>      LOG.error(""Error invoking method parseFrom in class "" + protoClass, e);"
<line21>        log.warn(exception, exception);<line31>            log.debug(exception, exception);
"<line4>      logger.trace(""fromOid("" + internalId + "")"");"
"<line14>      LOG.info(""Adding "" + numRows + "" rows to table: "" + selected);<line31>      LOG.info(""Added "" + numRows + "" rows to table: "" + selected);<line32>      LOG.warn(""Caught exception in action: "" + this.getClass());"
"<line15>      log.warn(""Bind failed for dn '{}'"", dn, e);"
"<line4>        logger.info(""Resolving condition with description: "" + condition.getDescription());<line7>          logger.warn(""An action language other than CQL was found: "" + condition.getLanguage());<line10>          logger.error(""Missing condition expression"");<line12>        logger.info(""Evaluating action condition expression "" + condition.getExpression());<line17>          logger.warn(""Expression Returned null"");<line20>          logger.warn(""The condition returned a non-boolean value: "" + result.getClass().getSimpleName());<line23>          logger.info(""The result of condition expression %s is false"", condition.getExpression());"
"<line31>      logger.info(""Error generating narrative"", e);"
"<line6>        logger.error(""Zip should fail when input String array is null"");<line8>        logger.debug(""Detecting null input File array (String, File): OK"");<line15>        logger.error(""Zip should fail when any input filename is null"");<line17>        logger.debug(""Detecting null input filename (String, File): OK"");<line28>        logger.error(""Zip should fail when any input filename does not exist"");<line30>        logger.debug(""Detecting non-existing input filename (String, File): OK"");<line37>        logger.error(""Zip should fail when destination File is null"");<line39>        logger.debug(""Detecting null destination File (String, File): OK"");<line46>        logger.error(""Zip should fail when destination file already exists"");<line48>        logger.debug(""Detecting existing destination File (String, File): OK"");<line55>        logger.error(""Zip should fail when the destination File does not represent a zip file"");<line57>        logger.debug(""Detecting destination File not representing a valid zip file (String, File): OK"");<line59>      logger.error(""Another exception was expected, but got {} instead: {}"",e.getClass().getName(),e.getMessage());"
"<line3>      logger.debug(""SBMLReader.readMathML called"");"
"<line7>      log.debug(""saveProgramAccess:"" + pa.getId());"
"<line1>    LOG.debug(""setting max tuples to {}"", maxNumbers);"
"<line14>              LOG.info(broker+ "" consumer: ""+ message.getText()+ "" ""+ message.getDestination()+ "" ""+ message.getMessageId()+ "" ""+ Arrays.toString(message.getBrokerPath()));<line19>        LOG.info(""browser '"" + broker + ""' browsed "" + totalCount);<line21>        LOG.info(""Exception browsing "" + e, e);<line30>          LOG.info(""Exception closing browser "" + e, e);"
"<line1>    logger.info(""In postCommit with: ["" + arg0 + ""]"");"
<line22>          log.warn(invalidDDMStructureException.getMessage());
"<line15>    log.info(""ruleBody:{}"", ruleBody);<line15>    log.info(""inputParameters:{}"", inputParameters);<line15>    log.info(""outputParameters:{}"", outputParameters);<line21>    log.info(""getting ready to submit rule:{}"", ruleAsString);"
"<line5>      LOG.debug(""Starting SNMP producer on {}"", this.endpoint.getAddress());<line13>      LOG.debug(""Snmp: i am sending"");<line47>        LOG.debug(""Snmp: sended"");"
"<line12>      logger.error(""error in extractCategoryFormValues"", t);"
"<line6>        Logger.warn(PentahoSystem.class.getName(),Messages.getInstance().getErrorString(""PentahoSystem.WARN_OBJECT_NOT_CONFIGURED"", interfaceClass.getSimpleName()));<line11>      Logger.debug(PentahoSystem.class.getName(),Messages.getInstance().getErrorString(""PentahoSystem.ERROR_0026_COULD_NOT_RETRIEVE_CONFIGURED_OBJECT"",interfaceClass.getSimpleName()),e);"
"<line21>        LOG.error(""Not able to instantiate pruning debug class"", e);"
"<line3>      LOG.debug(""Ending workitem at key "" + immutableBytesToString(currentRangeStartKey) + "" ."");"
"<line2>    log.debug(""Removing AuthenticatioKey with keyId {}"", keyId);"
"<line2>    LOGGER.trace(MessageFormat.format(""Updating ActionDesignTrace {0}."", actionDesignTrace.getMetadataKey().toString()));"
"<line13>            LOG.trace(""Check for trust status of class: {}"", clazz.getName());"
"<line6>          LOGGER.info(""channel {} read idle."", ctx.channel());<line11>          LOGGER.error(exx.getMessage());<line18>            LOGGER.debug(""will send ping msg,channel {}"", ctx.channel());<line21>          LOGGER.error(""send request error: {}"", throwable.getMessage(), throwable);"
"<line2>    LOG.trace(""Configuring client-side SSLContext parameters on SSLContext [{}]..."", context);<line3>      LOG.info(""Configuring client-side SSLContext session timeout on SSLContext [{}] to [{}]."",context,this.getSessionTimeout());<line5>    LOG.trace(""Configured client-side SSLContext parameters on SSLContext [{}]."", context);"
"<line1>    log.info(""Loading of trusted entities from Sesame"");<line10>        log.error(""Cannot create instance of Priorities class"", e);<line20>      log.error(""Loading failed."", e);<line22>      log.info(""The loading from Sesame finished"");"
"<line9>        LOG.debug(""Trust Factory is empty"");<line11>      LOG.error(e.getLocalizedMessage(), e);"
"<line2>    LOGGER.debug(""Finished sending ""+ (t.isFile() ? t.getFileName() : t.getObject())+ "" through connection ""+ c.hashCode());"
"<line12>      logger.error(""Error in buildGuiFragmentFromRes"", t);"
"<line15>      LOGGER.info(""No vocabularies found for uri {}"", resourceId);<line17>      LOGGER.warn(""Multiple vocabularies found for uri {}: {}"",resourceId,candidates.stream().map(Vocabulary::getName).collect(Collectors.joining("", "")));"
"<line25>        log.warn(""undefined property '""+ currentMimeTypePropName+ ""', no data will be dispatched to port '""+ portName+ ""'"");"
"<line9>    log.info(""Study with accession Id: {} found and updated"", accessionId);"
"<line4>    LOGGER.debug(""Starting "" + ActiveRequestSenderTest.testType.getName());"
"<line3>    log.info(""Creating Kafka Streams, store name: {}"", storeName);"
"<line7>        log.info(""Double-clicked on: {} Toggling filter enabled."", o);"
"<line8>      logger.error(""Error getting system properties"", ex);<line22>      logger.error(""Error updating service properties"", ex);"
"<line23>      logger.info(""New cube "" + cubeName + "" has "" + cuboidCount + "" cuboids"");"
"<line4>        logger.debug(""Using ThemeResolver ["" + this.themeResolver + ""]"");<line8>        logger.debug(""Unable to locate ThemeResolver with name '""+ THEME_RESOLVER_BEAN_NAME+ ""': using default [""+ this.themeResolver+ ""]"");"
"<line18>      log.trace(""calculated network delay for module {} in solution {} is (numVisitsModule[i] *""+ "" sumOfDelaysSingleModule): {}"",i,bestSol.toString(),numVisitsModule[i] * sumOfDelaysSingleModule);"
"<line7>        LOGGER.debug(""{} to go."", entities.getCount());<line15>    LOGGER.debug(""Deleted {} using {}."", count, doa.getClass().getName());"
"<line2>    logger.debug(testName + "": status = "" + statusCode);"
<line10>      log.error(e.getMessage(), e);
"<line2>    LOGGER.info(""Launching crawler for page "" + getUrl());"
"<line5>      LOG.trace(""[{}] Decoded response frame {} from self-contained segment"", logPrefix, frame.streamId);<line9>    LOG.trace(""[{}] Done processing self-contained segment ({} frames)"", logPrefix, frameCount);"
<line30>      log.error(exception, exception);
"<line4>      logger.error(""Unable to instantiate converter of type {} for key {}"",new Object[] {constructor.getClass().getName(), dbName});"
"<line15>    LOG.debug(""Internal AvroCoder's schema is {}"", internalAvroCoder.getSchema());<line15>    LOG.debug(""Encode value is {}"", value);"
"<line8>    log.debug(""createPageSource(transaction=%s, session=%s, split=%s, table=%s, columns=%s)"",transaction, session, split, table, columns);"
"<line14>      logger.error(""SQL Command failed: "" + sql.toString() + "":"" + e.getMessage());"
"<line4>    LOG.debug(""{} Received JMS message - topic: {}, message: {}"",this,MetadataTopics.FEED_INIT_STATUS_CHANGE,event);<line4>    LOG.info(""{} Received feed initialization status change event: {}"", this, event);<line5>      LOG.debug(""No metadata recorder registerd yet - ingoring event: {}"", event);"
"<line5>      LOGGER.error(""Unexpected file visiting failure: "" + path, e);"
"<line2>    LOG.debug(""Configuring file sessions: {}"", profile.getPrefix());<line3>    LOG.debug(""Configured file sessions: {}"", directory);"
"<line10>      log.info(""Loaded model {} from location {}"", model.getId(), modelPath);"
"<line1>    logger.info(MessageFormat.format(""Trying to load entityType: {0}, with id: {1}"", docEntityType, id));<line10>    logger.debug(MessageFormat.format(""Found these rows for entityType: {0}, with id: {1}, rows: {2}"",docEntityType, id, rows));"
"<line4>      LOG.warn(""oneside beforeDocumentChange - myChangedBlockData == null"");"
"<line7>      LOG.info(""Could not parse cleanup type from {} for {}"", statusMessage, taskId);"
"<line34>              LOGGER.error(""Unknown method: {}"", method);<line42>      LOGGER.error(""Failed to initialise Keycloak. There is a problem with the configuration."");<line54>            LOGGER.debug(""Logging out Http Sessions"");<line55>              LOGGER.debug(""Removed session: {}"", id);"
"<line6>      log.warn(""Could not get method by reflection. This could happen if you are using @Parameters in""+ "" combination with annotations."",e);"
<line3>      log.warn(message);
"<line6>        log.trace(""Attempting to load iterator class {}"", iterInfo.className);<line22>      log.error(e.toString());"
"<line26>        LOG.debug(""execute: "" + MemoryUtils.getRuntimeMemoryStats());<line51>        LOG.info(""execute: ""+ numPartitions+ "" partitions to process with ""+ numThreads+ "" compute thread(s), originally ""+ numComputeThreads+ "" thread(s) on superstep ""+ superstep);<line60>      LOG.info(""execute: BSP application done (global vertices marked done)"");"
"<line12>      log.error(""Exception happened while monitoring EntityId"", ex);"
"<line8>        logger.debug(""Exception, previous server on {} query interrupted or timed out, restoring playlist""+ "" anyway"",thing.getLabel());<line26>      logger.debug(""Restoring playlist to node {} on server {}"", parentId, thing.getLabel());"
"<line1>    LOG.info(""Pointing peer reviews to org {}"", orgToReference.getId());<line2>    LOG.info(""Found {} peer reviews referencing org"", peerReviews.size());<line6>            LOG.info(""Updated peer review {}"", p.getId());"
<line24>    logger.debug(exception.getErrorMessage());
"<line2>      log.debug(""change max [active] semaphore with new permit {}"", maxActive);"
"<line8>      log.debug(""Enter MapDeserializer::startElement()"");<line14>      log.debug(""Exit: MapDeserializer::startElement()"");"
"<line4>    log.info(""Published an API {0}"", api);"
"<line7>        LOG.info(""Failure while notifying listener {}"", listener, x);"
"<line8>        LOG.warn(""The listenerContainer is not instantiated. Probably there was a timeout during the""+ "" Suspend operation. Please restart your consumer route."");"
"<line15>              logger.warn(""Error during doExceptionCaught service listener notifications:"", ex);"
"<line4>    logger.error(""{} {} An exception occured on {}: "",MessageEnum.RA_GENERAL_EXCEPTION_ARG,ErrorCode.DataError.getValue(),context,e);"
"<line12>      LOG.info(""Initializing service: {}"", serviceClassName);<line15>        LOG.error(""Failed to initialize service {}"", serviceClassName, t);<line17>      LOG.info(""Service initialized: {}"", serviceClassName);"
"<line29>          LOG.debug(""One instance of PlatformTransactionManager found in registry: {}"",transactionManager);<line34>            LOG.debug(""Creating a new temporary SpringTransactionPolicy using the""+ "" PlatformTransactionManager: {}"",transactionManager);"
"<line4>    log.debug(""generated url: "" + url);<line15>        log.error(""Calendar parsing exception: ""+ e.getCause().getMessage()+ "" from calendar at ""+ url);<line20>        log.debug(""Storing calendar cache, key:"" + intermediateCacheKey);<line23>        log.debug(""Retrieving calendar from cache, key:"" + intermediateCacheKey);<line31>      log.debug(""contentProcessor found "" + events.size() + "" events"");<line45>        log.debug(""Retrieving calendar event set from cache, key:"" + processorCacheKey);"
"<line25>          logger.debug(""Unexpected NPE cought. Please report stacktrace"", e);<line26>          logger.error(""An exception occurred while calling the DeviceStatusListener"", e);"
"<line2>    log.info(""getTrashHomeForLoggedInUser())"");<line2>    log.info(""for user:{}"", getIRODSAccount());<line4>    log.info(""getting file at:{}"", trashHomePath);"
"<line8>      logger.trace(""individualACK messageID="" + messageID);<line11>        logger.trace(""individualACK starting new TX"");<line19>        logger.trace(""ACKing ref "" + ref + "" on tx= "" + tx + "", consumer="" + this);"
"<line2>      LOG.warn(""Notebook authorization module was called without initialization,""+ "" initializing with default configuration"");"
"<line8>      LOGGER.debug(""Error occurred while reconsructing the object, "" + ex.getMessage());"
"<line7>      logger.error(""Creating the nonProxyHosts pattern failed for http.nonProxyHosts="" + nonProxyHosts, e);"
"<line2>    logger.debug(""Listener: Disconnected from the ambient weather service)"");"
"<line6>        log.error(""Listener "" + listener + "" has thrown an exception"", ex);"
"<line3>      logger.debug(""getRoleSets using rolesQuery: "" + rolesQuery + "", username: "" + username);<line11>        logger.debug(""Executing query: "" + rolesQuery + "", with username: "" + username);<line20>          logger.debug(""No roles found"");"
"<line12>      LOG.error(""error during save address: {}"", e.getLocalizedMessage(), e);"
<line11>      log.error(exception, exception);
"<line21>      logger.info(ONAPLogConstants.Markers.EXIT, ""Exiting"");<line22>      logger.warn(""Error in incoming SOAP Message Inteceptor"", e);"
"<line2>    this.logger.debug(""ActivityTestGPRSRequest"");"
"<line20>        logger.info(""{} bytes have been written."", written);"
"<line8>    logger.info(""Checking collection "" + collectionName + "" for "" + entityId.toString());<line13>      logger.info((i++) + "" "" + id.toString());<line14>    logger.info(""----------------------------------------------------"");"
"<line3>        LOG.warn(""Snapshot restore timed out, failed to restore snapshot for %s, snapshot %s"",queryId.getId(), lastTriedId.toString());"
"<line9>          log.debug(String.format(""Cannot retrieve document '%s', probably deleted in the meanwhile"", docId));<line15>        log.trace(""Not able to resolve blob"");<line19>          log.debug(String.format(""Not allowed to bulk download blob for document %s"", doc.getPathAsString()));<line25>      log.debug(""No blob to be zipped"");"
"<line1>    LOGGER.info(""Teardown Keycloak in namespace: {}"", namespace);"
"<line12>          LOGGER.debug(""Unable to load buffer pool MBeans, possibly running on Java 6"");"
"<line9>      logger.debug(""OVS Bridge destroyed"");<line11>      logger.warn(""caught execption when destroying ovs bridge"", e);"
"<line9>      LOG.debug(""Setting job conf credstore location to ""+ jobKeyStoreLocation+ "" previous location was ""+ oldKeyStoreLocation);"
"<line3>      log.warn(""[TaskTrackerActor] receive ServerStopInstanceReq({}) but system can't find TaskTracker."",req);"
"<line29>    LOGGER.info(""did update the localization for pkg {} for {} natural languages"",pkg.getName(),updatePkgLocalizationRequest.pkgLocalizations.size());"
"<line12>      log.info(""Initialize zookeeper metadata driver with external zookeeper client : ledgersRootPath =""+ "" {}."",ledgersRootPath);<line19>        log.error(""Failed to retrieve metadata service uri from configuration"", e);<line31>      log.info(""Initialize zookeeper metadata driver at metadata service uri {} :""+ "" zkServers = {}, ledgersRootPath = {}."",metadataServiceUriStr,zkServers,ledgersRootPath);<line49>        log.error(""Failed to create zookeeper client to {}"", zkServers, e);"
"<line17>      logger.debug(""Updating client id since the received value is null (new value {})"", clientId);"
"<line4>        log.info(""Iteration: "" + (i + 1) + '/' + ITERATIONS);"
"<line1>    log.debug(""Starting Producing Stream Data In Json Format Thread ..."");<line13>    log.debug(""Ending Producing Stream Data Thread ..."");"
"<line1>    log.debug(""merging SysImport instance"");<line3>      log.debug(""merge successful"");<line5>      log.error(""merge failed"", re);"
"<line5>      log.info(""Exception caught at fault barrier while generating jobs."", e);"
"<line12>        LOGGER.info(inputPath);<line18>        LOGGER.info(outputPath);<line23>      LOGGER.error(""Error:"");<line23>      LOGGER.error(exp.getMessage());"
"<line4>        logger.debug(String.format(""BroadcastServiceHandler: caught exception %s, probably because session was closed""+ "" with pending writes"",cause));"
"<line2>    log.info(""download="" + this.downloadFilename + "" and id="" + id);<line8>    log.error(""bundleResponse not found for id="" + id);"
"<line5>        logger.warn(""Could not delete {} from workspace: {}"", url, e.getMessage());"
"<line2>    LOGGER.info("".\\ Available Consumer List \\.______________________________"");<line5>      LOGGER.info(""  {} : {} ({})"", consumerHint, consumer.getDescription(), consumer.getClass().getName());"
"<line3>    LOG.trace(""Attempted write and flush of buffer: {}"", output);"
"<line28>      LOG.error(""Error while converting the received string payload to byte[]."", e);<line33>        LOG.error(String.format(""Failed to publish the message to [topic] %s. Error: %s. Sequence Number "" + "": %d"",topic, e.getMessage(), kafkaSinkState.lastSentSequenceNo.get() - 1),e);"
"<line7>      LOGGER.debug(""Calling before() for '{}' in processor '{}'..."",interceptor,component.getLocation().getLocation());"
"<line3>      LOGGER.error(""No attachment given!"");<line7>      LOGGER.error(""Invalid attachment type: ""+ attachment.getAttachmentType()+ "". Expected CLEARING_REPORT(""+ AttachmentType.CLEARING_REPORT.getValue()+ "") or COMPONENT_LICENSE_INFO_XML(""+ AttachmentType.COMPONENT_LICENSE_INFO_XML.getValue()+ ""."");<line10>      LOGGER.info(""Attachment with content id ""+ attachment.getAttachmentContentId()+ "" is of correct type to be displayed as clearing report, but is not yet accepted.""+ "" So not dispaying it."");"
"<line6>          LOGGER.debug(""Unable to find bundle {} of app {} in system."", loc, name);"
"<line7>              LOG.info(""Node {} has been removed"", node.getNodeId().getValue());<line8>              LOG.warn(""Failed to remove node {}"", node.getNodeId().getValue());<line13>            LOG.warn(""Exception thrown when removing node... {}"", throwable);"
"<line5>          log.debug(""Can reuse existing instance (dynamic)"");<line15>                log.debug(""Can reuse existing instance (BeanValidatorForm)"");<line24>            log.debug(""Can reuse existing instance (non-dynamic)"");<line27>          log.debug(""Error testing existing instance for reusability; just create a new instance"", e);"
"<line9>      logger.error(e.toString() + "" parse "" + this);"
"<line2>    log.info(""[TRACKER] tenantId "" + tenantId + "" removed."");"
"<line12>          LOG.error(""While shutting down"", t);"
"<line7>    LOGGER.trace(""'{}' wrote '{}'."", key, value);"
"<line6>      LOG.debug(""NULL or empty parameter passed to CEF parser function. Not evaluating."");<line8>    LOG.debug(""Running CEF parser for [{}]."", cef);<line12>      LOG.error(""Error while parsing CEF message: {}"", cef, e);"
"<line35>                LOGGER.info(""Record from list failed transformation"", e);<line37>                LOGGER.info(CommonStringValues.EUROPEANA_ID_CREATOR_INITIALIZATION_FAILED, e);"
"<line8>      LOG.error(""While deleting {}"", model.getObject().getKey(), e);"
"<line5>      LOG.trace(""{}: receive {}"", getName(), reply);"
"<line4>    log.debug(""Recommending items for user ID '{}'"", userID);<line12>    log.debug(""Recommendations are: {}"", topItems);"
"<line3>      log.debug(""Missing resource in payload. [resource=({}), issuer=({}), "" + ""messageId=({})]"",affectedResource,issuerConnector,messageId);"
"<line9>        LOGGER.debug(format(""Adding Partition %s/%s"", metadata.topic(), part.partitionId()));<line11>          LOGGER.warn(format(""No leader for partition %s/%s found!"", metadata.topic(), part.partitionId()));"
"<line7>        logger.info(""Execute Service permitted commands: {}"", commandString);"
"<line2>    LOG.info(""This node elected Active Cluster Coordinator"");"
"<line14>      LOG.debug(""[testGetDriveUser] Testing User properties: {}."", user);<line17>      LOG.error(""[testGetDriveUser] Execution error: {}."", ee.getMessage());<line19>      LOG.error(""[testGetDriveUser] Operation Timeout."");"
<line8>      log.error(e.getMessage(), e);
"<line2>    LOGGER.debug(prefix + ""Operator: MProjectionOperator"");<line2>    LOGGER.debug(prefix + ""Argument 0: DocIdSet - "");<line5>      LOGGER.debug(prefix + ""Argument "" + (i + 1) + "": DataSourceOperator"");"
"<line8>        logger.error(""TableManagerImpl.userQuitTournamentSubTables table == null - userId "" + userId);"
"<line2>    logger.info(""{}"", Thread.currentThread().getStackTrace()[1].getMethodName());"
"<line6>      LOGGER.warn(""failed in parseMap: "" + ex.getMessage());"
<line9>        LOG.error(e);
"<line23>        logger.info(String.format(""Failed to soft-delete %s (transition from %s to %s): item is referenced, and will""+ "" be deprecated instead"",localItemCsid, localItemWorkflowState, sasWorkflowState));"
"<line6>      logger.error(""Error loading bpmWidgetInfo with id '{}'"", id, t);"
"<line2>    log.debug(""Getting client certificates for consumer: {}"", consumerUuid);"
"<line5>        logger.info(""DOCID: "" + d.getProperty(""docno""));"
"<line12>      logger.error(""Error occured while generating JSON!"");"
"<line1>    log.info(""Stopping iptables for {} at {}"", entity(), machine);"
"<line2>    logger.debug(""Returning XAResource [null]..."");"
"<line13>      logger.warn(""error calling {}.{}()"", method.getDeclaringClass().getName(), method.getName(), t);"
"<line1>    LOG.info(""default locale: "" + Locale.getDefault());<line2>    LOG.info(""Set default locale to: "" + locale);<line2>    LOG.info(""Messages file: "" + filename);"
"<line2>    logger.debug(""createWorklog started..."");<line11>      logger.debug(ex.getMessage(), ex);"
"<line7>        LOGGER.info(""Searching in {} ..."", download);<line12>          LOGGER.info(""...finished"");"
"<line11>        logger.debug(""getNextBatch returns with a buffer of "" + entries.length + "" entries."");"
"<line8>    LOG.debug(""Ask for '{}({})'"", typeRef.getSimpleName(), key);<line20>      LOG.debug(""Object '{}({})' has been deleted"", typeRef.getSimpleName(), uuid);"
"<line1>    log.info(""Query is Activity tab displayed"");"
<line12>      log.error(exception, exception);
"<line1>    log.info(""Pausing queue {}"", queueName);"
"<line2>    logger.info(""Accepting document: "" + document.getID());"
"<line8>    LOGGER.info(""ALTER charset execute result: {}"", result);"
"<line1>    log.debug(""Sending message to user "" + user);"
"<line11>      LOG.error(""Error while setting proxy."", t);"
<line8>      log.error(exception, exception);
"<line3>      LOG.error(""Transaction was already closed!"", new Throwable());<line13>          LOG.error(""Transaction was not closed, rolling back. Please add an explicit rollback so that we""+ "" know this was not a missing success()"");"
"<line4>        LOGGER.warn(""Element definition contains a reserved property name {}. ""+ ""This may prevent some analytics from being used on this graph."",property);"
"<line11>        logger.debug(testName+ "": Created an AccountRole instance for account with knownResourceId=""+ knownResourceId);"
"<line19>      LOG.error(""Error reading metadata properties"", ex);"
"<line8>      LOGGER.warn(""Failed to dispose: "" + encoder.getClass().getName() + "" ("" + encoder + ')');"
"<line20>      LOGGER.error(""ActivityMetaDataDao - getQuestionnaireFrequencyDetailsForOneTime() :: ERROR"", e);"
<line9>    logger.trace(format, object1, object2, object3);
"<line2>    LOG.debug(""Channel connected {}"", e);"
"<line3>    LOGGER.debug(""[lastVal={}, curVal={}]"", lastVal, curVal);"
<line5>      logger.error(e.getMessage(), e);
"<line4>      log.info(""servlet context provided s3.user="" + user);<line13>        log.info(""servlet context provided config bucketName="" + bucketName);<line15>        log.info(""servlet context missing bucketName, using "" + getBucketName());"
"<line5>        LOG.debug(""appending ["" + warnings.size() + ""] warning(s)"");"
"<line6>      logger.error(""error in search groups"", t);"
"<line1>    LOG.error(""Error processing auth message, erroring connection {}"", errorCode);"
"<line2>    log.info(""Pending slot request [{}] timed out."", slotRequestId);"
<line27>      log.error(e.getMessage(), e);
"<line8>      LOGGER.error(""Exception launching activities to executes"", t);"
<line39>      log.error(systemException, systemException);
"<line13>        this.logger.info(""(Re)building the automaton from '"" + this.grammarSource.getURI() + ""'"");<line23>        this.logger.info(""Store automaton into store for '"" + this.grammarSource.getURI() + ""'"");<line27>        this.logger.info(""Getting automaton from store for '"" + this.grammarSource.getURI() + ""'"");"
"<line11>            log.info(""Stopping node "" + idx);"
"<line1>    LOGGER.info(""Sending message to the da requests queue"");"
"<line13>      LOG.debug(""Switch deduction off for the requirement."");<line20>      LOG.debug(""Change the safeguard implementation status to: "" + ImplementationStatus.PARTIALLY);"
"<line12>                    logger.warn(""Exception at getOAuthRequestTokenAsync"", e);"
"<line7>      logger.debug(""SSE stream ended. Closing stream."");<line11>        logger.warn(""SSE connection failed unexpectedly: {}"", exception.getMessage());<line14>    logger.debug(""SSE stream closed."");"
"<line10>    log.debug(""put {} index mapping finished, isAcknowledged: {}"", indexName, response.isAcknowledged());"
"<line11>        LOG.debug(taskName + "" interrupted while waiting for the writer thread to die"", e);"
"<line5>      LOGGER.error(""Naming health check fail."", e);"
"<line19>              log.debug(""Failed to parse META-INF/versions entry"", ex);"
"<line5>      logger.debug(""Failed to delete storage pool"", e);"
"<line2>      logger.info(""Updating {}..."", properties.get(ConfigurationService.KURA_SERVICE_PID));<line13>            logger.info(""Password is not encrypted"");"
"<line17>      LOG.error(""Unable to retrieve job details for ""+ context.getWorkflowId()+ "" on cluster ""+ context.getClusterName(),e);"
"<line6>      logger.warn(""Cannot store an email in the Sent folder, the collection was not found: {}"",e.getMessage());<line7>      logger.error(""Cannot store an email in the Sent folder"", t);"
"<line15>      LOGGER.error(""Exception writing to internal frame buffer"", ex);"
"<line46>        LOGGER.trace(""ignore"", ex);<line47>        LOGGER.error(""SemVer comparison error: left:\""{}\"", right:\""{}\"""", left, right);<line47>        LOGGER.debug(""SemVer comparison resulted in NPE"", ex);"
"<line12>      LOGGER.warn(""unhandled object to calculate size for: ""+ value.getClass().getName()+ "", defaulting to 100"");"
"<line1>    LOGGER.info(""Configure Paragraph for user {}"", user);<line2>      LOGGER.warn(""{} is trying to update paragraph {} of note {} with empty config"",user,p.getId(),p.getNote().getId());"
"<line4>        LOGGER.warn(""[capacityManagement] tenant content is over maxSize, tenant: {}, maxSize: {},""+ "" currentSize: {}"",tenant,maxSize,currentSize);<line5>        LOGGER.warn(""[capacityManagement] group content is over maxSize, group: {}, maxSize: {},""+ "" currentSize: {}"",group,maxSize,currentSize);"
"<line7>                LOG.info(""Schedule reconnect after {} millis"", initialReconnectDelay);<line17>                    LOG.info(""Trying to reconnect to {} - attempt #{}"",getEndpoint().getConnectionString(),attempt);<line20>                    LOG.warn(""Failed to reconnect to {}"", getEndpoint().getConnectionString());<line28>                  LOG.info(""Reconnected to {}"", getEndpoint().getConnectionString());"
"<line9>        logger.error(""Fatal error! Could not get a resource from the pool."");<line12>        logger.info(""Allocating jedis resource to caller: "" + subscriberJedis);"
<line27>      log.error(exception, exception);
"<line26>    log.trace(""Started getAllPhasingOnlyControls : \t indexBeanParam={}"", indexBeanParam);<line32>    log.trace(""getAllPhasingOnlyControls result: {}"", dto);"
"<line3>    logger.debug(""Registered 'script' configuration parser"");"
"<line5>    LOG.debug(""<<<< {}"", endpoint);"
"<line5>      LOG.debug(""Ignored error: "", e);"
"<line25>          LOG.warn(""Deadlock detected, retrying"", exception);"
"<line11>    log.trace(""Downloaded external keys from "" + jwksUri + "", keys: "" + keys);"
"<line12>      LOGGER.info(""Posting message '"" + message + ""'"");<line25>                      LOGGER.info(""Client OnMessage called '"" + message + ""'"");<line43>            LOGGER.info(""Client OnClose called '"" + closeReason + ""'"");<line47>            LOGGER.info(""Client OnError called '"" + thr + ""'"");"
"<line6>        logger.warn(""??"");"
"<line3>    log.debug(""File uploaded"");<line9>    log.debug(""Request 1 Passed"");<line12>    log.debug(""Request 2 Passed"");<line15>    log.debug(""Request 3 Passed"");"
"<line3>      LOGGER.debug(format(""unscheduling run once job: "" + LOG_SUBJECT_EXTERNAL_ID, subject, externalId));"
"<line6>          LOG.trace(""Removing artifact {}"", artifact);"
"<line2>    LOGGER.info(""Properties: {}"", getProperties());"
"<line38>        logger.error(""datenLesen"", ex);"
"<line1>    log.info(""Database contents will be completely deleted"");<line4>    log.info(""Database contents was completely deleted"");"
"<line2>      log.info(""Scheduling connection retries."");<line4>      log.info(""Monitor already running."");"
"<line5>      logger.error(""Can't find Ignite JDBC driver"", e);<line9>      logger.info(""connect to "" + getProperty(IGNITE_JDBC_URL));<line11>      logger.info(""Successfully created JDBC connection"");<line12>      logger.error(""Can't open connection: "", e);"
"<line4>      log.info(""servlet context provided s3.user="" + user);<line13>        log.info(""servlet context provided bucketName="" + bucketName);<line15>        log.info(""servlet context missing bucketName, using "" + getBucketName());"
"<line12>      LOG.debug(""Generating a new ephemeral 'remember me' key in a secure way."");<line14>      LOG.warn(""Using a fixed 'remember me' key because we're in development mode, this is INSECURE."");<line16>      LOG.info(""Using a fixed 'remember me' key from system properties, this is insecure."");"
"<line7>      LOG.debug(""Updating entity reference {} for reference attribute {}"", attributeDef.getName());"
"<line3>    LOGGER.debug(""Check if the IP: "" + ip + "" is free. The vcpeID: "" + vcpeId);"
"<line7>      LOG.warn(""Final reporting of metrics failed."", e);<line13>            LOG.warn(""ScheduledExecutorService did not terminate."");"
"<line2>    logger.info(""Obtained session"");"
"<line27>          logger.info(""Completed {} comprised of {} steps in {} millis. Index found {} hits. Read {} events""+ "" from Event Files."",query,numSteps,queryTime,hitCount,matchingRecords.size());<line28>          logger.info(""Completed {} comprised of {} steps in {} millis. Index found {} hits. Read {} events""+ "" from Event Files. Only completed {} steps because the maximum number of""+ "" results was reached."",query,numSteps,queryTime,hitCount,matchingRecords.size(),numCompletedSteps);"
"<line6>      logger.info(toString() + "" is at: "" + received);"
"<line1>    logger.info(""Task failed callback for taskId: "" + tg.getTaskId());"
"<line10>          log.warn(""Unable to create the partitioner for ""+ tableName+ "" despite its configuration.""+ ""Will use the default partitioner for this table."",e);"
"<line1>    log.debug(""No user and password set for ECR, checking EC2 instance role"");<line14>          log.debug(""No instance role found, return code was %d"",response.getStatusLine().getStatusCode());<line20>      log.debug(""Found instance role %s, getting temporary security credentials"", instanceRole);<line27>          log.debug(""No security credential found, return code was %d"",response.getStatusLine().getStatusCode());<line35>          log.debug(""Received temporary access key %s..."", user.substring(0, 8));"
<line7>      log.error(exception, exception);
"<line3>    LOGGER.trace(""Converting TieLine {}"", line.getId());"
"<line8>      log.error(""Could not adapt document model to relation resource ; ""+ ""check the service relation adapters configuration"");<line21>          log.debug(""quote removal succeded for id: "" + commentId);<line22>          log.error(""quote removal failed"", e);<line24>        log.warn(""quote/comment not found: id="" + commentId);"
"<line7>        logger.info(""No default Widget found for pagemodel '{}' of page '{}'"",model.getCode(),page.getCode());<line14>            logger.info(""Widget Type null when adding defaulWidget (of pagemodel '{}') on frame '{}' of""+ "" page '{}'"",model.getCode(),i,page.getCode());<line22>      logger.error(""Error setting default widget to page {}"", this.getPageCode(), t);"
"<line25>                log.info(""copying header text of "" + alert.getHeaderText().getTranslation(0));<line35>            log.error(""Unable to process service alert"", e);"
"<line17>      log.debug(""loading failed."");"
<line5>        log.debug(_NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey);
"<line1>    Freedomotic.logger.info(""Trying to connect to ethernet relay board on address "" + address + ':' + port);"
"<line7>      logger.error(""Error while delete on hbase for : "" + rowKey);"
"<line12>        logger.info(""Element {} from media package {} has already been removed or has never been""+ "" distributed to publication channel {}"",elementId,mediapackageId,channelId);<line14>      logger.debug(""Retracting element {} ({})"", element, elementFile);<line15>        logger.debug(""Unable to delete folder {}"", elementFile.getParentFile().getAbsolutePath());<line19>      logger.debug(""Finished retracting element {} of media package {} from publication channel {}"",elementId,mediapackageId,channelId);<line21>      logger.warn(""Error retracting element {} of media package {} from publication channel {}"",elementId,mediapackageId,channelId,e);"
<line7>          LOG.error(e);
<line17>        log.error(message, t);
"<line13>        logger.debug(""Ignoring unexpected update for id {}"", integrationId);<line15>      logger.debug(""Encountered number format exception while handling update for greenmode {}"",integrationId);"
"<line18>    log.info(""Creating HttpSimpleTable ..."");<line19>    log.info(""SERVER_PORT : "" + port);<line19>    log.info(""DATASET_DIR : "" + dataset);<line19>    log.info(""TIMESTAMP : "" + timestamp);<line19>    log.info(""------------------------------"");<line19>    log.info(""STS_VERSION : "" + STS_VERSION);"
"<line2>    logger.info(""Starting Source Task with properties {}"",StatelessKafkaConnectorUtil.getLoggableProperties(properties));"
"<line17>      this.logger.error(""Error while trying to retrieve matches for ""+ query+ "" ""+ ex.getClass().getName()+ "" ""+ ex.getMessage(),ex);"
"<line11>      LOG.error(""Create a specific Deployment require specify a Deployment name"");<line14>      LOG.error(""Create a specific Deployment require specify a namespace name"");<line18>      LOG.error(""Create a specific Deployment require specify a Deployment spec bean"");"
"<line2>    logger.info(""Setup begin: "" + this.getClass().getSimpleName());<line38>    logger.info(""CarbonStore created at location : "" + storePath);"
<line67>      log.error(systemException, systemException);
"<line7>    LOG.info(""Creating jar {} for job {}"", outputFile.getAbsolutePath(), jobName);<line19>    LOG.info(""Jar {} created successfully."", outputFile.getAbsolutePath());"
"<line2>      log.debug(""definitionsService or alertsEngine are not defined. Only valid for testing."");<line15>            log.debug(""Ignoring setFiring, loaded Trigger already in firing mode ""+ loadedTrigger.toString());"
"<line2>    LOG.debug(""{}: getSchemaSource for {} failed"", id, sourceIdentifier, throwable);"
"<line2>    LOG.debug(""SchemaTransaction remove vertex label '{}'"", id);"
"<line3>      LOGGER.warn(""The SingedCertificateTimestamp length shouldn't exceed 2 bytes as defined in RFC 6962. ""+ ""Length was ""+ message.getExtensionLength().getValue());<line5>    LOGGER.debug(""The context SignedCertificateTimestamp was set to ""+ ArrayConverter.bytesToHexString(message.getSignedTimestamp()));"
"<line11>        log.error(""Default user male portrait is not available"");<line14>      log.error(""Unable to configure the default user male portrait: "" + exception.getMessage());"
"<line11>    LOG.info(""ZeppelinHub REST API get note {} "", noteId);"
"<line12>        logger.error(""An error occurred while fetching method details"", exception);<line21>        logger.debug(""Trace sampling is true, Recording trace. methodInvoked:{}, remoteAddress:{}"",methodNameBuilder.toString(),remoteAddress);<line24>        logger.debug(""Trace sampling is false, Skip recording trace. methodInvoked:{}, remoteAddress:{}"",methodNameBuilder.toString(),remoteAddress);"
"<line10>      LOG.debug(""Internal storage content successfully exported"");<line11>      LOG.error(""While exporting internal storage content"", e);"
"<line11>          LOG.warn(""Unable to close {}"", serverChannel, e);"
"<line8>      LOG.debug(""Creating composite index for property {} of type {} and {}"",propertyKey.getName(),propertyClass.getName(),systemPropertyKey);<line20>      LOG.info(""Created composite index for property {} of type {} and {}"",propertyKey.getName(),propertyClass.getName(),systemPropertyKey);"
"<line3>      LOG.info(""reading checkpoint info for:"" + instant + "" key: "" + extraMetadataKey);"
"<line2>    LOG.info(""Terminating Persistence Processor..."");<line4>    LOG.info(""\tPersistence Processor Disruptor shutdown"");<line7>      LOG.info(""\tPersistence Processor Disruptor executor shutdown"");<line8>      LOG.error(""Interrupted whilst finishing Persistence Processor Disruptor executor"");<line10>    LOG.info(""Persistence Processor terminated"");"
"<line1>    logger.debug(""Updating GPIO Driver..."");<line1>    logger.debug(""Updating GPIO Driver... Done"");"
"<line7>      logger.error(""Error deleting user config record by id {}"", username, t);"
<line19>      log.error(systemException, systemException);
"<line25>      logger.error(""Error extracting services"", t);"
"<line16>      log.error(""Cannot add token to db"", e);"
"<line18>          LOG.debug(""Failed to process request to read entry at {}:{}. Too many pending requests"",r.ledgerId,r.entryId);"
"<line1>    logger.info(""Installing Kapua application broker plugin..."");<line4>      logger.error(""Error in plugin installation."", e);"
"<line6>      LOG.debug(""consumeAll {}"", this, x);<line10>      LOG.debug(""failed all content of http channel EOF={} {}"", atEof, this);"
"<line3>      log.warn(""Too many parameters"",""Function ""+ function+ "" has more than 254 in parameters. Skipping generation of convenience method."");"
"<line7>      log.debug(""Authentication using bearer token"");<line18>      log.debug(""Authenticating using client certificate"");<line22>      log.debug(""Found username {}, groups {}, extra {}"", userName, groups, extras);<line24>        log.debug(""Client certificates trusted... impersonating {}"", userName);<line30>        log.debug(""Peer certificate not valid, proceeding as anonymous"");"
"<line8>        LOG.info(""change the value"");<line12>        LOG.error(""Error: "" + e.getMessage(), e);"
"<line17>        logger.debug(""Published root WebApplicationContext as ServletContext attribute with name [""+ WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE+ ""]"");<line21>        logger.info(""Root WebApplicationContext: initialization completed in "" + elapsedTime + "" ms"");<line27>      logger.error(""Context initialization failed"", ex);"
"<line4>      log.info(""Starting remote app entries"");"
"<line2>    LOG.debug(""get("" + key + "","" + fields + "")"");"
"<line15>      logger.error(""Unexpected error"", t);"
"<line7>      log.error(""Error getting leaderboard: {} "", usecase);"
"<line29>      LOGGER.error(""Could not retrieve network interface: {}"", ex.getMessage(), ex);"
"<line3>    logger.debug(""Reader for TCP port {} starting..."", port);<line17>      logger.debug(""Reader for TCP port {} finished..."", port);"
"<line5>      log.info(""Trying request with url : "" + requestUrl);"
"<line11>          LOG.warn(""Going to buffer response body of large or unknown size. ""+ ""Using getResponseBodyAsStream instead is recommended."");<line12>        LOG.debug(""Buffering response body"");"
"<line14>      LOG.warn(""no committed message for topic {} partition {}"",topicPartition.getTopic(),topicPartition.getPartition());"
"<line2>      LOG.debug(""==> RangerValidator.getAccessTypes("" + serviceDef + "")"");<line5>      LOG.warn(""serviceDef passed in was null!"");<line6>      LOG.warn(""AccessTypeDef collection on serviceDef was null!"");<line9>          LOG.warn(""Access type def was null!"");<line12>            LOG.warn(""Access type def name was null/empty/blank!"");<line19>      LOG.debug(""<== RangerValidator.getAccessTypes("" + serviceDef + ""): "" + accessTypes);"
"<line2>    LOGGER.debug(""Adjusting EC public key in context"");"
"<line7>            log.info(""HealthServer started"");<line9>            log.error(""Error starting HealthServer"");"
"<line18>            log.warn(""Stale full copy {} found for volume {}"", fullCopyId, volume.getLabel());<line22>            log.warn(String.format(""skipping volume %s becuase fullCopySetName is null"",fullCopyVolume.getLabel()));"
"<line8>          log.warn(""Unable to parse date "" + text, exception);"
"<line7>    LOGGER.info(""Task object returned to client: {}"", task.toJsonString());"
<line4>    LOG.info(result);
"<line3>    logger.info(""query size: {}"", timeValues.size());"
"<line9>        LOG.trace(""Getting key by use: "" + use);<line11>            LOG.trace(""Found "" + key.getKid() + "", use: "" + use);<line17>        LOG.trace(""Use staticKid: "" + staticKid);<line25>      LOG.trace(""Try to re-load configuration due to keystore exception (it can be rotated)."");"
"<line22>      logger.debug(""startPlan started..."");<line23>      logger.debug(""Sending a message to start-session."");"
"<line1>    log.debug(""doModify start"");<line11>      log.warn(""UsersAction.doEdit: user not found: {}"", id);"
"<line13>        logger.debug(""Set AsyncContext {}"", asyncContext);"
"<line6>      LOGGER.trace("""", e);"
"<line7>      logger.error(""Interrupted"", e);"
"<line1>    logger.debug(""{} stopped. Will no longer distribute FlowFiles across the cluster"", this);"
"<line3>    LOGGER.debug(""Running jobs: {}"", runningJobs.keySet());"
<line5>      log.error(exception, exception);
"<line8>    logger.info(""Access URLs:\n""+ ""----------------------------------------------------------\n""+ ""\tLocal-API: \t\thttp://127.0.0.1:{}\n""+ ""\tExternal-API: \thttp://{}:{}\n""+ ""\tweb-URL: \t\thttp://127.0.0.1:{}/index.html\n""+ ""\t----------------------------------------------------------"",path,externalAPI,path,port);"
<line12>    logger.info(response);
"<line8>      LOGGER.error(""getFileDescriptor from FileImageOutputStream"", e);"
<line21>      log.error(systemException, systemException);
"<line15>              logger.debug(""delete uncomplated file {}"", newFile);<line18>              logger.error(""Create new TsFile {} failed because it exists"", newFile);<line26>            logger.error(""Create new TsFile {} failed "", newFile, e);"
"<line13>        log.warn(""Default credentials (jcifs.smb.client.username/password)""+ "" not specified. SMB signing may not work propertly.""+ ""  Skipping DC interrogation."");"
"<line6>        LOGGER.debug(String.format(""http.proxy service is upgrading session %s"", session));"
"<line5>      logger.warn(""Could not find existing MAC in {}. Generating new MAC. This will require re-pairing of""+ "" iOS devices."",storage.getClass().getName());"
"<line1>    LOGGER.info(MessageFormat.format(""Loading licenses into version home: {0}"", versionHome));"
"<line14>    logger.info("">> Registered new Image %s, waiting for it to become available."", newImageId);"
"<line12>      logger.debug(""Received exception {} when trying to parse: {}"",ex.getMessage(),serializedDistinctContinuationToken);"
"<line8>    LOG.info(""Found {} agents left in rack {}"", numInRack, lostAgent.getRackId());"
<line36>      logger.error(e.getMessage());
"<line7>        logger.warn(""failed to create config"");"
"<line19>      log.warn(""Got a request to the SAML Single Logout endpoint, ""+ ""with invalid request (XML is broken)"",e);"
"<line18>          LOG.warn(""You use localhost interface! It means that no external connections will be""+ "" available. Don't you want to use 0.0.0.0 instead (all network interfaces)?"");"
"<line1>    logger.info(""---------- Running H2 queries: "" + queryFolder);<line5>      logger.info(""Query Result from H2 - "" + queryName);"
"<line2>    LOGGER.debug(""ProtocolVersion: "" + ArrayConverter.bytesToHexString(msg.getProtocolVersion().getValue()));"
"<line4>      LOG.error(""[{}] Unexpected error while failing {}"", logPrefix, callback, throwable);"
"<line2>    LOG.debug("">>>BasicService.list(neutralQuery)"");"
"<line10>      log.warn(""Unable to load level.dat file, attempting to load backup."");"
"<line1>    LOG.debug(""Rebuilding cache of capacity for each RS"");<line9>      LOG.debug(sn.getHostname() + "" can hold "" + capacity + "" regions"");<line12>    LOG.info(""Cluster can hold ""+ this.cluster.numRegions+ ""/""+ this.totalCapacity+ "" regions (""+ Math.round(overallUsage * 100)+ ""%)"");<line13>      LOG.warn(""Cluster is overused, {}"", overallUsage);"
"<line4>      LOGGER.error(""IllegalStateException"");"
<line25>      log.error(exception, exception);
"<line13>      logger.warn(""Not a valid URI: {}"", service.configDescriptionURI);"
"<line7>    LOG.info(""entity -submit -type cluster -file "" + filePath);<line12>    LOG.info(""Submit datatsource entity {} via entity -submit -type datasource -file {}"",dsName,filePath);<line15>    LOG.info(""Submit import feed with datasource {} via entity -submitAndSchedule -type feed -file {}"",dsName,filePath);"
"<line2>    LOG.error(""Failure reading ActionDefinition {}"", id.getValue());"
"<line3>    logger.debug(""Set the selected object to: {}"", selObj);"
"<line8>      log.warn(""Error cleaning up origin for repository '{}': {}"", repository.getAlias(), e);"
"<line2>    log.debug(""Constructing Harveststatus based on given query."");<line7>      log.debug(""Unpopulated query is {}."", harvestStatusQueryBuilder);<line8>      log.debug(""Query is {}."", s);<line14>      log.debug(""Harveststatus constructed based on given query."");<line20>      log.warn(message, e);"
"<line18>          LOGGER.trace(""Ignorable error"", ex);"
"<line12>        log.error(""Error"", e);"
"<line6>      log.info(""Finished notifying {} messages in {}"", topicMessages.size(), stopwatch);"
"<line6>      logger.debug(""LSHIL {}"", ex.getLocalizedMessage());"
"<line5>    logger.debug(""*** Checking for perfect configuration ***"");<line6>      logger.debug(""Checking configuration of atom "" + ac.indexOf(atom));<line6>      logger.debug(""Atom has bondOrderSum = "" + bondOrderSum);<line6>      logger.debug(""Atom has max = "" + bondOrderSum);<line12>          logger.debug(""Atom "" + ac.indexOf(atom) + "" has perfect configuration"");<line18>      logger.debug(""*** Atom "" + ac.indexOf(atom) + "" has imperfect configuration ***"");"
"<line11>      LOGGER.error(""run() exiting due to uncaught error"", t);"
<line17>      log.error(systemException, systemException);
"<line49>            logger.debug(""The SQL to execute in beeline: {} \n"", hql);"
"<line20>          logger.error(""class: {}, invokeInfo: {}"", clazz.getName(), invokeInfo, e);"
"<line3>    logger.info(""Created total {} connections with {} deployed clients, waiting {} s for system to react"",getConnections(),getClients(),sleepMs / 1000);<line3>    logger.info(""#######################################"");"
"<line26>      logger.info(""Located function "" + function.getFunctionDefinition());"
"<line5>      log.info(""FetchResultSet: session:{} query:{}"", sessionHandle, queryHandle);"
"<line13>          this.logger.error(ERROR_MESSAGE_DATA_IN_MEMORY_IN_WRONG_FORMAT);<line18>      this.logger.error(""Failed to save global qualifiers data: {}"", ex.getMessage(), ex);"
"<line6>      LOG.fatal(""Can't access to TaskResult at "" + TaskRunner.class.getName() + ""!"");"
<line19>        log.debug(exception, exception);
"<line5>        log.info(""jdbc sink task connection is closed."");<line7>      log.warn(""sink task stop error while closing connection to {}"", ""jdbc"", e);"
<line20>          logger.error(ex.getMessage(), ex);
"<line11>        log.debug(""Response XML\n"" + xml);<line21>          log.warn(exception, exception);<line26>      log.debug(""No resource found for "" + storage.getRootPath() + webDAVRequest.getPath());"
<line16>        log.warn(exception, exception);
"<line2>    logger.debug(""isSameFile({},{})"", a, b);"
"<line5>      log.debug(String.format(""Send command %s"", command));<line49>            log.info(String.format(""Command %s returned no errors"", command));"
<line7>        log.warn(noSuchFolderException, noSuchFolderException);
"<line10>      log.error(""Can't write to a pin in input mode. Change direction to OUTPUT ({}) with pinMode first."",OUTPUT);"
"<line9>    LOG.info(""Deleting temporary table with query results {}"", tableToRemove);<line12>      LOG.info(""Deleting temporary dataset with query results {}"", tableToRemove.getDatasetId());"
"<line9>      log.error(StringBundler.concat(""Unable to get OpenId configuration for company "",companyId,"": "",configurationException.getMessage()),configurationException);"
"<line9>        LOG.error(WAIT_QUEUE + "" does not exist."");"
"<line5>      logger.error(""Error occurred while running DataCleaner command line mode"", e);"
"<line9>        LOG.debug(""Certificate is not appropriate."" + UpdaterUtil.getStringRepresentation(certificate));"
"<line3>      LOG.debug(""sessionRemoved: ""+ session.getId()+ "" timedout:""+ timedout+ "" channels: ""+ channelsAsString(session.getSubscriptions()));"
"<line2>    logger.error(""Exception in API http handler"", cause);"
"<line1>    logger.trace(""starting modem database download"");"
"<line12>      LOG.debug(""Plugin metadata back-fill was completed during a previous startup, skipping back-fill""+ "" this time."");"
"<line3>      LOG.warn(""Ignoring deprecated config value(s) on ""+ obj+ "" because contains value for ""+ ""'""+ key.getName()+ ""', other deprecated name(s) present were: ""+ deprecatedValues.keySet());<line4>      LOG.warn(""Using deprecated config value on ""+ obj+ "", should use '""+ key.getName()+ ""', but used ""+ ""'""+ Iterables.getOnlyElement(deprecatedValues.keySet())+ ""'"");<line5>      LOG.warn(""Using deprecated config value on ""+ obj+ "", should use '""+ key.getName()+ ""', but used ""+ ""'""+ Iterables.get(deprecatedValues.keySet(), 1)+ ""' and ignored values present for other ""+ ""deprecated name(s) ""+ Iterables.skip(deprecatedValues.keySet(), 1));"
<line5>      logger.warn(null, ex);
"<line8>      LOG.error(""Group {} on node {} does not exist"", groupId, nodeId);"
"<line15>      logger.error(""Error during refres pages"", t);"
"<line1>    LOGGER.debug(""Setting sortByRelevanceFeatureProperty to: {}"", relevanceFeatureProperty);"
<line9>          log.debug(_NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey);
"<line18>        LOGGER.debug(""Applied feature actions to "" + ids.size() + "" elements in "" + delta + "" Î¼s"");"
"<line2>      log.warn("""" + this + "" call to unmanage null location; skipping"",new IllegalStateException(""source of null unmanagement call to "" + this));<line5>      log.warn(""{} call to stop management of unknown location (already unmanaged?) {}; skipping, and""+ "" all descendants"",this,loc);"
"<line1>    log.debug(""persisting StgMbZielobjSubtypTxt instance"");<line3>      log.debug(""persist successful"");<line4>      log.error(""persist failed"", re);"
"<line30>        logger.warn(""unable to discover the ClusterManagementService on locator "" + locator.toString());"
"<line15>        log.debug(""Login: adding login name to shared state."");"
"<line14>          log.debug(""No node found for nodeId: ""+ nodeId+ "", handling of single message will be stopped: ""+ msg);"
"<line8>      log.debug(""Unexpected node type of .tokens node {}."", nt);"
"<line1>    log.debug(""persisting StgMbBauMasGef instance"");<line3>      log.debug(""persist successful"");<line4>      log.error(""persist failed"", re);"
<line31>            log.info(clname);
"<line10>      LOG.info(""Reading snapshot {}"", snap);<line21>        LOG.warn(""problem reading snap file {}"", snap, e);"
"<line4>        log.debug(""Committing JDBC Connection ["" + connection + ""]"");"
"<line1>    log.info(""Administration: Database dump."");"
"<line9>    LOG.debug(""Got PullRequest ""+ pullRequest.getHtmlUrl()+ "" [""+ pullRequest.getTitle()+ ""] From ""+ pullRequestUser.getLogin());"
"<line11>    LOG.info(""Fetched {} Jira tickets using query - {}"", issues.size(), jiraQuery.toString());"
"<line4>        logger.warn(""no record found with id {}"", recordId);<line18>      logger.error(""error in delete comment for id {}"", recordId, t);"
"<line14>      LOG.error(errMsg);<line21>      LOG.error(""Retrieving COS key: [{}] with byteRangeStart: [{}] "" + ""occurs an exception: [{}]."",key,byteRangeStart,e);"
"<line2>    log.debug(""Application Level throttle decision is {} for key:tier {}:{}"",decision.isThrottled(),throttleKey,tier);"
"<line2>    LOG.info(""NoopTask.TaskSuspendHandler.send() invoked."");"
"<line2>    logger.debug(""[actionDone][already done]{}"", migrationState);"
"<line6>      logger.debug(""TX: Committing: {}"", txId);<line12>          logger.debug(""TX: found a previously committed transaction:{}"", txId);"
"<line1>    LOGGER.trace(""{} waiting for {}"", update, waitFor);<line2>    LOGGER.trace(""{} stopped waiting for {}"", update, waitFor);"
"<line7>    logger.info(""checkDetectedLanguageForQuestion: 0. {}"", detectedLanguage);<line17>        logger.info(""IGNORE (null expected): {}"", logmessage);<line18>        logger.debug(logmessage);<line19>        logger.warn(logmessage);<line22>      logger.warn(""no language detected for {}"", textualQuestion);<line23>      logger.warn(""many ({}) languages detected for {}"", detectedLanguages.size(), textualQuestion);"
"<line1>    log.debug(""merging StgNZielobjektRollen instance"");<line4>      log.debug(""merge successful"");<line6>      log.error(""merge failed"", re);"
"<line2>    logger.debug(""Getting bond for atoms: atom1="" + atom1, "" atom2="" + atom2);"
"<line6>      log.warn(""Missing request content in context, skipping stripWhitespaces"");"
"<line11>        log.error(""Unable to write portlet footer paths "" + portlet.getPortletId(), exception);"
"<line12>        LOG.warn(""Unexpected exception getting done (and non-error) task result for ""+ source+ "" ""+ getContextDescription(context)+ ""; continuing: ""+ e,e);<line15>        LOG.warn(""Intercepting and skipping request to serialize a Task""+ getContextDescription(context)+ "" (only logging this once): ""+ source);"
"<line2>    Log.debug(""Test"");"
"<line8>    log.info(""message size is "" + message.length());"
"<line5>      LOG.warn(""unexpected: taskid({}) missed."", taskid);"
"<line3>      logger.trace(""Running cleanupAnalysisSubmissions"");<line13>          logger.trace(""Attempting to clean up submission "" + submission);<line18>            logger.error(""Error cleaning submission "" + submission, e);"
"<line27>        LOG.info(projects.size() + "" projects: \n   "" + String.join(""\n   "", projectNameList.values()));"
"<line2>    LOGGER.info("" Log Artifacts in Experiment Negative test start................................"");<line47>      LOGGER.warn(""Error Code : "" + status.getCode() + "" Description : "" + status.getDescription());<line49>    LOGGER.info(""Log Artifacts in Experiment tags Negative test stop................................"");"
"<line27>      log.error(""Unable to get field values from dynamic data mapping form ""+ ""instance record ""+ ddmFormInstanceRecord.getFormInstanceRecordId(),portalException);"
"<line3>      LOG.info(""Skipping dryrun as directed by param in cli/RestApi."");<line8>        LOG.info(""Skipping dryrun as directed by Runtime properties."");<line20>        LOG.info(""dryRun with properties {}"", props);"
"<line26>      LOG.trace(""IGNORED"", e);"
<line36>      LOGGER.error(ex.getLocalizedMessage(), ex);
"<line9>      logger.info(""{} REQ {} {} {} {} {}"",reqId,ctx.request().remoteAddress(),tenant,ctx.request().method(),ctx.request().path(),mods);"
"<line13>      logger.info(""This is a pseudo-information message"");"
"<line20>      LOG.warn(""Can't update cache due to EventHub is too busy"");"
<line4>            logger.debug(msg);<line7>            logger.debug(msg, cause);<line10>            logger.debug(format, arguments);<line15>            logger.info(msg);<line18>            logger.info(msg, cause);<line21>            logger.info(format, arguments);<line26>            logger.warn(msg);<line29>            logger.warn(msg, cause);<line32>            logger.warn(format, arguments);<line37>            logger.error(msg);<line40>            logger.error(msg, cause);<line43>            logger.error(format, arguments);
"<line9>      LOGGER.warn(""Line '{}' of contingency '{}' not found"", element.getId(), contingency.getId());"
"<line21>    log.debug(""Fragments for type ""+ docTypeName+ "": ""+ getTypeFragments(docTypeName)+ "", prefetch: ""+ getTypePrefetchedFragments(docTypeName));"
"<line3>      LOG.debug(""==> mapStructValue({})"", ctx);<line35>      LOG.debug(""<== mapStructValue({})"", ctx);"
"<line3>    LOGGER.info(String.format(""**********DataSource %s has been closed,cost:%s ms.**********"", name, cost));"
"<line14>        logger.info(""Initializing syn1Neg..."");"
"<line45>        LOG.warn(""Exception raised while deleting directory {}"", stageDir, e);"
"<line2>      log.info(""Trying to cancel compilation due to user request"");<line5>      log.info(""Trying to cancel compilation due to server stopping"");<line9>        log.info(""Changes detected, cancelling build"");<line12>      log.info(""IO error occurred"", e);"
"<line9>      LOGGER.error(""The storage group of path {} doesn't exist."", path, e);"
<line18>      log.error(systemException, systemException);
<line21>      log.error(systemException, systemException);
<line7>        log.debug(restoreEntryException, restoreEntryException);
"<line8>          logger.info(""Skipping sequence from queue ""+ genTree.getChanges()+ "" because it contains NO sequence""+ seq);"
"<line1>    log.info(""Stopping task"");<line9>        log.warn(""Error while closing the {} dialect: "", dialect.name(), t);"
<line35>        log.trace(sb.toString());<line46>          log.debug(portalException, portalException);
"<line18>            log.warn(""Server started with errors"");<line29>            log.error(""Server stopped before it started!"");<line38>      log.warn(""Process is already running"");"
"<line2>    LOGGER.debug(""Destroying MessageListenerContainerRegistry"");<line3>    LOGGER.debug(""Destroying ConnectionFactoryRegistry"");"
"<line6>      logger.error(""Error Calling Workflow Engine"", e);"
"<line15>    logger.debug(""Source [{}] created. id={}, acces_token={}, key={}"",sourceName,map.get(""id""),map.get(""accessToken""),map.get(""key""));"
<line11>        log.debug(exception, exception);
"<line2>    LOGGER.info(""Waiting for a notification for at most {} milliseconds."",MAX_WAIT_FOR_UNKNOWN_NOTIFICATION);<line11>    LOGGER.info(""Received notification for correlation UID {} for type {} with result {}."",notification.getCorrelationUid(),notification.getNotificationType(),notification.getResult());"
"<line7>      logger.warn(""Not found request header"");<line12>      logger.warn(""SocketId not exist. header:{}"", header);<line24>      logger.warn(""Failed to update state. closeState:{} lifeCycle={} {}/{}"",closeState,pingSession,agentLifeCycleState,agentEventType,e);"
<line5>      log.error(exception, exception);
"<line35>        log.error(""fail getParameter   [param]""+ param+ ""   [type]""+ paramtypes.getName()+ ""   [value]""+ PropertyUtil.reflectionToString(value));"
"<line16>      logger.warn(String.format(""Could not get database product type: %s"", e.getMessage()));"
"<line9>      logger.error(""Error loading disabling codes from file {}"", disablingCodesFileName, t);"
"<line12>        LOG.warn(""mapSoftRefValue: Was expecting AtlasObjectId, but found: {}"",ctx.getValue().getClass());"
"<line11>    log.info(""Starting Tomcat port {} dir {}"", port, webappsPath);<line11>    log.info(""-----------------------------------------------------------------"");"
"<line9>    LOGGER.info(String.format(""By default unknown fields are analyzed so here '%s' is ANALYZED."", name));"
<line8>      log.error(exception, exception);
"<line5>          Logger.trace(String.format(""< %02x"", d & 0xff));"
"<line2>    logger.error(""Default rollback method invoked on error. Error Code: "" + error.getCode());"
"<line14>              Console.error(Console.MESSAGES.failed(Console.MESSAGES.flushConnectionsError(dsName)),response.getFailureDescription());<line15>              Log.info(""Successfully executed flush operation ':"" + flushOp + ""'"");<line15>              Console.info(Console.MESSAGES.successful(Console.MESSAGES.flushConnectionsSuccess(dsName)));"
"<line6>    LOG.trace(""on update: \n""+ "" old OfOverlayL3Context: {} \n""+ "" new OfOverlayL3Context: {} \n""+ "" rootIdentifier: {}"",rootNode.getDataBefore(),rootNode.getDataAfter(),rootIdentifier);<line7>      LOG.debug(""Cannot update location for L3EP {} because port-name is missing."",rootIdentifier.firstKeyOf(EndpointL3.class));<line10>      LOG.debug(""No need to update location for L3EP {} because port-name {} was not changed."",rootIdentifier.firstKeyOf(EndpointL3.class),oldPortName.getValue());"
"<line5>      LOG.error(""Transforming FHIR data resulted in exception: {}"", ex.getLocalizedMessage(), ex);"
"<line14>          logger.error(""Error load configuration  {} "", e.getMessage());"
"<line2>    LOG.debug(""space "" + event.getSpace().getDisplayName() + "" was removed!"");"
"<line1>    log.debug(""Processing node ("", node.toString(), "")"");<line21>      log.debug(""Adding parameter (name="", name, "",value="", node.asText(), "")"");"
"<line22>      log.error(""Invalid label Name "", e);"
"<line18>      LOG.debug(""Release Write {} lock on resource {} and {}"", resource.name, firstUser, secondUser);"
"<line47>                LOG.info(""inflight count: "" + coreQueue.getDeliveringCount());"
"<line3>      log.info(""here"");"
<line12>    logger.info(context_format,new Object[] {batchId, size, memsize, format.format(new Date()), startPosition, endPosition});
<line3>    LOGGER.info(String.format(Messages.Log.GETTING_INSTANCE_S, volumeOrder.getInstanceId()));
"<line2>    log.trace(""Aggiorno il batch "" + codBatch);<line3>      log.trace(""ClusterId non impostato. Gestione concorrenza non abilitata. Aggiornamento non""+ "" necessario"");<line17>        log.trace(""Aggiornato semaforo rosso per il batch ""+ codBatch+ "" inserito per il nodo ""+ GovpayConfig.getInstance().getClusterId()+ ""."");<line20>      log.error(""Errore nell'aggiornamento del semaforo di concorrenza per il batch "" + codBatch, e);<line31>          log.error(""Errore "" + e.getMessage(), e);"
"<line1>    log.info(""Entering: focus"");"
"<line2>    logger.debug(""Loading jars from Working Directory"");<line8>        logger.debug(""Deploying DeployedJar: {} from working directory"", deployedJar);<line17>      logger.error(e);"
"<line2>    LOGGER.info(""  test14ReadRead"");"
<line3>    LOGGER.info(String.format(Messages.Log.DELETING_INSTANCE_S, publicIpOrder.getInstanceId()));
"<line4>      logger.debug(""OpenTherm Gateway connector is already connecting ..."");<line8>      logger.debug(""Starting OpenTherm Gateway connector"");<line12>      logger.debug(""OpenTherm Gateway connector started"");"
"<line3>    LOG.info(""Received transaction end event, global tx id: {}"", request.getGlobalTxId());"
<line5>      LOGGER.warn(RelationalProviderI18n.threadAssociatedWithAnotherTransaction,Thread.currentThread().getName(),activeTx,id);
"<line15>        LOGGER.error(""Exception occurred while clearing assignments metadata cache..."", e);<line19>      LOGGER.error(""clearMetadataCache failed with error:"", exception);"
"<line2>    logger.debug(""generateConsumerInfo started..."");"
"<line8>      logger.info(String.format(""Took: %6.3f seconds"", (end - start) / NANOS));"
"<line4>      ActiveMQXARecoveryLogger.LOGGER.debug(""end "" + xaResource + "" xid "");"
"<line25>      logger.error(""Error while rendering content {}"", contentId, t);"
"<line14>      LOGGER.error(""Exception in getQualysDetail "", e);"
"<line2>      logger.debug(""Profile with id: \'"" + profileId + ""\' has been explicitly activated."");"
"<line27>      log.info(this, ""Will execute cmd: "" + cmd);"
"<line34>          logger.error(""Error creating table {}/{}"", this.getDatabaseName(), tableClassName, t);<line41>      logger.error(""Error on setup Database - {}"", this.getDatabaseName(), t);"
"<line10>        log.info(""Using "" + resolver + "" to resolve Keycloak configuration on a per-request basis."");<line23>      log.info(""Keycloak is using a per-deployment configuration loaded from: "" + keycloakConfigFile);"
"<line7>              logger.info(""shutdown "" + vm.getName() + "" guest OS failed, power off directly"");"
"<line4>      LOG.debug(""reload not forced and reload check disabled or check interval not yet elapsed"");"
"<line2>    LOGGER.info(""Consumer [{}] consume item [{}] produced by [{}]"", name, item.getId(), item.getProducer());"
"<line8>    logger.info(action + "" command is: "" + cmd);<line13>          logger.info(action + "" succeed."");<line16>        logger.warn(""Got exception when "" + action, e);<line23>        logger.info(""Interrupted when waiting for setup login tty, retry immediately..."");<line25>    logger.info(action + "" failed"");"
<line5>        logger.info(factory.loadPolicy(policy).toString());
<line1>    log.trace(XTCE_ARRAY_PARAMETER_TYPE);
"<line7>        Logger.error(this, ""%s"", ExceptionUtils.getStackTrace(ex));"
<line7>      LOGGER.info(format(Messages.JOB_WITH_ID_AND_TASK_NAME_EXPIRED,jobEntity.getProcessInstanceId(),jobEntity.getElementName()));
"<line10>      log.error(""Error while opening editor."", e);"
"<line8>      logger.debug(""Jumping to page "" + getPage() + "" and index "" + current);"
"<line8>          LOGGER.trace(""Checking tracker overlaps for ""+ tracker+ "" for ""+ list.size()+ "" other trackers"");<line15>                LOGGER.trace(""Slaves created for all satisfactions"");<line19>            LOGGER.trace(""Other tracker is done or equal: "" + other);<line23>          LOGGER.trace(""No other trackers to check overlaps with"");"
"<line2>    LOG.info(""Executing operation getConceptsInDataCluster"");"
"<line11>      LOG.debug(""redirecting to login page loginPath"" + loginPath);"
"<line4>    LOG.warn(""No instance for WebSocketServer found, creating one with a fallback port: {}"",fallbackPort);"
"<line9>    LOG.info(""Loading service {}"", name);"
"<line5>      LOG.info(""Failure reading with GML3 parser. Trying with GML2"");"
"<line5>    logger.debug(""Deleting an alert {}."", alert);"
"<line7>        logger.warn(""Failed to get default properties for component: {}"", pid, e);"
"<line5>      logger.error(""Failed to retrieve server id from KieServerState"", e);"
<line16>      LOG.error(msg, e);
"<line2>    logger.debug(""Disposing weatherunderground bridge handler."");"
"<line8>      log.error(""Exception in loginProxy: "" + exception.getMessage());"
"<line24>      logger.debug(""Couldn't restore item '{}' of type '{}' ~Â there is no appropriate ItemFactory""+ "" available."",itemName,persistedItem.itemType);"
"<line1>    log.info(""Checking broker infra"");"
"<line1>    LOG.info(""Executing operation queryEndpoints"");"
"<line4>      logger.debug(""Label annotation present with value: {}"", text);<line7>      logger.debug(""Setting label from property name: {}"", label);"
"<line2>    LOGGER.info(""Validating Index Document, indexName={}, id={}."", indexName, id);<line8>      LOGGER.error(""Caught IOException while attempting to use the ElasticsearchRestHighLevelClient."",ioException);"
"<line24>      log.info(""Connecting"");"
"<line10>      log.debug(""Monitoring thread was interrupted"", ex);"
"<line3>    LOG.info(""éç¥ç¢ºèª"");"
"<line8>      logger.error(""TTransportException writing to internal frame buffer"", e);<line10>      logger.error(""Exception writing to internal frame buffer"", e);"
"<line11>      LOG.error(""Error getting API Level by Service Name: {}"", ex.getLocalizedMessage(), ex);"
"<line23>      logger.debug(""Received get server templates"");<line24>      logger.debug(""Returning response for get server templates: {}"", response);<line31>      logger.error(""Get server templates failed due to {}"", e.getMessage(), e);"
"<line3>      LOGGER.warn(""Get data "" + dataId + "" is null."");"
"<line12>          log.error(""Invalid/unknown ND4J namespace provided: "" + s);<line18>          log.error(""Invalid/unknown SD namespace provided: "" + s);<line26>      log.info(""Starting generation of namespace: {}"", ns);<line35>        log.info(""Output path: {}"", outputPath.getAbsolutePath());<line50>    log.info(""Complete - generated {} namespaces"", cnt);"
"<line4>      LOG.info(""Looking for SANs in cert: "" + cert.getTBSCertificate().getSubject());"
"<line2>      LOG.debug(""Copying folder(id={}) to destination_folder(id={}) {}"",folderId,destinationFolderId,newName == null ? """" : "" with new name '"" + newName + ""'"");"
"<line17>        Log.warn(""Unable to parse back-expiry for cache: "" + cacheInfo.getCacheName());"
"<line2>      LOGGER.info(""Skipping start controller step. Assumes controller is already started"");"
"<line12>                  logger.info(""Truncating table {}..."", sqlTableName);<line14>                  logger.info(""Partially emptying table {}"", sqlTableName);<line26>      logger.error(""Error in truncating the table {}..."", sqlTableName, sqlException);"
"<line10>          LOG.info(""Registered RecordDao for data source: {}, record-dbName={}"",dsConfig.getId(),dsConfig.getRecordDbName().get());<line11>          LOG.info(""No record db configured for data source: {}"", dsConfig.getId());<line15>          LOG.info(""Registered RecordRedirectDao for data source: {}, redirect-dbName={}"",dsConfig.getId(),dsConfig.getRedirectDbName().get());<line16>          LOG.info(""No redirect db configured for data source: {}"", dsConfig.getId());"
<line1>    logger.info(constructFormatOrMsg(eventName, format), arg);
"<line8>      log.warn(""Found no data for job={} checkPath={}; returning zero"", jobId, checkPath);<line10>      log.warn(""Found multiple results for job={} checkPath={}; using first row"", jobId, checkPath);"
<line6>        log.error(e, e);
"<line6>        logger.warn(""Interrupted while waiting for event latch."", e);"
<line15>      LOG.error(e);
<line4>    logger.debug(ExceptionUtils.getStackTrace(ex));
"<line9>      log.info(""Starting batch layer"");<line11>      log.info(""Starting consumer thread"");<line14>      log.info(""Producing data"");"
"<line12>                  LOG.error(""task info deserialization failed "" + e);"
"<line14>      logger.warn("""", pex);"
"<line4>      logger.info(""Failed to ping stream, streamId={}, cause={}"", streamId, statusError.getMessage());<line5>      logger.info(""Failed to ping stream, streamId={}, cause={}"",streamId,statusError.getMessage(),statusError.getThrowable());"
"<line3>      log.warn(""Specified TLS cert '{}' doesn't exist!"", nodeProperties.nettyTlsCrtPath());<line7>      log.warn(""Specified TLS key '{}' doesn't exist!"", nodeProperties.nettyTlsKeyPath());"
"<line9>      log.error(""ack {}"", evt, e);"
"<line8>                log.info(""deploying verticle: {}"", verticle);<line12>              return CompositeFuture.all(futures);"
"<line8>      log.info(""Size {}: {} = {} % (diff: {}%)"", label, hist[i], format, error);"
"<line10>          log.debug(""Unable to parse JSON"", jsonException);"
"<line1>    logger.trace(""[{}] getSize() -> {}"", name, size);"
"<line24>                logger.error(""Error during remove repository"", e);"
"<line19>        logger.debug(""No ViewResolvers found in servlet '"" + getServletName() + ""': using default"");"
"<line10>      logger.error(""Error while updating the page metadata record for table {} and page {}"",PageMetadataDraft.TABLE_NAME,pageCode,t);"
"<line16>            log.debug(String.format(""Active instance is removed and added to the ""+ ""termination pending instance list. [Instance Id] %s"",instanceId));"
"<line18>          Log.warn(""Date not found. Date must be in format YYYY-MM-DD or YYYY-MM-DD:YYYY-MM-DD."", ex);<line50>      Log.error(e.getMessage(), e);"
"<line4>          logger.debug(""pruneQueueIfNeeded - (""+ attributeName+ "") queue entry (""+ str+ "" ""+ new java.util.Date(Long.parseLong((String) str)));<line8>        logger.debug(""pruneQueueIfNeeded - (""+ attributeName+ "") removed (""+ conversationId+ "" ""+ new java.util.Date(Long.parseLong(conversationId)));"
"<line9>          LOG.error(""Close channel for location "" + loc + "" error "", e);"
"<line15>      LOGGER.error(""error handling MQTT subscriptions"", ex);"
"<line21>          log.info(""Sending Map messages on '"" + topicName + ""' topic"");<line23>          log.info(""Sending  "" + format + "" messages on '"" + topicName + ""' topic"");<line26>        log.error(""Can not subscribe."" + e.getMessage(), e);<line33>      log.error(""Error when publishing messages"" + e.getMessage(), e);<line34>    log.info(""All Order Messages sent"");"
"<line28>      LOGGER.error(""Encrypt password in '"" + location + ""' error."", e);"
"<line19>        LOG.info(String.format(""psFunc: the best split after looping a split: fid[%d], fvalue[%d], loss gain[%f]""+ "", leftSumGrad[%f], leftSumHess[%f], rightSumGrad[%f], rightSumHess[%f]"",fid, splitIndex, lossGain, leftSumGrad, leftSumHess, rightSumGrad, rightSumHess));"
"<line22>      logger.error(""error in edit"", t);"
<line23>        log.debug(msg, e);<line26>        log.error(msg, e);
"<line2>    LOGGER.debug(""get system properties"");<line73>      LOGGER.warn(""Exception while reading the system.properties file."", e);"
"<line14>        log.error(""Could not delete release from DB"", e);"
"<line4>    LOG.warn(""Featured occurrences have been removed."");"
"<line17>    LOG.info(""InputParallelism: ${""+ inputParallelism+ ""}, IndexParallelism: ${""+ config.getBloomIndexParallelism()+ ""}"");"
"<line6>    logger.error(""Cassandra schema not installed, starting administration services only"");"
"<line10>    logger.debug(""Replacing current select statement with "" + select.toString());<line18>      logger.warn(""Syntax error with the MySQL FROM statement. Syntax permitted is FROM, from or From "");<line21>    logger.debug(""Final query with columns "" + queryString);"
"<line4>      logger.debug(""Unable to decrypt event because libsodium is not loaded"");<line7>      logger.info(""Received malformed version 1 doorbell event, length not 70 bytes"");<line16>      logger.trace(""Calling cryptoPwHash with passwordFirstFive='{}', opslimit={}, memlimit={}, salt='{}'"",password5,opslimit,memlimit,HexUtils.bytesToHex(salt, "" ""));<line26>      logger.info(""Got SodiumException"", e);<line37>    logger.trace(""Call cryptoAeadChaCha20Poly1305Decrypt with ciphertext='{}', nonce='{}', key='{}'"",HexUtils.bytesToHex(ciphertext, "" ""),HexUtils.bytesToHex(nonce, "" ""),HexUtils.bytesToHex(k, "" ""));<line40>      logger.trace(""Decryption FAILED"");<line44>      logger.info(""Length of decrypted text is invalid, must be 18 bytes"");<line46>    logger.debug(""Received and successfully decrypted a Doorbird event!!"");<line55>    logger.debug(""Event is eventId='{}', intercomId='{}', timestamp={}"",eventId,eventIntercomId,eventTimestamp);"
"<line1>    logger.info(""Start Sakuli-Test-Suite from folder \""""+ testSuite.getTestSuiteFolder().toAbsolutePath().toString()+ ""\"""");<line10>        logger.info(""Sahi-Script-Runner starts!\n"");<line12>        logger.info(""test suite '""+ testSuite.getId()+ ""' stopped at ""+ TestSuite.GUID_DATE_FORMATE.format(testSuite.getStopDate()));<line12>        logger.info(""Sahi-Script-Runner executed with "" + output);<line14>            logger.warn(""Sahi-Script-Runner timeout detected, start retry!"");<line28>      logger.info(""test suite finished"");"
"<line8>        this.logger.warn(""You're using the deprecated [{}] configuration property. You should instead use the ""+ ""newer [{}] one"",PROPERTY_DEPRECATED_PERMANENTDIRECTORY,""environment.permanentDirectory"");"
"<line3>      logger.debug(""Cache remove: "" + userCert.getSubjectDN());"
"<line3>      logger.trace(""addBooleanField fieldName: {}; value: {}"", fieldName, value);"
"<line11>      logger.error(""Some parser properties are not supported."");"
"<line10>      logger.error("""", fex);"
"<line5>      logger.error(""Exception in cache service: {} "", ex.getMessage());"
"<line3>      log.info(processDescription + "" is being shutdown"");<line4>      log.warn(processDescription + "": unexpected shutdown!"");"
"<line4>              LOG.warn(""Could not publish event to sns, will retry later ({})"", t.getMessage());<line7>                LOG.error(""Could not save update to zk for retry, dropping"", t2);"
"<line6>    LOGGER.info(""Activate organisation: {}."", request.getOrganisationIdentification());"
"<line3>    log.trace(""[{}][{}][{}] Processing change resource"",tenantId,resource.getResourceType(),resource.getResourceKey());"
"<line2>    Log.debug(""Test"");"
"<line2>    LOG.info(""Registering the trigger named \""{}\"""", getName());"
"<line12>        LOGGER.warn(String.format(Locale.ROOT,""Could not import: '%s' - invalid namespace: '%s'"",model,entry.getValue()));<line14>        LOGGER.info(String.format(Locale.ROOT, ""Importing: '%s' from '%s' namespace."", model, namespaceName));<line17>        LOGGER.info(String.format(Locale.ROOT, ""Mapping: '%s' class to '%s'"", model, modelClass));<line20>        LOGGER.info(String.format(Locale.ROOT,""Adding dependency: '%s', version: '%s', framework: '%s'"",assembly,assemblyVersion,assemblyVersion));"
"<line10>    final Errors error = leaveResponse.error();<line11>      log.debug(""LeaveGroup response with {} returned successfully: {}"", sentGeneration, response);<line13>      log.error(""LeaveGroup request with {} failed with error: {}"", sentGeneration, error.message());"
"<line12>    LOGGER.warn(""*         RUNNING CustomCRSKDERasterResizeIT    *"");<line12>    LOGGER.warn(""*                                               *"");<line12>    LOGGER.warn(""-------------------------------------------------"");<line15>      LOGGER.warn(""Unable to tear down default spark session"", e);"
"<line9>        logger.error(""Error while loading implementations of {}"",service.getName(),serviceConfigurationError);"
"<line43>        log.warn(""Reclaim max age parameter is less then 1, are your sure?"");<line45>        log.debug(""Using reclaim-aged strategy for group-window age ""+ reclaimMaxAge+ "" frequency ""+ reclaimFrequency);"
"<line17>    LOG.info(""Current Snapshot Index {}, takeSnapshot took {} ms"",lastAppliedIndex,Time.monotonicNow() - startTime);"
"<line10>        log.warn(""Cache Listener thread interrupted in MultiThreadedListener."", e);"
"<line6>        logger.warn(""Unknown pigpio exception while handling Refresh"", e);<line12>        logger.warn(""An error occured while changing the gpio value: {}"", e.getMessage());"
"<line15>              logger.warn(""calling property change listener {} failed. {}"", listener, re.getMessage());<line18>          logger.debug(""Ignoring received message of unknown type: {}"", message);<line21>      logger.warn(""received invalid message: {}"", message);"
"<line9>      log.warn(""Error checking string constraint "" + this, e);"
"<line2>      logger.trace(""addNULLField fieldValue: {}"", fieldValue);"
"<line28>            logger.debug(""ConfigurationPropertyAnalysis: whilst looking at type ""+ typesCollectedFromAnnotation+ "" making these accessible:""+ newMap.entrySet().stream().map(e -> ""\n"" + e.getKey() + "":"" + AccessBits.toString(e.getValue())).collect(Collectors.toList()));<line47>            logger.debug(""Couldn't resolve ""+ visibleAnnotation.desc+ "" annotation type whilst searching for hints on ""+ getName());"
"<line16>        logger.info(""{} {}: error: {}"", method, url, e.getMessage());"
"<line23>    LOGGER.debug(""Starting interactive table sorting on column(s) "" + sortColNames);<line40>      LOGGER.debug(""Interactive table sorting finished ("" + time + "")"");"
"<line13>          logger.info(""Outbound interface is NULL! Looks like there was no ""+ transport+ "" in the list of connectors"");<line14>          logger.info(""Outbound interface found: "" + result.toString());<line18>        logger.info(""ServletContext return null or empty list of connectors"");"
"<line52>      log.info(""Using version: "" + topVer);"
"<line10>      LOG.info(""Update rows {}"", count);"
"<line7>      LOG.error(""Connector not found {}"", resourceTO.getConnector(), e);"
"<line1>    log.info(""VLO monitor run - {}"", Calendar.getInstance().getTime());<line3>    log.info(""Loading previous stats"");<line9>        () -> log.info(""No previous state, skipping comparison!""));<line9>    log.info(""Writing new stats"");<line11>    log.info(""Done"");"
"<line2>      LOG.error(""Incoming RegisterDocumentSetRequestType was null"");<line5>      LOG.error(""Incoming RegisterDocumentSetRequestType metadata was null"");<line7>    LOG.debug(""List of All Identifiable Registry Object(s) on Identifiers Form Request {}"",request.getSubmitObjectsRequest().getRegistryObjectList().getIdentifiable());<line9>      LOG.debug(""Name of Identifiable Registry Object on Identifiers Form Request {}"",object.getIdentifiable().get(x).getName());<line12>        LOG.debug(""Slot(s) in registry Package is {}"", registryPackage.getSlot().size());"
"<line9>      log.info(""Value found in SystemSettings: dataValuePageSize: "" + dataValuesPageSize);"
"<line12>    LOGGER.info(""ServerProfile object returned to client : "" + serverProfileUpdated.toJsonString());"
"<line4>        log.debug(""updateProcessBusinessKeyInHistory : {}"", processInstance.getId());"
"<line2>    LOG.debug(""starting {}"", getClass().getSimpleName());"
"<line1>    LOGGER.debug("" flushPendingWriteRequests()"");<line6>      LOGGER.debug("" Flushing buffered write request: {}"", scheduledWrite.data);"
"<line10>            logger.error(""scan failed"", e);"
"<line14>      LOGGER.warn(""Unable to open jar file '{}'."", dependency.getFileName());<line14>      LOGGER.debug("""", ex);"
<line13>      logger.debug(marker, format, argArray);
"<line8>        LOGGER.info(""Loaded coverage store '""+ cs.getName()+ ""', ""+ (cs.isEnabled() ? ""enabled"" : ""disabled""));"
"<line11>            log.info(""Updating identity type {}, setting confirmationConfiguration to {}"",name,emailConfig.toJson());"
"<line3>      log.error(""processHandle is null when looking up elapsed time"");"
"<line2>      logger.info(""m_bluetoothAdapter.isScanning"");<line7>        logger.info(""startLeScan"");"
"<line4>      log.error(""Cannot save complex data where obsId=""+ obs.getObsId()+ "" because its ComplexData is null."");"
"<line1>    log.debug(""Fixing lateral caches:"");"
"<line6>    log.debug(""sparql-based matching for atom {} (found {} matches)"",atomURI,bulkHintEvent.getHintEvents().size());"
"<line14>      log.debug(""Loading artifact to the command object for editing."");"
"<line7>    LOGGER.info(""Discover Object Instance : ""+ ""Object = ""+ theObject.toString()+ "", Object class = ""+ theObjectClass.toString()+ "", Object name = ""+ objectName+ "", Producing federate = ""+ producingFederate.toString());"
<line2>    LOGGER.trace(ENTERING, methodName);<line2>    LOGGER.trace(EXITING, methodName);
"<line1>    logger.debug(""Network state ONLINE: Process running. {} Nodes in network."", networkNodes.size());<line11>      logger.debug(""Network state ONLINE: Notifying node {} [{}]"",node.getIeeeAddress(),String.format(""%04X"", node.getNetworkAddress()));"
<line10>      log.error(exception, exception);
"<line13>      log.debug(""Property 'username' was not found"");<line18>      log.debug(""Property 'password' was not found"");<line31>      log.debug(""One or more of 'host' or 'database' properties were not found"");"
"<line3>    LOGGER.debug(""Creating topic: ""+ topic+ "" , partitions: ""+ partitions+ "" , ""+ ""replication factor: ""+ replicationFactor+ ""."");"
"<line2>    log.debug(""Going to store validation result in key-value storage"");<line6>    log.debug(""invoked {} failed {}"", invoked, failed);"
"<line3>    logger.info(""Validating service name"");"
<line6>      log.error(exception, exception);
"<line18>            logger.warn("""", fex);"
"<line8>        logger.info(""Paused {}"", this);"
"<line6>        LOGGER.debug(""FÃ¼ge Textfragment von URL '{}' ein."", urlStr);"
"<line29>    logger.info(""Setting number of cooperative threads and default parallelism to ""+ config.getInstanceConfig().getCooperativeThreadCount());"
"<line4>      log.debug(""key:"" + entry.getKey() + "" value:"" + entry.getValue());"
<line26>          log.debug(exception, exception);
"<line8>      LOG.info(""save subrequest, complete number=""+ receivedSubResult+ "", total number=""+ subResults.size());<line16>            LOG.error(""Master combine model files failed "", e);<line20>          LOG.error(""PS save model failed. "" + failedMsg);"
"<line3>      ActiveMQRALogger.LOGGER.trace(""readShort()"");"
"<line8>      LOG.info(""zkclient{} watchForChilds path not existing:{} skipWatchingNodeNoteExist: {}"",_uid,path,skipWatchingNonExistNode);"
"<line10>      logger.debug(""Updating entity {}:{}  app {}\n"", entityId.getType(), entityId.getUuid(), appId);<line21>        logger.debug(""Wrote {}:{} version {}"",cpEntity.getId().getType(),cpEntity.getId().getUuid(),cpEntity.getVersion());<line24>        logger.trace(""WriteUniqueVerifyException encountered during update of entity with id {}"",cpEntity.getId().getUuid());"
"<line7>      LOGGER.error(""Cannot get refined schema for {}, {}"", resource, e.getMessage(), e);<line38>      LOGGER.debug(""Association for {}/{} not supported by resource {}"", kind, shadowIntent, resource);"
"<line30>      log.error(""Error writing memory crash dump information to disk: {}"", f.getAbsolutePath(), e2);<line31>    log.error("">>> Out of Memory Exception Detected. Memory crash dump written to: {}"",f.getAbsolutePath());<line31>    log.warn(""Memory crash dump reporting can be disabled with CrashUtil.crashDumpsEnabled(false) or""+ "" using system property -D""+ DL4JSystemProperties.CRASH_DUMP_ENABLED_PROPERTY+ ""=false"");<line31>    log.warn(""Memory crash dump reporting output location can be set with""+ "" CrashUtil.crashDumpOutputDirectory(File) or using system property -D""+ DL4JSystemProperties.CRASH_DUMP_OUTPUT_DIRECTORY_PROPERTY+ ""=<path>"");"
"<line37>      LOGGER.info(getClass().getSimpleName() + ""::onEntry,"" + infoLog);"
"<line2>    logger.trace(""Handling command '{}' for channel '{}'"", command, channelUID);<line3>      logger.debug(""Refreshing channel '{}'"", channelUID);<line28>                  logger.debug(""None of the configured GAs on channel '{}' could handle the command '{}' of""+ "" type '{}'"",channelUID,command,command.getClass().getSimpleName());"
"<line14>        log.warn(""Unable to get info item field values provider for class "" + className);"
"<line25>      log.info(""init()"");<line26>      log.warn(""init(): "", t);"
"<line4>        logger.info(""Shutting down the proxy..."");"
"<line2>      LOG.error(""invalid ledgerId {} < 0"", ledgerId);"
"<line1>    log.warn(""containsKill("" + file + "","" + access + "")"");<line6>          log.warn(""searching "" + lastEnd + "" to "" + len);<line12>                log.warn(""[warning] skipping search @ "" + size);<line15>                log.warn(""[warning] searching > 4k space @ "" + size);<line19>              log.warn(""scan of "" + LessBytes.toString(scan));<line25>        log.warn("""", ex);"
"<line17>            logger.debug(""Skipping value for "" + name);"
"<line7>                log.debug(String.format(""Incremented Redis key %s to %d"", sizeKey, incremented));"
"<line1>    LOGGER.trace(""NFSService - generatePresignedUrl called"");<line4>      LOGGER.trace(""NFSService - generatePresignedUrl - put url returned"");<line11>      LOGGER.trace(""NFSService - generatePresignedUrl - get url returned"");"
<line3>    LOGGER.info(Messages.Log.REQUESTING_INSTANCE_FROM_PROVIDER);
"<line8>      log.warn(""Exception occurred while shutting down HSQLDB :"" + StringUtils.stringifyException(ex));"
"<line10>          logger.info(""{} - Driver does not support get/set network timeout for connections. ({})"",poolName,e.getMessage());<line11>            logger.warn(""{} - A validationTimeout of less than 1 second cannot be honored on drivers""+ "" without setNetworkTimeout() support."",poolName);<line12>            logger.warn(""{} - A validationTimeout with fractional second granularity cannot be honored on""+ "" drivers without setNetworkTimeout() support."",poolName);"
"<line1>    logger.debug(""Bridge: Checking for valid Zoneminder host: {}"", host);"
"<line20>      log.info(""Trying {} additional authN option from policy, result: {}"", policyElement, option);<line23>      log.info(""Additional authn is required but no option was found, blocking operation"");"
"<line7>          LOG.warn(""Failure passing provided arguments (""+ getIllegalArgumentsErrorMessage(constructor, argValues)+ ""; ""+ e+ ""); attempting to reconstitute"");<line10>          LOG.warn(""Reconstitution attempt failed (will rethrow original excaption): "" + e2, e2);"
"<line22>          log.error(""caught exception closing InputStream: "" + e);"
"<line4>        logger.error(""partial reversed weight order not implemented"");<line15>      logger.info(""reverse = "" + t + "", from = "" + this);<line26>    logger.info(""reverse = "" + t + "", from = "" + this);"
"<line2>    logger.trace(""Computing CBD for {} ..."", literal);<line11>      logger.trace(""Got {} triples in {} ms."", model.size(), (end - start));<line13>      logger.error(""CBD retrieval failed when using query\n{}"", query);"
"<line10>                  log.info(""Client Cert SubjectDN: {}"", principal.getName());"
"<line1>    log.debug(""finding StgMbZeiteinheiten instance by example"");<line8>      log.debug(""find by example successful, result size: "" + results.size());<line10>      log.error(""find by example failed"", re);"
"<line6>        LOG.debug(""websocket open: "" + session.getId() + "" : "" + loginuser.getUserId());<line16>        LOG.debug(""websocket is allready closed: "" + session.getId());"
"<line1>    log.debug(""getting StgMbZeiteinheitenTxt instance with id: "" + id);<line6>        log.debug(""get successful, no instance found"");<line7>        log.debug(""get successful, instance found"");<line10>      log.error(""get failed"", re);"
"<line14>      logger.info(String.format(""success starting routing verticle %d at %s"", id, deploymentID()));<line15>      logger.error(String.format(""failed starting routing verticle %d at %s"", id, deploymentID()), e);"
"<line4>      logger.warn(""Received a Consume op for topic: {} that the server does not own."",topic.toStringUtf8());<line32>        logger.debug(""Only advanced consume pointer in memory, will persist later, topic: ""+ topic.toStringUtf8()+ "" subscriberId: ""+ subscriberId.toStringUtf8()+ "" persistentState: ""+ SubscriptionStateUtils.toString(subState.getSubscriptionState())+ "" in-memory consume-id: ""+ MessageIdUtils.msgIdToReadableString(subState.getLastConsumeSeqId()));"
"<line6>      LOGGER.warn(""Cannot index null enum, skipping entry"");"
"<line10>      logger.info(""Cannot add contact with said '""+ said+ ""' [tenant=""+ tenant.getId()+ ""]:""+ "" there's already another User with id ""+ user.getId()+ "" whose accountUri is ""+ user.getAccountUri());<line20>      logger.debug(""Created new user [id=""+ user.getId()+ "", username=""+ user.getUsername()+ "", role=""+ user.getRole()+ ""]"");"
<line13>      LOG.trace(e.getMessage());
"<line3>    logger.trace(""Decoding {}"", t);<line8>        logger.debug(""Removing {} as it was cancelled!"", registration);<line13>        logger.debug(""Removing {} as its timed out (timeout of {} was set till {} and now is {})"",registration,registration.getTimeout(),registration.getTimeoutAt(),now);<line17>      logger.trace(""Checking handler {} for Object of type {}"", registration, t.getClass().getSimpleName());<line18>        logger.trace(""Handler {} has right expected type {}, checking condition"",registration,registration.getExpectClazz().getSimpleName());<line29>              logger.trace(""Registration {} with predicate {} does not match object {} (currently wrapped to""+ "" {})"",registration,predicate,t.getClass().getSimpleName(),instance.getClass().getSimpleName());<line33>        logger.trace(""Handler {} accepts element {}, calling handle method"", registration, t);<line40>    logger.trace(""None of {} registered handlers could handle message {}, using default decode method"",this.registeredHandlers.size(),t);"
"<line13>        LOGGER.warn(""CPCT hospital ID is not present in hospital address list: '{}'"", CPCT);<line18>        LOGGER.warn(""DRUP hospital ID is not present in hospital address list: '{}'"", DRUP);<line23>        LOGGER.warn(""WIDE hospital ID is not present in hospital address list: '{}'"", WIDE);<line28>        LOGGER.warn(""COREDB hospital ID is not present in hospital address list: '{}'"", COREDB);<line33>        LOGGER.warn(""Sample mapping hospital ID is not present in hospital address list: '{}'"",sampleMapping);"
"<line6>      logger.error(""Unable to parse access policy"", e);"
"<line3>      log.info(""Element ["" + this + ""] set next write value to ["" + valueOpt.orElse(null) + ""]."");"
"<line11>      logger.debug(""No Result found: "" + e);"
"<line8>        logger.trace(""Uploading string to file [""+ filename+ ""], data_connection_mode [""+ client.getDataConnectionMode()+ ""]"");<line9>        logger.trace(""Finished uploading string to file [""+ filename+ ""]""+ "", response [""+ client.getReplyString()+ ""]"");<line15>        logger.error(""Error uploading file: "" + reply, e);"
"<line1>    logger.debug(""onNewConnection"");"
"<line7>        logger.warn(""Failed to insert address result in the DB "" + e.getMessage());"
"<line16>        Log.error(""Unable to send presence information of user '{}' to unblocked entity '{}' as local""+ "" user is not found."",user.getUsername(),recipient);"
"<line22>      logger.error(""TTransportException inside handler"", e);<line25>      logger.error(""TApplicationException inside handler"", e);<line28>      logger.error(""Exception inside handler"", e);<line36>      logger.error(""Exception writing to internal frame buffer"", ex);"
"<line19>        log.error(""Oups, Set of task id's is not of type Set<Integer>, can't migrate this list."");"
"<line31>          LOGGER.warn(""Unknown parameter group. That should not happen."");"
"<line3>      logger.debug(""-Dio.netty.bitMode: {}"", bitMode);<line7>      logger.debug(""-Dio.netty.bitMode: {} (sun.arch.data.model)"", bitMode);<line11>      logger.debug(""-Dio.netty.bitMode: {} (com.ibm.vm.bitmode)"", bitMode);<line23>      logger.debug(""-Dio.netty.bitMode: {} (os.arch: {})"", bitMode, arch);"
<line5>    LOGGER.debug(keyParts.toString());<line8>    LOGGER.debug(otherKeyParts.toString());
"<line8>      logger.trace(""Permission denied for reading sequencing object id=""+ sf.getId()+ "" by user=""+ authentication.getName()+ "", no joined sample found."");"
"<line6>        log.debug(""Attempt connect to peer: {} attempt# '{}'"", p.getAnnouncedAddress(), i);<line8>          log.debug(""SUCCESS! connected to peer: {} on attempt# '{}'"", p.getAnnouncedAddress(), i);<line11>            log.debug(""FAILED connect to peer: {} attempt# '{}' , DELAY for '{}' ms before next""+ "" attempt..."",p.getAnnouncedAddress(),i,PeersService.connectTimeout);"
"<line2>    log.info(""The method [ {} ] of bean [ {} ] is candidate to be overridden by plugin child contexts"",m.toString(),id);"
"<line10>      LOG.warn(""{}:{} could not be processed!"", entityType, qualifiedName);<line12>      LOG.error(""{}:{} could not be processed!"", entityType, qualifiedName, ex);"
"<line13>        LOG.error(""Could not serialize new user {}"", newUser, e);"
<line28>        log.debug(workflowException, workflowException);
"<line2>    logger.trace(""RemoveStale(ccId={}, maxLastSeenDateMs={}) called."",clusterControllerId,maxLastSeenDateMs);<line5>      logger.info(""RemoveStale(ccId={}, maxLastSeenDateMs={}) deleted {} stale entries."",clusterControllerId,maxLastSeenDateMs,deletedCount);<line7>      logger.error(""RemoveStale(ccId={}, maxLastSeenDateMs={}) failed."",clusterControllerId,maxLastSeenDateMs,e);"
"<line4>      logger.error(""No GBIDs found in default properties: {}"", joynrDefaultProperties);"
"<line2>    LOG.info(""Removing completed compaction instant ("" + instant + "")"");"
"<line11>        LOGGER.warn(""{} does not appear in {}!"", path, mcrPath.getOwner());"
"<line16>        LOG.debug(""WAITING FOR SCENARIO"");<line21>      LOG.error(ex.getMessage(), ex);<line22>      LOG.debug(""listening client shutting down"");<line25>        LOG.debug(""listening client shutdown succesful"");<line26>        LOG.error(e.getMessage(), e);"
"<line3>    LOGGER.info(""actual output;\n{}"", jobSource.getByteSource().asCharSource(Charsets.UTF_8).read());"
"<line9>        LOG.info(""JPSONIC_HOME directory will be {}"", jpsonicHomeDirForTest.getAbsolutePath());"
"<line5>      logger.info(""Could not scan, as there is no USB-Serial discovery service configured."");"
"<line57>      logger.debug(""cuboid-{} saved dimension:{}, took: {}ms"",cuboidId,dimension.getName(),stopwatch.elapsed(MILLISECONDS));"
"<line23>    LOGGER.info(""Reloading only package {} took {}ms"", packageName, System.currentTimeMillis() - start);"
"<line7>      log.debug(""Clearing cached path "", path, "" for "", binding);<line10>        log.debug(""Removing "", binding, "" from conflicts list "", conflicts);<line26>        log.debug(""Clearing cached prefix "", prefix, "" for "", binding);<line34>      log.debug(""Resolved conflicts with "", resolvedConflicts);"
"<line16>                      LOG.info(""Received: {}"", destinationInfo);"
"<line19>        logger.debug(""Unable to createCq. Error: {}"", cqe.getMessage(), cqe);"
"<line7>      LOG.warn(""Exception hit while trying to recreate directory: "" + destPath.getParent().toString());"
"<line15>        LOGGER.debug(""IOException reading stream from file attachment in multipart body."", e);<line42>        LOGGER.debug(""Unable to get input stream for mime attachment. Ignoring override attribute: {}"",name,e);"
"<line2>    logger.debug(""Start scan for Sensibo devices."");"
"<line3>      logger.debug(""{}: Peeked {}->{}"", this, currentKey, object);<line7>        logger.debug(""Unable to make heap copy and will not be added to peekedIds for object"" + "" : {} "",object.toString());"
"<line10>          logger.fatal(""Unknown error closing streamer: {}"", e.getMessage(), e);"
"<line8>          log.error(msg);<line12>          log.warn(""Multiple versions of the same content received during refresh; ""+ ""discarding previous: {} => {}, {}"",content.getId(),existing,content);"
"<line12>        logger.debug(""Error sending CQ request to peers. {}"", ex.getLocalizedMessage(), ex);"
"<line7>    logger.debug(""about to transform '{}' by the JavaScript '{}'"", source, filename);<line18>      logger.trace(""JavaScript execution elapsed {} ms. Result: {}"",System.currentTimeMillis() - startTime,result);"
"<line25>      LOG.error(""While exporting SAML 2.0 SP metadata"", e);"
"<line27>      log.error(""Worlflow Detail ==== SystemNotificationDetail Subscriber Completed with error "", ijfe);<line34>      log.error(""Worlflow Detail ==== SystemNotificationDetail Subscriber Completed with error "", e);"
"<line13>      logger.error(""Error extracting auths for user {}"", username, e);"
"<line5>    log.debug(""run_custom_filter"");"
"<line4>      logger.trace(LogMarker.SERIALIZER_VERBOSE, ""Read Boolean {}"", value);"
"<line6>    LOGGER.info(""Get Tariff Schedule Response Request received from organisation: {} for correlationUID:""+ "" {}."",organisationIdentification,request.getAsyncRequest().getCorrelationUid());"
"<line23>      log.debug(""printenv({}) {}={}"", getServerChannelSession(), varName, varValue);"
<line5>      log.warn(e.getMessage(), e);
"<line10>          logger.warn(""Skipping entry with id ""+ entry.getId()+ "" since it has the same date as our last feed update."");"
"<line3>    LOG.info(""using GDAL command line to tranform the coverage"");"
"<line14>      LOG.warn(""Entity: {}: Does not exist!"", objectId);<line25>    LOG.warn(""GUID Updated: Entity: {}: from: {}: to: {}"", objectId, vertexGuid, entity.getGuid());"
<line27>          LOG.debug(e.getMessage());<line29>          LOG.error(e.getMessage(), e);
"<line4>        LOGGER.trace(""Purge Occured: "" + cmd);"
"<line20>    logger.info(""Initializing channel buffer manager."");<line22>    logger.info(""Created thread pool: "" + executorServicePool);<line29>      logger.error(""Fatal error! Couldn't establish connection to REDIS!"", e);<line34>      logger.info(""Created new Jedis connection: "" + subscriberJedis);<line38>        logger.info(""Created pattern subscription for pattern: "" + channelRegEx);<line40>        logger.error(""Fatal exception occurred attempting subscription: "" + e.toString(), e);"
"<line7>          log.info(""Poll rate {} changed to {} ms"", pollRate, newPollRate);<line10>        log.error(""Error in exchange client"", e);<line12>        log.warn(""Agent {} didn't registered on current coordinator! Reset agent registration."",nodeContext.getId());<line16>        log.error(""Agent "" + nodeContext.getId() + "" got an exception from coordinator"", e);<line17>      log.debug(""Pack exchange completed. Poll rate on agent {} is {} ms"", nodeContext.getId(), pollRate);"
"<line6>      LOG.error(""Failed to get user name from uid {}, fallback to {}"", uid, DEFAULT_USER_NAME);"
"<line10>    log.info(""{} zookeeper client register success: {}"", rpcType, metadata.toString());"
"<line1>    LOG.info(""Stopping EntryLogger"");<line15>      LOG.error(""Error flush entry log during shutting down, which may cause entry log corrupted."", ie);"
"<line3>    LOGGER.debug(""Setting client caching status to {}"", packet.isEnabled());"
<line5>      log.error(exception, exception);
<line15>        log.debug(sb.toString());
"<line1>    logger.debug(""HeatClient.getServerId() Endpoint: "" + connection.getHeatEndpoint());<line33>      logger.error(""HeatClient.getStackDetails()"", e);"
"<line4>    LOGGER.debug("" {} files found in directory {}"", files.length, dir);<line16>    LOGGER.debug(""  Done reading {} files "", currentFileIndex);"
<line18>      log.error(systemException, systemException);
"<line19>        LOG.info(""Error persisting sessions for deployment "" + deploymentName, e);<line21>      LOG.debug(""No sessions to persist for deployment "" + deploymentName);"
"<line6>        LOG.error(""load task meta from file failed."", e);"
"<line2>    logger.debug(""Close rocks db."");"
"<line1>    log.error(""Internal error. Destroying plan ..."", e);<line6>      log.error(""Cannot destroy plan"", ex);"
<line11>              logger.debug(problem.getMessage(), problem.getException());
"<line5>          logger.info(""start() started."");<line7>          logger.info(""start() completed."");<line10>        logger.info(""start() failed. caused:already initializing."");<line12>        logger.info(""start() failed. caused:already started."");"
"<line3>    LOGGER.debug(""[{}] Blocked inbound from all destinations"", address);"
"<line18>    logger.debug(""Received request to Deactivate an Operational Environment"");"
"<line3>        log.debug(""Loading font "" + ttf);"
"<line5>      LOGGER.error(""An exception occurred during validation of field - {}, objectName - {}, value - {}"",field,objectName,value);"
"<line4>      LOG.debug(""Gauge {} is null-valued, defaulting to 0."", gauge);<line15>    LOG.debug(""Invalid type for Gauge {}: {}, only number types and booleans are supported by this""+ "" reporter."",gauge,value.getClass().getName());"
<line4>        logger.debug(message);<line6>        logger.info(message);<line8>        logger.warn(message);<line10>        logger.error(message);
"<line3>    log.info(""Starting TEST optimizer for the TOSCA syntax of September 2015 Of an ""+ TEST_CHARACTERISTIC);"
"<line5>      logger.warn(""Instance ID has changed from "" + bootstrapId + "" to "" + id);"
"<line38>            LOGGER.warn(""Best Match Heuristic requires statistic RowRangeHistogramStatistics for each index""+ "" to properly choose an index."");<line48>              LOGGER.warn(""No max range decomposition hint was provided, this should be provided from the""+ "" data store options"");"
"<line14>              logger.warn("""", e);<line32>        logger.debug(""{} {} is penalized; will not communicate with this peer"", this, peerStatus);<line36>    logger.debug(""{} All peers appear to be penalized; returning null"", this);"
"<line15>                  logger.info(""Accessing SystemProperty field {}#{}"", className, field.getName());<line17>                  logger.warn(""Unable to access field {}#{}"", className, field.getName(), t);<line21>            logger.warn(""Unable to load class {}"", className, t);<line25>      logger.warn(""Unable to scan classpath for SystemProperty classes"", t);"
"<line2>    log.info(""Got event PaymentError token='{}'"", event.getUserToken());"
<line15>      log.error(exception, exception);
"<line2>    logger.info(""GTAS Home Controller Shutting Down!"");<line5>      logger.error(""error shutting down GTAS Home Controller."", ex);"
"<line2>    LOG.trace(""Starting Liquibase Update"");<line7>        LOG.error(""An exception occurred during database migration"");<line7>        LOG.error(""A rollback file has been generated at "" + rollbackFile);<line7>        LOG.error(""Execute it within your database to rollback any changes"");<line7>        LOG.error(""The exception is as follows\n"", e);<line7>        LOG.error(""==============================================="");"
"<line1>    logger.info(""Creating and editing simple pipeline"");<line4>    logger.info(""Adding an echo step"");<line8>    logger.info(""Pipeline created and ready to be saved"");"
"<line6>      logger.debug(""[{}] Websocket connection closed with code {} reason : {}"", debugId, statusCode, reason);"
"<line1>    LOGGER.info(""Channel active: {}"", ctx.channel());"
"<line27>            log.warn(message);<line30>        log.warn(""Cannot detect bucket versioning configuration."");"
"<line1>    LOG.debug(""CONNECTED STATE: Cancelling Timeouts in handleError"");"
"<line6>      LOG.error(""Failed to send metrics: "", e);"
"<line21>    LOG.debug(""Parsed retry policy BoundedExponentialBackoffRetry with ""+ ""baseSleepTime:{}, maxSleepTime:{}, maxRetries:{}, maxElapsedTime:{}"",new Object[] {baseSleepTime, maxSleepTime, maxRetries, maxElapsedTime});"
"<line5>    LOGGER.debug(""Registering handler {} -> {}"", key, handler);"
"<line5>      logger.warn(String.format(""Unhandled exception happened when running %s"", task.getClass().getName()),t);"
"<line5>        logger.debug(format(""Channel %s removing mapping peer %s to mspid %s"", name, peer, mspid));"
<line12>      LOG.error(e.getMessage(), e);
"<line14>    LOG.debug(""Creating SaslServer for {} with mechanism {}"", kerberosName, saslMechanism);"
"<line13>      LOGGER.warn(""Something going wrong here..."");"
"<line6>    LOGGER.info(""Created scope object returned to client : "" + createdScope.toJsonString());"
"<line6>      log.info(""Added sync specification: {}"", spec);<line10>    log.info(""Vault Sync service is {}"", enabled ? ""enabled"" : ""disabled"");"
"<line19>      LOGGER.error(""Cannot create folder"", e);"
"<line14>        logger.debug(""Scheduled strategy {} with cron expression {}"",cronStrategy.getName(),cronExpression);"
"<line5>      logger.error(""Registration resp. initialization of child '{}' of bridge '{}' has been failed: {}"",child.getUID(),bridge.getUID(),ex.getMessage(),ex);"
"<line2>      this.logger.info(""Rx :  DialogRelease="" + evt);"
"<line8>    LOG.info(""Finished deleting the ledgers contains most entries."");"
"<line12>    LOGGER.debug(""enqueueUpdateDeviceSslCertificationRequest called with organisation {} and device {}"",organisationIdentification,deviceIdentification);"
"<line1>    LOG.info(""Waiting to exit safe mode..."");<line12>    LOG.info(""Exited safe mode"");"
"<line4>      LOG.info(""Updated task: saved changes in element properties."");"
"<line2>      LOG.debug(MessageFormat.format(""Truncating Direct I/O resources: {0}:{1} (id={2})"", fullPath, resourcePattern, id));"
"<line53>    log.info(""Created consumer {} in group {}"", this.name, this.groupId);"
"<line4>      log.info(""Starting checkout of ["" + _path + ""]"");<line6>      log.info(""["" + _path + ""] already exists, no checkout needed"");"
"<line11>    LOG.debug("">>>>> {}"", body);"
"<line10>            logger.trace(LogMarker.TOMBSTONE_COUNT_VERBOSE,""tombstone for {} was resurrected with v{}; destroyed version was v{}; count is {};""+ "" entryMap size is {}"",re.getKey(),re.getVersionStamp().getEntryVersion(),destroyedVersion,this._getOwner().getTombstoneCount(),size());<line14>              logger.trace(LogMarker.TOMBSTONE_COUNT_VERBOSE,""removing tombstone for {} with v{} rv{}; count is {}"",re.getKey(),destroyedVersion,version.getRegionVersion(),(this._getOwner().getTombstoneCount() - 1));<line15>              logger.trace(LogMarker.TOMBSTONE_COUNT_VERBOSE,""removing entry (v{}) that is older than an expiring tombstone (v{} rv{}) for {}"",entryVersion,destroyedVersion,version.getRegionVersion(),re.getKey());"
"<line7>      LOG.info(""Projecting fields schema : "" + projectionSchema.toString());"
"<line30>              LOG.debug("""" + index + "" "" + aField.getName() + "" "" + aField.getType());"
"<line28>      LOG.info(""ASYNC_PROFILER_HOME environment variable and async.profiler.home system property ""+ ""not specified. Disabling /prof endpoint."");"
"<line51>    LOG.info(""Tajo Worker is initialized."" + "" connection :"" + connectionInfo.toString());<line54>      LOG.fatal(e.getMessage(), e);"
"<line7>    log.debug(""Creating DiskCache for attributes = {0}"", idca);"
"<line3>    log.info(""-- Loading temp storage %s --"", name);<line24>    log.info(""-- Loaded temp storage %s --"", name);"
"<line7>      logger.debug(""Looking up endpoint for ["" + key + ""]"");"
"<line3>      LOG.debug(""Ignoring message as test already completed (either pass or fail)"");<line7>      LOG.debug(""Got message {}"", actualIndex);<line9>        LOG.debug(""Ignoring message "" + actualIndex + "" and calling recover"");<line15>        LOG.debug(""Acknowledging message {}"", actualIndex);"
"<line5>      LOG.info(""Will wait for all threads to shutdown gracefully. Final shutdown Time: ""+ new Date(endTime));<line8>          LOG.info(""Number of active threads = "" + activeCount + ""."");<line10>          LOG.info(""Number of active threads = ""+ activeCount+ "". Waiting for all threads to shutdown ..."");<line19>    LOG.info(""Shuting down the Server."");"
"<line7>      logger.debug(""PartitionedRegionFunctionResultSender sending result from local node to client {}"",oneResult);<line11>          logger.debug(""PartitionedRegionFunctionResultSender sending result from remote node {}"",oneResult);<line19>        logger.debug(""PartitionedRegionFunctionResultSender adding result to ResultCollector on local node""+ "" {}"",oneResult);"
"<line2>    logger.debug(""Start customizing the animation"");<line15>      logger.debug(""Falling back to workspace to read {}"", input);"
"<line1>    log.debug(""merging FilterResZob instance"");<line4>      log.debug(""merge successful"");<line6>      log.error(""merge failed"", re);"
"<line6>        log.warn(""Unable to index object entry "" + objectEntry.getObjectEntryId(), exception);"
"<line3>    logger.debug(""initializing bean "" + getClass().getName());"
<line15>        LOGGER.warn(UNABLE_TO_ACCESS_FIELD_ON_OBJECT, f.getName(), target);
"<line10>        LOG.error(""fail to scan peer table in cleanup"", e);<line19>        LOG.error(""fail to close source table in cleanup"", e);<line25>        LOG.error(""fail to close source connection in cleanup"", e);<line31>        LOG.error(""fail to close replicated table in cleanup"", e);<line37>        LOG.error(""fail to close replicated connection in cleanup"", e);"
"<line6>          log.debug(""Found Layout Template named "" + at.getDisplayName());<line10>      log.debug(""No Layout Templates defined"");"
"<line2>    logger.info(""bot is shutting down"");"
"<line13>      LOG.warn(""The identity maybe duplicate."");<line50>          LOG.info(""Transaction is being rolled back."");<line56>        LOG.error(""Error occurred while adding identity."");"
"<line14>    LOG.debug(""Property {} is set to value {}"", name, answer);"
"<line16>        LOG.error(""error while retrieving suite from mongo db: '{}', correlationId: '{}', suiteName:""+ "" '{}'"",dbKey,patternCorrelationId,patternSuiteName,se);"
"<line7>      RBridgeControl.LOGGER.error(""Error casting value from R: {} Cause: {}"", input, exc);<line9>      RBridgeControl.LOGGER.error(exc.getMessage(), exc);"
<line4>      logger.info(e);
"<line19>        LOGGER.error(""Failed to remove timeline data - no data type found for "" + dataTypeKey);"
"<line2>    log.debug(""Starting KDC..."");<line8>    log.debug(""serverKeyTab written to:"" + serverKeyTab);<line15>    log.debug(""serverUnboundKeyTab written to:"" + serverUnboundKeyTab);"
"<line3>    logger.trace(""onPostExecute()"");"
"<line29>                logger.debug(""Successfully recorded subscription for topic: ""+ topic.toStringUtf8()+ "" subscriberId: ""+ subscriberId.toStringUtf8()+ "" data: ""+ SubscriptionStateUtils.toString(data));"
"<line17>      LOGGER.info(""will produce spreadsheet spreadsheet report"");<line37>      LOGGER.info(""did produce spreadsheet report for {} packages in {}ms"",count,System.currentTimeMillis() - startMs);"
"<line3>    LOG.debug(""Begin""+ "" EntityPatientDiscoveryDeferredResponseProxyWebServiceUnsecuredImpl.processPatientDiscoveryAsyncResp(...)"");<line6>      LOG.debug(""Before target system URL look up."");<line8>        LOG.debug(""After target system URL look up. URL for service: {} is: {}"", serviceName, url);<line35>    LOG.debug(""End EntityPatientDiscoveryDeferredResponseProxyWebServiceUnsecuredImpl.processPatientDiscoveryAsyncResp(...)"");"
"<line7>        log.debug(""Invalid shard ID:{}"", envVal);"
<line4>      logger.warn(msg);
"<line3>    log.info(""setUserQuotaTotal()"");<line9>    log.info(""userName:{}"", userName);<line9>    log.info(""quotaValue:{}"", quotaValue);<line10>    log.debug(""executing admin PI"");<line11>    log.info(""quota set"");"
"<line5>        logger.trace(LogMarker.DM_VERBOSE, ""Shutting down conduit"");<line28>          logger.warn(""Unable to shut down listener within {}ms.  Unable to interrupt socket.accept() due""+ "" to JDK bug. Giving up."",LISTENER_CLOSE_TIMEOUT);"
"<line9>            LOGGER.warn(""Unable to parse a \""TO\"" header address in the original message: {}; ignoring."",toHeader);<line14>      LOGGER.warn(""Unable to parse the \""TO\"" header  in the original message; ignoring."");"
"<line1>    logger.error(""HTTProx request failed: "" + error.getMessage(), error);<line10>        logger.debug(""Response error complete."");<line12>      logger.error(""Failed to close httprox request: "" + error.getMessage(), error);"
"<line2>    LOG.trace(""bound {} to {}"", key.value(), value);"
"<line16>    logger.info(""Adding new CP member: "" + member);"
"<line24>      Logger.error(XmlHelper.class.getName(), ""Error loading localized file: "" + fullPath);"
"<line16>        logger.debug(""Check command context {}"", commandContext);"
"<line4>      LOG.error(""Can't fetch relevant Csar from storage: StorageService not available"");"
"<line27>          LOG.warn(""JAXB error: {}"", ex.getMessage(), ex);"
<line34>      log.error(e.getMessage(), e);
"<line3>    LOG.debug(""Raising NodeReconciliationOperationOngoing alarm, alarmText {} source {}"",alarmText,source);"
"<line20>          LOGGER.info(""heartbeat handler is already part of this pipeline"", e);<line31>          LOGGER.info(failure.getMessage());<line31>          LOGGER.debug(""Failed to write {}"", message, failure);<line37>          LOGGER.info(""Heartbeat handler was concurrently removed"");"
"<line1>    LOG.debug(""Session {} end of input detected while session was in state {}"",this,isUp() ? ""up"" : ""initialized"");"
"<line43>        logger.error(""The content length filter (configured with {}) is blocking the site-to-site""+ "" connection: {}"",NiFiProperties.WEB_MAX_CONTENT_SIZE,e.getMessage());<line48>      logger.error(""The NiFi DoS filter has interrupted a long running session. If this is undesired,""+ "" configure a longer web request timeout value in nifi.properties using '{}'"",NiFiProperties.WEB_REQUEST_TIMEOUT);"
<line10>      LOG.error(e);
"<line4>      logger.debug(""Doorbird returned json response: {}"", statusResponse);<line6>      logger.info(""Unable to communicate with Doorbird: {}"", e.getMessage());<line7>      logger.info(""Unable to parse Doorbird response: {}"", e.getMessage());"
"<line2>    LOGGER.debug(""Processing common get configuration message"");<line8>      LOGGER.error(""UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up."", e);"
"<line1>    LOG.info().$(""copying folder [from="").$(from).$("", to="").$(to).$(']').$();"
"<line6>      LOG.debug(""The project report cannot be found."", e);"
"<line8>      log.error(""Exception during datastore cutover"", ex);"
"<line12>      log.error(""Exception reading theme ["" + this.getName() + ""] template file ["" + templateFile + ""]"");<line13>        log.debug(noprob);"
"<line17>      log.debug(""Could not determine volume for Path: {}"", path);"
"<line4>      LOGGER.debug(""This entity was already interrupted in the past. Not going to sleep again."");<line8>    LOGGER.info(""{}.setValue() started and going to sleep."", TestdataSleepingEntity.class.getSimpleName());<line14>    LOGGER.info(""{}.setValue() interrupted after {}ms."",TestdataSleepingEntity.class.getSimpleName(),System.currentTimeMillis() - start);"
"<line14>          log.debug(""Notifying invoice of failed payment: id={}, amount={}, currency={}, invoiceId={}"",paymentControlContext.getPaymentId(),paymentControlContext.getAmount(),paymentControlContext.getCurrency(),invoiceId);<line26>          log.error(""InvoicePaymentControlPluginApi onFailureCall failed ton update invoice for attemptId""+ "" = ""+ paymentControlContext.getAttemptPaymentId()+ "", transactionType  = ""+ transactionType,e);<line44>          log.warn(""onFailureCall failed for attemptId='{}', transactionType='{}'"",paymentControlContext.getAttemptPaymentId(),transactionType,e);"
"<line1>    LOG.info(""SeaCloudsInitializerPolicy is starting to enforce SLA Agreements for "" + entity.getId());"
<line3>    logger.trace(format, arg1, arg2);
"<line48>          logger.trace(""Created flight ({})."", flight);"
"<line6>      LOG.debug(""ArtifactSpecificContent specified!"");"
"<line3>    LOG.debug(""Creating IndexClient with given filesystem client with root path %s"", root);"
"<line23>      logger.warn(""failed to get channel for "" + operand);"
"<line8>          LOG.warn(""Failed to load chat projects! "" + e, e);<line19>      LOG.warn(""Failed to get issue project names: "" + e, e);"
"<line13>      logger.error(""Error removing messages for user {}"", username, t);"
"<line8>        log.warn(""Advertised commit is already merged into current head in branch '{}'. Current HEAD:""+ "" {}, advertised ref: {}"",branch,localRef.getObjectId().name(),advertisedRef.getObjectId().name());<line9>        log.warn(""Found commits that are not fast forwarded in branch '{}'. Current HEAD: {}, advertised""+ "" ref: {}"",branch,localRef.getObjectId().name(),advertisedRef.getObjectId().name());"
"<line2>    LOG.info(""start open ..."");<line27>      LOG.error(""Exception while creating connection to HBase."", e);<line30>    LOG.info(""end open."");"
"<line22>      log.info(""got excluded ids: "" + excludedIdsCSV);<line25>      log.info(""got no excluded ids"");"
<line12>    logger.debug(sb.toString());
"<line7>      log.debug(""Multiple expected values for {}: keeping initial value {} and discarding later value {}"",lockID,m.get(lockID),value);<line9>      log.debug(""Store expected value for {}: {}"", lockID, value);"
"<line25>          logger.info(""Downloading builtin template ""+ template.getUniqueName()+ "" to data center: ""+ dcId);"
"<line8>      LOG.debug(""Checking fragment {}"", r);<line11>        LOG.error(""Invalid fragment found : {}"", r);<line13>        LOG.error(""BKException when checking fragment : {}"", r, e);"
"<line8>      logger.info(""dns lookup fail. host:{}"", host);"
"<line14>      log.error(""Error in enable rules"", e);"
<line15>      log.error(systemException, systemException);
"<line11>            LOG.warn(""Ignoring invalid property name: {}"", name);<line22>          LOG.warn(""Cannot listen for changes on invalid path: {}"", path);<line35>        LOG.warn(""Error while dispatching observation events"", e);"
"<line6>      log.debug(""Invoking the callback function for sent message, IoT Hub responded to message ({}) with""+ "" status {}"",packet.getMessage(),status);"
"<line1>    LOG.info(""{}: followerNextIndex = {} but logStartIndex = {}, send snapshot {} to follower"",this,getFollower().getNextIndex(),getRaftLog().getStartIndex(),snapshot);<line18>      LOG.warn(""{}: failed to install snapshot {}: {}"", this, snapshot.getFiles(), e);<line34>      LOG.info(""{}: installed snapshot {} successfully"", this, snapshot);"
"<line22>        LOGGER.debug(""Unable to find message with uid {}"", result.getUid(), e);<line23>        LOGGER.error(""Unable to fetch message with uid {}, so skip it"", result.getUid(), e);"
"<line5>          log.debug(""Transaction committed"");<line8>          log.debug(""Transaction rolled back"");<line11>          log.debug(""Received unexpected transaction completion status: {}"", status);<line13>      log.error(""Error occurred while attempting to synchronize database and message bus"", e);"
"<line2>      log.info(formatLogString(""sending "" + WorkerDoneEvent.class.getSimpleName() + "" to sync""));"
"<line5>        LOG.debug(""Thing \""{}\"" changes something in its status (eg: a behavior value)"",this.getPojo().getName());"
<line18>      log.error(systemException, systemException);
<line7>      log.error(exception, exception);
"<line61>      logger.debug(""handleNewPendingConnection {} myAddr={} theirAddr={}"",con,getConduit().getMemberId(),con.getRemoteAddress());"
<line5>      log.error(e.getMessage(), e);
"<line14>        log.error(""Invalid HDFS base path for vectors: "" + strBasePath, e);"
"<line3>      LOGGER.info(""Closing session"");<line5>      LOGGER.info(""Session closed"");"
"<line4>      log.debug(""#validateAndFillPolicy - The 'type' member is not found under policy {}"",emptyPolicyDefinition.getName());"
"<line3>    log.info(""output specification location: "" + currentSpecLocation);"
"<line3>      logger.info(""executing: ifconfig and looking for {}"", primaryNetworkInterfaceName);<line24>      logger.error(""Failed to get network interfaces"", e);"
"<line2>      log.warn(""[{}] ThreadPool[{}] overload, the task[{}] will run by a new thread!"", source, p, r);<line2>      log.warn(""[{}] Maybe you need to adjust the ThreadPool config!"", source);<line4>        log.info(""[{}] create new thread[{}] to run job"", source, threadName);"
"<line7>    logger.info(""RetryableHttpConnection: endpoint: %s, data.len: %d, method: %s"",endpoint, data == null ? -1 : data.length(), method);<line12>        logger.info(""connecting to %s:%s ..."", ips.get(index), ports.get(index));<line35>              logger.info(""succeed to connect to %s:%s!"", ips.get(index), ports.get(index));<line37>            logger.error(""code: %d, msg: %s"", code, conn.getResponseMessage());<line45>        logger.error(e);<line47>    logger.error(""Unable to connect to all IPs"");"
"<line20>        LOG.debug(""Unresolved reference"", ignore);"
"<line2>    logger.debug(""Removing all configured devices ..."");<line5>        logger.debug(""Removing device: {}"", thing.getLabel());<line7>        logger.warn(""Removing device failed (DeviceHandler is null): {}"", thing.getLabel());"
"<line6>    log.debug(""DME2 GRM URI: {}"", lookupURI);<line11>      log.debug(""DME returns EP with UEB host {}, UEB port: {}"",dmeEndpoint.getHost(),dmeEndpoint.getPort());"
"<line8>        log.warn(""[OmsJarContainer-{}] delete jarFile({}) failed."",containerId,localJarFile.getPath(),e);<line13>        log.info(""[OmsJarContainer-{}] container destroyed successfully"", containerId);<line14>        log.error(""[OmsJarContainer-{}] container destroyed failed"", containerId, e);<line17>    log.warn(""[OmsJarContainer-{}] container's reference count is {}, won't destroy now!"",containerId,referenceCount.get());"
"<line4>      log.warn(""Argument 'bytes={}' was not a serialized HFileMeta!"", CommonUtils.hex(bytes));"
"<line2>    logger.info(""Using <{}> for container type <{}>"", containerClass.getName(), containerType);"
"<line1>    LOG.error(message, e);<line2>    LOG.error(""ScreenShot::\ndata:image/png;base64,""+ new String(Base64.encodeBase64(FileUtils.readFileToByteArray(scrFile))));"
"<line1>    log.info(""AutoML Executor === AutoML is running on usecase {} wait for a while to finish"",autoMlConfig.getUseCaseName());<line16>      log.info(""AutoML Executor === AutoML on usecase {} is completed successfully and usecase is ready""+ "" for leaderboard and prediction for modelId {}"",autoMlConfig.getUseCaseName(),modelId);"
"<line21>      LOG.info(""Create container placement policy of type {}"", placementClass.getCanonicalName());<line26>      LOG.error(msg);"
"<line5>      logger.warn(""new eps not set: "" + e);"
"<line7>            LOGGER.debug(""Error initializing the {}"", a.getName());"
"<line7>        log.debug(""grabbed"");<line8>          log.debug(""publishDisplay"");"
"<line8>      LOGGER.debug(""Content-Length is "" + contentLength + "" bytes"");"
<line30>      log.error(systemException, systemException);
<line6>      log.error(exception, exception);
<line8>    LOGGER.info(warrior.toString());<line17>    LOGGER.info(mage.toString());<line19>    LOGGER.info(thief.toString());
"<line1>    logger.info(""Failed "" + getProgress() + ""% -- "" + getTask() + "" ["" + getPrintable(getOwner()) + ""]"");"
"<line37>        LOG.debug(""message encoding took {} nanoseconds"", y);<line49>        LOG.debug(""lastWrittenCharacter={}, lastThrownException={}, sentData: {}"",lastWrittenCharacter,lastThrownException,sentData);"
"<line7>      log.debug(""AccessControlImporter may not be used with the WorkspaceImporter"");"
<line8>      log.info(message);<line10>      log.error(message, e);
"<line18>              logger.trace(LogMarker.DLS_VERBOSE, ""[becomeLockGrantor] creating lockGrantorFutureResult"");<line36>          logger.trace(LogMarker.DLS_VERBOSE,""become set lockGrantorId to {} for service {}"",this.lockGrantorId,this.serviceName);<line46>          logger.trace(LogMarker.DLS_VERBOSE, ""[becomeLockGrantor] Calling makeLocalGrantor"");"
"<line37>          LOGGER.info(""https server working"");"
"<line1>    log.debug(""merging MbZielobjRelation instance"");<line4>      log.debug(""merge successful"");<line6>      log.error(""merge failed"", re);"
"<line15>                  log.error(""Error notifying event listener"", e);"
"<line43>          logger.info(""idle timeout set to "" + sessionTimeoutSec);<line44>          logger.error(e);"
"<line9>          log.info(""mail finished"");<line10>            log.info(result.result().toString());<line17>            log.warn(""got exception"", result.cause());<line24>          log.info(""closing mail service"");"
"<line15>      logger.error(""Error loading categories"", t);"
"<line13>        LOGGER.error(""Class not found for property "" + property, e);<line14>        LOGGER.error(""Invalid class for property"" + property, ex);<line17>    LOGGER.warn(""Using default class for parameter : "" + property.self().toString());"
<line65>      log.error(exception, exception);
"<line7>      log.error(String.format(""Error reading file: \""%s\"""", filename), e);"
"<line10>        logger.error(Messages.getInstance().getErrorString(""GwtRpcPluginProxyServlet.ERROR_0004_MALFORMED_URL"", moduleBaseURL),ex);<line40>      logger.error(Messages.getInstance().getErrorString(""GwtRpcPluginProxyServlet.ERROR_0007_FAILED_TO_OPEN_FILE"",serializationPolicyFilename),e);<line41>      logger.error(Messages.getInstance().getErrorString(""GwtRpcPluginProxyServlet.ERROR_0008_FAILED_TO_PARSE_FILE"",serializationPolicyFilename),e);"
"<line18>      LOGGER.error(""Cannot check current user authorization to submit work item: {}"",ex.getLocalizedMessage(),ex);"
"<line34>      logger.error(""Unable to get the upcoming recording for agent '{}'"", agentId, e);"
"<line7>      log.warn(""Could not process request message. [exception=({}), artifact=({}), ""+ ""contract=({}), issuer=({}), messageId=({})]"",exception.getMessage(),requestedArtifact,transferContract,issuerConnector,messageId,exception);"
"<line3>    log.info(""Average time per run: {"" + perRun + ""} ms"");"
"<line19>    LOGGER.debug(keyEx.length());<line24>    LOGGER.debug(""405e2a60cefcb557edd6d41336a3fa4b2dfdae20f4ac7adacbb29c13456e2800"".length());<line28>    LOGGER.info(ArrayConverter.bytesToHexString(message.getComputations().getPlainPaddedPremasterSecret(), false));"
"<line3>      LOGGER.warn(""Interpreter Setting file {} is not existed"", interpreterSettingPath);<line5>    LOGGER.info(""Load Interpreter Setting from file: {}"", interpreterSettingPath);"
"<line12>          LOG.error(""Couldn't save notification: {}"", notification.getDocument(), e);"
"<line16>      LOG.info(""Created schema on persistent store and initialized cache for persistent bean {}."",super.getPersistentClass().getSimpleName());"
"<line1>    logger.debug(""validateSystemId started..."");"
"<line9>        logger.info(""Creating new manager instance of {}"", clz);"
"<line12>        log.debug(""Loading from mrsImageProviderCache"");<line12>        log.debug(""   cacheKey: {}"", cacheKey);<line12>        log.debug(""   name: {}"", name);<line12>        log.debug(""   accessMode: {}"", accessMode.name());<line12>        log.debug(""   conf: {}"", conf);<line12>        log.debug(""   provider properties: {}"", providerProperties);"
"<line11>          logger.debug(""ClientHealthMonitor: Registering client with member id {}"", proxyID);"
"<line2>    log.info(""Map-SubKey: "" + MutableMap.copyOf(entity.getConfigMap().asMapWithStringKeys()));"
"<line8>            LOG.warn(""Invalidating session {} found to be expired when requested"", id, e);<line15>      LOG.warn(""Error loading session {}"", id, e);<line18>        LOG.warn(""Error cross-context invalidating unreadable session {}"", id, x);<line21>      LOG.warn(""Unable to get Session"", other);"
"<line3>      logger.debug(""Ignoring command {} = {} because device is offline."", channelUID.getId(), command);<line56>          logger.warn(""Ignoring unknown command: {}"", command);"
"<line23>                LOG.warn(""Error converting the json string {} to StructuredRecord"", recordString, e);"
"<line2>    LOGGER.info(""                  Enmasse operator install"");<line6>    LOGGER.info(""***********************************************************"");"
"<line20>      log.info(""Subscribing to [{}] id [{}]"", eventName, id);"
"<line13>      LOGGER.warn(""Unable to check if '{}' is a zip file"", dependency.getActualFilePath());<line13>      LOGGER.trace("""", e);"
"<line6>      log.trace(""Could not initialize generator "" + this, e);"
"<line11>        LOGGER.warn(""Unable to delete mini zookeeper temporary directory"");<line13>      LOGGER.warn(""Unable to delete or shutdown mini zookeeper temporary directory"", e);"
"<line2>    logger.debug(""Background started."");<line6>      logger.debug(""Background done."");"
<line3>      LOG.error(cause.getMessage(), cause);
"<line7>      log.error(""No versions for created path : "" + e.getMessage());<line9>      log.error(""Get version error : "" + e.getMessage());"
<line11>          LOG.warn(MessageFormat.format(JGitText.get().unableToReadPackfile, p.getPackFile().getAbsolutePath()),e);
"<line3>    LOG.trace(""Entering wait"");<line4>    LOG.trace(""Wait completed"");"
"<line4>    LOGGER.info(""Initializing wsDistributionAutomationOutboundDomainRequestsConnectionFactory bean."");"
"<line28>          log.debug(""received "" + count);"
<line6>      LOG.error(e.getMessage());
"<line6>      LOGGER.info(""Processing CSR for scm {}, nodeId: {}"",scmNodeDetails.getHostName(),scmNodeDetails.getScmNodeId());"
"<line2>      log.info(""Load model..."");<line4>      log.info(""Model not found."");<line9>      log.error(""Can not open the cam !!!"");<line23>        log.info(""It takes "" + per + ""Seconds to make detection"");<line29>          log.error(ex.getMessage());"
"<line2>    LOGGER.debug(""Processing public lighting set light request message"");<line8>      LOGGER.error(""UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up."", e);"
"<line2>    logger.info("" ---> Creating a smaller file small.txt"");"
"<line4>      logger.info(""Inheriting Empty Policies, Users & Groups. Will backup existing Policies, Users & Groups""+ "" first."");<line10>      logger.debug(""Inheriting Polciies, Users & Groups"");<line12>      logger.info(""Cannot directly inherit Policies, Users & Groups. Will backup existing Policies, Users &""+ "" Groups, and then replace with proposed configuration"");"
"<line5>    LOG.info(""Total processed: "" + processedCount);"
"<line2>    logger.debug(""Received notfification of a new remote connection!"");<line8>        logger.error(""Error closing channel: "" + ex.getMessage());"
"<line2>      log.info(""[{}] ZK state changed: {}"", self.getServiceId(), newState);"
<line26>      LOGGER.error(e);
"<line2>    LOGGER.debug(""SignatureHashAlgorithms: ""+ ArrayConverter.bytesToHexString(msg.getSignatureHashAlgorithms().getValue()));"
"<line5>    log.info(""WRAPPED STREAM json is: "" + result);"
"<line2>      LOGGER.info(""{} query SQL: {}"", Thread.currentThread().getName(), sql);<line3>    LOGGER.debug(""execute sql {}"", sql);"
"<line11>                    logger.debug(""Invoking function {}"", function);<line19>                      logger.debug(""Invoking object+function {}"", name);<line22>                    logger.info(""Member {} not a function in {}"", name, jsThis);<line25>                  logger.info(""{} not a member in {}"", name, jsThis);<line28>                logger.info(""Exception while trying to invoke "" + function, x);"
"<line8>      LOG.warn(""No 'quarkus.sentry.in-app-packages' was configured, this option is highly recommended as""+ "" it affects stacktrace grouping and display on Sentry. See""+ "" https://quarkus.io/guides/logging-sentry#in-app-packages"");"
"<line9>        LOG.debug(""Resolve name {} to rack {}."", n, rack);"
"<line10>      LOG.error(""getSessionStatDataList error"", e);"
<line18>      log.error(systemException, systemException);
"<line1>    log.info(""Click Perform Actions"");"
"<line1>    LOG.trace(""Emitting - {}"", id);"
"<line6>    logger.debug(""createBulkAuthorizationIntraCloudResponse started..."");"
"<line4>      LOGGER.debug(""Error on ping attempt (""+ pingQueueEntry.getAttempts()+ "") for ""+ pingQueueEntry+ "": [""+ ex.getMessage()+ ""]. Will re-queue for later attempts."");<line7>      LOGGER.warn(""Error on ping attempt (""+ pingQueueEntry.getAttempts()+ "") for ""+ pingQueueEntry+ "": [""+ ex.getMessage()+ ""].  Entry will be REMOVED from ping queue."");<line7>      LOGGER.debug(""Error on last ping attempt was: "", ex);"
"<line4>      LOG.info(""Offer {} on {} rescinded"", offerId.getValue(), maybeCached.getOffer().getHostname());<line5>      LOG.info(""Offer {} rescinded (not in cache)"", offerId.getValue());"
"<line2>    logger.info(""[getEventDetailsWithEventId][begin] eventId: {}"", eventId);<line6>      logger.error(""[GetEvent][fail]Cannot findRedisHealthCheckInstance with null event id."");<line7>    logger.info(""[getEventDetailsWithEventId][end] eventId: {}"", eventId);"
"<line11>      LOG.info(""success to create the store directories..."");<line12>      LOG.error(""fail to create the store directories: %s"", e.getMessage());"
"<line9>          logger.trace(""Handler for id {} not initialized"", integrationId);"
"<line11>        log.trace(""%s: asking %s to fetch the shared group key %s via an external key exchange protocol""+ "" (srv=%s)"",local_addr,encr_msg.getDest() == null ? ""all members"" : encr_msg.getDest(),Util.byteArrayToHexString(sym_version),srv);<line18>          log.trace(""%s: sending encrypted group key to %s (version: %s)"",local_addr,encr_msg.getDest() == null ? ""all members"" : encr_msg.getDest(),Util.byteArrayToHexString(sym_version));<line22>      log.warn(""%s: unable to send message to %s: %s"",msg.getDest() == null ? ""all"" : msg.getDest(), local_addr, ex);"
"<line1>    logger.debug(""obtainKeys started..."");"
"<line27>      logger.error(""Could not delete series: {}"", e.getMessage());"
<line5>        log.debug(_NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey);
"<line2>    logger.info(""Shutting down job scheduler"");<line3>    logger.info(""Job scheduler shut down"");"
"<line2>    LOGGER.info(""generating data between ""+ formatter.print(startDate)+ "" and ""+ formatter.print(endDate));"
"<line3>      log.error(""Ticket is null or blank."");<line7>      log.error(""Unable to find permissions registered for given ticket:"" + ticket);"
<line17>      log.error(exception, exception);
"<line3>    LOGGER.debug(""import agency file {}"", fileName);"
"<line5>        LOG.error(""Error closing JMS topic subscriber: "" + topicSubscriber, ignore);"
"<line4>    LOGGER.info(""Discover Object Instance : ""+ ""Object = ""+ theObject.toString()+ "", Object class = ""+ theObjectClass.toString()+ "", Object name = ""+ objectName);"
"<line3>      log.info(""Cleaner Timer is disabled."");<line3>      log.warn(""Cleaner Timer Interval (cleanServiceInterval in oxauth configuration) is negative which""+ "" turns OFF internal clean up by the server. Please set it to positive value if you""+ "" wish internal clean up timer run."");"
"<line6>      LOG.debug(""Received PubAck packageID: {}"" + packageId);"
"<line1>    logger.debug(""handleIncreaseDecrease called for channel: {}, command: {}"", channelUID, command);"
"<line2>    LOG.info(""AdviceWith replace input from [{}] --> [{}]"", from.getEndpointUri(), uri);"
"<line4>      log.debug(""ResourceUtils got error reading ""+ url+ (context == null ? """" : "" "" + context)+ "" (rethrowing): ""+ e);"
"<line19>      logger.error(""TTransportException inside handler"", e);<line22>      logger.error(""TApplicationException inside handler"", e);<line25>      logger.error(""Exception inside handler"", e);<line33>      logger.error(""Exception writing to internal frame buffer"", ex);"
"<line4>      LOGGER.warn(MessageFormat.format(this.getActionExecution().getAction().getType()+ "" does not accept {0} as type for iterationType"",iterationType.getClass()));"
"<line4>      logger.error(""Current authUser is trying to access to a forbbiden api : ""+ authUser.getAccountRepresentation());"
"<line9>      LOG.error(""Error during resource conversion"", e);<line12>      LOG.error(""No resource received"");<line17>      LOG.error(""Illegal resource received"");"
<line1>    logger.info(text);
"<line7>      Log.debug(""CoordinateEditor: '"" + value + ""' -> "" + d);"
"<line2>    log.debug(""Invalidating collection sync root member cache for all users"");"
"<line1>    log.warn(""Rolling back ActiveMQ transaction."");<line5>        log.error(""Error rolling back ActiveMQ transaction"", e);"
"<line2>    log.info(""Found {} classes annotated with {} "", (classes != null ? classes.size() : 0), clazz);"
"<line4>      logger.warn(""Cannot create bridge: Bridge UID is missing."");<line8>      logger.warn(""Cannot create bridge: E-mail address is missing."");<line14>      logger.warn(""Cannot create bridge: Bridge UID '{}' is malformed."", bridgeUid);<line17>      logger.warn(""Cannot create bridge: E-mail address '{}' is malformed."", email);<line20>    logger.debug(""Auto configuring Miele account using locale '{}' (requested locale was '{}')"",locale,request.getParameter(LOCALE_PARAMETER_NAME));<line25>      logger.warn(""{}"", e.getMessage());<line36>      logger.warn(""Thing creation failed because there was no binding available that supports the thing."");"
<line2>      LOG.debug(I18n.msg(I18n.MSG_04102_BIND_REQUEST, name));
"<line29>    logger.debug(""Sample references extraction. Query: {}, update: {}"",bsonQuery.toBsonDocument(Document.class, MongoClient.getDefaultCodecRegistry()),update.toBsonDocument(Document.class, MongoClient.getDefaultCodecRegistry()));<line30>    logger.debug(""Sample uid '""+ sampleUid+ ""' references removed from ""+ updateResult.getNumUpdated()+ "" out of ""+ updateResult.getNumMatches()+ "" individuals"");"
"<line24>      log.debug(DataFormatter.class.getName()+ "" could not apply mask to data. The original data was returned"");"
"<line2>    log.info(""Internal blob store set: {}"", blobStore);"
"<line6>        logger.debug(""Creating region {}"", this.newRegion);<line9>          logger.debug(""CreateRegionProcessor.initializeRegion, no recipients, msg not sent"");<line43>            logger.debug(""initialized bucket event tracker: {}"",((LocalRegion) this.newRegion).getEventTracker());"
"<line9>          LOG.debug(""FULL Updated paths seq Num [old=""+ authzPaths.getLastUpdatedSeqNum()+ ""], [new=""+ newAuthzPaths.getLastUpdatedSeqNum()+ ""]"");<line10>          LOG.debug(""FULL Updated perms seq Num [old=""+ authzPermissions.getLastUpdatedSeqNum()+ ""], [new=""+ newAuthzPerms.getLastUpdatedSeqNum()+ ""]"");"
"<line2>      LOG.info(""Creating durable consumer"");"
"<line9>      logger.debug(""in Rule ""+ context.getRule().getName()+ "" added to context: [""+ t.toString()+ ""], deductions notified ""+ deductionsNotified);"
<line6>      logger.warn(warningReport);<line9>      logger.error(errorReport);
"<line36>      logger.error(""Error creating Entity"", t);"
"<line11>        LOGGER.error(""Error while checking (URL) URI: {}"", absoluteSystemId, e);<line16>        LOGGER.debug(""Checking: {}"", pathTest);<line19>      LOGGER.error(""Error while checking (Path) URI: {}"", absoluteSystemId, e);"
"<line14>      log.error(""Cannot load stream definitions from ""+ directory.getAbsolutePath()+ "" directory not exist"");<line17>      log.error(""Cannot load stream definitions from ""+ directory.getAbsolutePath()+ "" not a directory"");<line35>          log.error(""Error in reading file : "" + fullPathToStreamDefinitionFile, e);<line36>          log.error(""Error in converting Stream definition : "" + e.getMessage(), e);<line42>            log.error(""Error occurred when reading the file : "" + e.getMessage(), e);"
<line16>      LOG.debug(SSL_SOCKET_CIPHER_SUITE_LOG_MSG,socket,enabledCipherSuites,enabledCipherSuitePatterns,socket.getSSLParameters().getCipherSuites(),socket.getEnabledCipherSuites(),defaultEnabledCipherSuitePatterns,filteredCipherSuites);<line28>      LOG.debug(SSL_SOCKET_PROTOCOL_LOG_MSG,socket,enabledSecureSocketProtocols,enabledSecureSocketProtocolsPatterns,socket.getSSLParameters().getProtocols(),socket.getEnabledProtocols(),defaultEnabledSecureSocketProtocolsPatterns,filteredSecureSocketProtocols);
"<line15>                        log.info(""task running (""+ i+ ""): ""+ Tasks.current()+ "" ""+ Tasks.current().getStatusDetail(false));<line22>    log.info(JavaClassNames.niceClassAndMethod() + "" - submitting {} {}"", t, t.getStatusDetail(false));<line23>    log.info(""submitted {} {}"", t, t.getStatusDetail(false));<line24>    log.info(""done one ({}) {} {}"", new Object[] {interimResult, t, t.getStatusDetail(false)});<line31>    log.info(""blocked until ended ({}) {} {}, in {}"",new Object[] {i, t, t.getStatusDetail(false), Duration.of(timer)});<line37>    log.info(""ended ({}) {} {}, in {}"",new Object[] {i, t, t.getStatusDetail(false), Duration.of(timer)});"
"<line6>    logger.info(name + "": "" + total + "" from "" + results.size() + "" worker threads"");"
"<line36>      LOG.error(""get local address failed"", e);"
"<line5>      logger.error(""TTransportException writing to internal frame buffer"", e);<line7>      logger.error(""Exception writing to internal frame buffer"", e);"
"<line1>    log.debug(""findAll() - pageable: {}"", pageable);"
"<line7>      LOG.error(""Unable to decode CloudEvent data to Map<""+ keyClass.getName()+ "",""+ valueClass.getName()+ "">"",e);"
"<line4>        logger.debug(""sendCommand getRevision :: {}"",SierraMc87xxAtCommands.getFirmwareVersion.getCommand());"
"<line3>      ActiveMQRALogger.LOGGER.trace(""setDoubleProperty("" + name + "", "" + value + "")"");"
"<line13>                log.warn(""Failed to parse UI input '{}': {}"", input, parsingError.get());"
"<line5>          logger.info(""Provided private key is in PKCS #8 format"");<line11>        logger.warn(""Expected one of %s or %s but got %s"",PrivateKeyInfo.class, PEMKeyPair.class, parsedObject.getClass());"
"<line16>      log.warn(""Deleting submitted task before completion: ""+ removed+ ""; this task will continue to run in the background outwith ""+ this+ "", but perhaps it should have been cancelled?"");"
"<line20>      logger.error(""Unsupported physical plan: {}"", plan);"
"<line14>      logger.error(""Error on extracting widgetUtilizers : widget type code {}"", t);"
<line19>      log.error(systemException, systemException);
"<line9>      LOG.warn(""audit record too long: entityType={}, guid={}, size={}; maxSize={}. entity attribute""+ "" values not stored in audit"",entity.getTypeName(),entity.getId()._getId(),auditSize,auditMaxSize);<line15>        LOG.warn(""audit record still too long: entityType={}, guid={}, size={}; maxSize={}. audit will""+ "" have only summary details"",entity.getTypeName(),entity.getId()._getId(),auditSize,auditMaxSize);"
"<line14>      log.error(""Failed to reload api proxy service: {}"", e.getMessage());"
"<line6>        log.warn(""Versioning not implemented for bucket: {}: {}"", () -> bucketName, e::getMessage);<line6>        log.debug(e, e);"
"<line10>      logger.error(""Error loading the CatalogModel"", e);"
"<line18>            log.error(""The latest computed high watermark {} is smaller than the current ""+ ""value {}, which suggests that one of the voters has lost committed data. ""+ ""Full voter replication state: {}"",highWatermarkUpdateOffset,currentHighWatermarkMetadata.offset,voterStates.values());"
"<line14>    LOG.info(""Start writing product to file: "" + productFile.getName());<line15>    LOG.info(""formatting done in [ms]: "" + (System.currentTimeMillis() - t0));<line19>    LOG.info(""archiving done in [ms]: "" + (System.currentTimeMillis() - t0));"
"<line6>    LOGGER.info(""handleResponse for MessageType: {}"", messageType);<line11>        LOGGER.error(""Device Response not ok."", osgpException);<line16>      LOGGER.error(""Unexpected Exception"", e);"
"<line13>      LOGGER.debug(""Found match: ""+ row.get(row.getSchema().getField(""Name"").pos())+ "" id: ""+ row.get(row.getSchema().getField(""Id"").pos())+ "" shippingPostalCode: ""+ row.get(row.getSchema().getField(""ShippingPostalCode"").pos())+ "" billingPostalCode: ""+ row.get(row.getSchema().getField(""BillingPostalCode"").pos())+ "" billingStreet: ""+ row.get(row.getSchema().getField(""BillingStreet"").pos()));"
"<line3>      LOG.debug(""Free slot {}."", taskSlot, cause);<line4>      LOG.info(""Free slot {}."", taskSlot);"
"<line2>    logger.debug(""Starting KeepLatestContainerOnlyPolicy policy..."");<line11>    logger.debug(""Started {} policy"", this);"
"<line5>      log.info(""Former leader was reachable at "" + host + "":"" + port);<line7>      log.debug(""Exception thrown in getOracleClient()"", e);"
"<line3>      log.trace(""[IRAC] Topology Updated. Checking pending keys."");"
"<line29>        LOG.error(""allocation {} scheduleJob response error. responseMessage:{}"",analyzeInstance.toString(),responseMessage.toJSONString());<line34>      LOG.error(""allocation {} scheduleJob fail. "", analyzeInstance.toString(), e);"
"<line2>    LOGGER.debug(""init vitam tenant is mandatory : {}"", mandatory);"
"<line4>          logger.info(""{} initialization started."", this.getClass().getSimpleName());<line6>          logger.info(""{} initialization completed."", this.getClass().getSimpleName());<line12>        logger.info(""{} already initializing."", this.getClass().getSimpleName());<line14>        logger.info(""{} already started."", this.getClass().getSimpleName());"
<line10>      LOGGER.error(e.toString());
"<line8>      LOG.warn(""Failed to cleanup states with error {}."", e.toString());"
"<line15>    LOG.debug(""instanceName: ""+ instanceName+ "" isLinux: ""+ isLinux+ "" guestid: ""+ configSpec.getGuestId()+ "" OS: ""+ configSpec.getGuestFullName());"
"<line1>    log.info(""Click Sign In"");"
<line2>    log.debug(format, arg1, arg2);
"<line19>      LOGGER.error(""[capacityManagement] do4Update "", e);"
"<line10>        log.debug(""Ignore JobManager gained leadership message for {} because we are already connected to""+ "" it."",jobMasterGateway.getFencingToken());<line17>    log.info(""Establish JobManager connection for job {}."", jobId);"
"<line7>      log.info(""Error while receiving notification about key-value state de-registration"", e);"
"<line2>    Log.debug(""Test"");"
"<line2>    logger.info(""Reserved {} started."", ClassUtils.simpleClassName(this));"
"<line11>      LOGGER.warn(""Problem loading"", e);"
<line4>      LOGGER.debug(() -> new XMLOutputter(Format.getPrettyFormat()).outputString(metsDocument));
"<line1>    logger.error("""" + o);"
"<line1>    log.debug(""Loading blobs from the file system: "" + blobInfos);"
"<line5>      log.error(""Error reading message channel"", ex);"
"<line9>      log.error(""Failed to write meta-data file '{}'"", spMetadataFile, ex);"
"<line14>          LOGGER.warn(""Unable to copy {} to {} after "" + MAX_COPY_ATTEMPTS + "" attempts; skipping file"",child,destChild,e);"
"<line35>        LOG.debug(""key:{} is non-existent parent, permit access to user:{}"",keyName,context.getClientUgi());<line39>        LOG.debug(""user:{} has access rights for key:{} :{} "",context.getClientUgi(),ozObject.getKeyName(),hasAccess);<line45>      LOG.error(""CheckAccess operation failed for key:{}/{}/{}"", volume, bucket, keyName, ex);"
"<line9>    log.info(""validate and update file policy parameters started -- "");<line30>      log.info(""Updating the storage system policy started.."");"
"<line2>    logger.info(""Reading configuration for 1.0"");<line40>          logger.debug(""Found unexpected entry"");"
"<line42>                          LOG.trace(""IGNORED"", th);"
"<line5>        LOG.debug(""register transaction member {}"", memberId);<line9>          LOG.warn(""releasing overwritten transaction member {} / {}."", memberId, old.getMemberId());"
"<line6>    LOGGER.debug(""resolveCollision: {}, {}, {}, {}"", composite, existing, added, intersect);"
"<line4>      LOGGER.info(""GUI created"");"
"<line16>      LOG.error(e, ""Error while testing"");"
"<line7>      logger.warn(""Remove keyValue by key preFix: "" + preFix);"
"<line10>    log.info(""Created zookeeper entry {} with data {}"", path, work);"
"<line5>      LOG.debug(""Skipping index set with ZERO refresh interval <{}/{}>"", indexSetTitle, indexSetId);<line8>      LOG.debug(""Skipping non-writable index set <{}/{}>"", indexSetTitle, indexSetId);<line11>    LOG.debug(""Schedule index field type updating for index set <{}/{}> every {} ms"",indexSetId,indexSetTitle,refreshInterval.getMillis());<line20>                  LOG.debug(""Updating index field types for active write index <{}> in index set <{}/{}>"",activeWriteIndex,indexSetTitle,indexSetId);<line22>                  LOG.warn(""Active write index for index set \""{}\"" ({}) doesn't exist yet"",indexSetTitle,indexSetId);<line24>                LOG.error(""Couldn't get active write index"", e);<line25>                LOG.error(""Couldn't update field types for index set <{}/{}>"",indexSetTitle,indexSetId,e);"
"<line10>      LOGGER.error(""An error have been encountered while watching resources - leaving the redeploy mode"", e);"
"<line14>          logger.debug(""Skip async servlet request event. isAsyncStarted={}, dispatcherType={}"",request.isAsyncStarted(),request.getDispatcherType());<line21>        logger.info(""Failed to servlet request event handle."", t);"
"<line1>    log.debug(""findByModuleId() - moduleId: {}"", moduleId);"
"<line10>      log.error(""IO exception, Caused by {}."", e);"
"<line13>      LOGGER.warn(""Null text reader: ""+ item.getPath()+ "" (""+ (item.getLength() != null ? item.getLength() : ""null"")+ "" bytes)"");<line34>        LOGGER.debug(""Added to bulk request {}"", item.getPath());"
"<line3>    LOG.debug(""{}: Delete {} {}"", id, store, path);"
"<line7>      logger.info(""Channel {} only accepts StringType, RefreshType. Type was {}."",channelUID,command.getClass());"
"<line2>    LOG.info(""The following activity types were found: "" + activityTypes);"
"<line9>      log.info(""Ignoring exception, likely coming from Hadoop 1"", e);"
"<line5>      logger.warn(String.format(""Trying to distribute to streaming from tenant where streaming url or port aren't""+ "" set."",securityService.getOrganization().getId()));<line8>      logger.warn(""Streaming distribution directory isn't set (org.opencastproject.streaming.directory)"");"
"<line11>    LOGGER.info(""{} device log items deleted."", size);"
"<line1>    log.info(""Activation user with hash key {}"", userHashKey);<line3>      log.error(""Cannot activate user with hash key {}. Wrong hash key."", userHashKey);"
"<line38>      LOG.info(""Job failed. Try cleaning up temporary directory [{}]."", src);"
"<line7>        logger.warn(""Rename directory {} failed!"", partitionBase.getAbsolutePath());"
"<line2>    log.debug("""");"
<line17>      log.error(UNEXPECTED_ERROR_OCCURRED, exception);
"<line1>    log.debug(""attaching dirty SysExportBau instance"");<line3>      log.debug(""attach successful"");<line4>      log.error(""attach failed"", re);"
"<line16>              LOGGER.error(""Erreur lors la modification de la valeur d'une propriÃ©tÃ©."");<line16>              Session.get().error(getString(""common.error.unexpected""));"
"<line10>        log.warn(""Spec "" + this + "" ignoring unknown config key "" + entry.getKey());"
<line40>      log.error(systemException, systemException);
"<line65>      log.warn(""Could not convert given {} index query: [{}]"", resultType, query.getQuery());<line65>    log.info(""Converted query string with {} replacements: [{}] => [{}]"",replacements,query.getQuery(),queryStr);"
<line7>      log.error(exception, exception);
"<line1>    log.debug(""Synchronizing memory archive with file archive."");<line2>      log.warn(""Archive in memory out of sync with archive in file."");"
"<line8>      log.error(""Request handler for path specification with prefix {} is already registered"",spec.prefix);"
"<line8>        logger.debug(""tryAfter() returns false: interceptorScopeTransaction: {}, executionPoint: {}. Skip""+ "" interceptor {}"",transaction,policy,interceptor.getClass());"
"<line3>      LOG.debug(""Testing Journal Rolling"");"
"<line2>      log.info(""Publishing application instance active event: [application] ""+ appId+ "" [instance] ""+ instanceId);"
<line21>      log.error(systemException, systemException);
"<line6>    LOG.debug(""Sending empty message as there were no messages from polling: {}"", this.getEndpoint());"
"<line6>      logger.error(""Unable to access dump directory: "" + e.toString());<line19>          logger.error(""Incomplete local dump file data. Maybe delete ""+ dumpFile.getDumpfileDirectory()+ "" to attempt fresh download."");<line23>    logger.info(""Found "" + result.size() + "" local dumps of type "" + dumpContentType + "": "" + result);"
"<line1>    log.debug(""getting SysDatadict instance with id: "" + id);<line5>        log.debug(""get successful, no instance found"");<line6>        log.debug(""get successful, instance found"");<line9>      log.error(""get failed"", re);"
"<line4>        TRACE_BYTES.info(""Received: {}"", ByteBufUtil.hexDump(input));<line6>        LOG.trace(""Skipping data processing, proton transport previously errored."");<line18>      LOG.warn(""Caught problem during data processing: {}"", t.getMessage(), t);"
"<line52>      LOG.error(""setup failed "", x);"
"<line21>                log.info(""Started node: "" + ignite.name());<line32>      log.info(""Check metadata on node: "" + i);"
"<line5>    Log.debug(""Could not retrieve extended test db password, using default from unit tests"");"
"<line10>      LOGGER.info(""Populating authorizations for Initial Admin: '"" + initialAdminIdentity + ""'"");<line13>      LOGGER.info(""Populating authorizations for NiFi identities: [{}]"",StringUtils.join(nifiIdentities, "";""));<line16>      LOGGER.info(""Populating authorizations for NiFi Group: '"" + nifiGroupName + ""'"");"
"<line4>    LOG.info("">>>> "" + value);"
"<line3>    LOG.debug(""{} -> {}"", value, result);"
<line5>      logger.error(ex);
"<line12>      log.warn(""File-close threw the exception: "", continued);"
"<line6>    LOGGER.info(String.format(""\n\n=========== START new SAKULI Testsuite from '%s' ================="",testSuiteFolderPath));<line21>    LOGGER.debug(tempLogCache);<line25>      LOGGER.debug(""start new sakuli test suite"");<line27>      LOGGER.error(""Unexpected error occurred:"", e);<line29>      LOGGER.info(""========== TEAR-DOWN SAKULI TEST SUITE '{}' =========="", result.getId());"
"<line19>          log.error(""SSLException during unwrap"", e);"
"<line13>        LOG.warn(""Difference in auth of user {} for READ, scopes authorizer: {}, groups authorizer:""+ "" false, user: {}"",user.getId(),grantedByScopes,user);<line17>      LOG.warn(""Difference in auth of user {} for READ, scopes authorizer: {}, groups authorizer: true,""+ "" user: {}"",user.getId(),grantedByScopes,user);"
"<line8>      logger.debug(""Moving email, USER:{} UIDs:{} SRC:{} DST:{}"",udr.getUser().getLoginAtDomain(),messages,srcFolder,dstFolder);"
"<line17>      LOGGER.info(""We had some exception in Authentication filter"", e);"
"<line9>      LOG.info(""messageReceived: Already completed request (taskId = ""+ senderId+ "", requestId = ""+ requestId+ "")"");<line15>      LOG.info(""messageReceived: Already received response for (taskId = ""+ senderId+ "", requestId = ""+ requestId+ "")"");<line17>        LOG.debug(""messageReceived: Completed (taskId = ""+ senderId+ "")""+ requestInfo+ "".  Waiting on ""+ clientRequestIdRequestInfoMap.size()+ "" requests"");"
"<line10>      logger.error(""Save failed"", ex);"
"<line4>      logger.debug(String.format(""[Vm Tracer] detects stranger vm[identity:%s, state:%s]"", vmUuid, actualState));<line6>        logger.debug(String.format(""[Vm Tracer] detects stranger vm[identity:%s, state:%s] but it's already in cache,""+ "" skip firing event"",vmUuid, actualState));<line15>      logger.warn(String.format(""A strange vm[%s] was found on the host[%s], May cause problems, Please manually""+ "" clean this vm"",vmUuid, hostUuid));"
"<line3>      logger.error(""Cannot invoke signing without signing certificate. Add 'withSigningCertificate()' method""+ "" call or call 'withSignatureToken() instead.'"");"
"<line1>    log.debug(""finding StgMapMas instance by example"");<line8>      log.debug(""find by example successful, result size: "" + results.size());<line10>      log.error(""find by example failed"", re);"
<line19>      logger.warn(e.getMessage(), e);
"<line4>      logger.error(""Error in getCurrentSession()"", e);"
<line51>      log.error(systemException, systemException);
"<line6>    logger.info(""{} change logger level to DEBUG."", ctx.clazz().getName());"
"<line2>    logger.debug(""Replacing atom container at pos: "", position);"
"<line2>    logger.debug(""initializing bridge handler"");<line5>    logger.debug(""bridge handler host {}, port {}"", addr, port);"
"<line8>      LOG.error(""Can't connect to Solr: {}"", e);"
"<line4>      LOGGER.debug(""Added ON: {}"", channel.getConfiguration().get(""on""));<line7>      LOGGER.debug(""Added OFF: {}"", channel.getConfiguration().get(""off""));"
"<line6>        log.info(String.format(""Valid receipt %s in %s"", receipt, file));<line12>        log.warn(e.getMessage());<line15>      log.error(String.format(""Invalid receipt found in %s"", file));"
"<line5>        log.error(""Exception while setting ISSOUNDSTREAMING sensor {} to {}"",streamingSensor.getDisplayName(),mode);"
"<line17>      logger.error(""Error when get assignments"", e);"
"<line4>    LOGGER.trace(""Serving relative path {}"", relativePath);<line20>    LOGGER.trace(""Serving file {}"", file.getPath());"
"<line4>      logger.warn(""Linked Temperature device has no Value parameter: {}"", device);"
<line3>      logger.debug(msg);
"<line14>      LOG.warn(""Error calculating map update for enricher "" + this, t);"
"<line11>        logger.info(""TEST PLC STDOUT [{}]: {}"", field.getName(), value.getString());<line22>              logger.info(""Write failed"");<line24>        logger.info(""TEST PLC RANDOM [{}]: {}"", field.getName(), value.toString());"
<line26>      LOGGER.error(e.getMessage(), e);
"<line4>      LOG.warn(""Could not delete file '{}' from file store"", fileMeta.getId());"
<line1>    log.debug(format(arg0, arg1, arg2));
"<line4>      LOGGER.info(""Sorting data for group {} and id {} ({} input files, results will be stored in {})"",group,id,files.size(),sortedFiles);<line14>      LOGGER.info(""Aggregating data for group {} and id {} ({} input files, results will be stored in {})"",group,id,files.size(),aggregatedFiles);<line18>        LOGGER.info(""Sorting aggregated data for group {} and id {} (results will be written to {})"",group,id,sortedFiles);<line26>        LOGGER.info(""Deleting aggregated files in {} for group {} and {}"", aggregatedFiles, group, id);<line28>        LOGGER.info(""Not sorting aggregate data for group {} and id {} as there were no results"",group,id);<line31>      LOGGER.info(""Moving files of sorted data from {} to {} (group {}, id {})"",sortedFiles,outputDir,group,id);<line37>      LOGGER.info(""No files of sorted data so there is nothing to move"");"
"<line12>      logger.warn(""A null map was provided to withOverrides. This could be a bug in your script."");<line23>        logger.trace(""Removed key '""+ oKey+ ""': '""+ removed+ ""' from script params because it was ""+ oVal+ "" in overrides"");<line27>        logger.trace(""Overrode key '"" + oKey + ""': from '"" + was + "" to "" + oVal);"
"<line6>    LOG.debug(""Creating Queue entry : ""+ object.getId()+ "" From : ""+ exeQueueId+ "" targetState : ""+ targetState.toString());<line24>          LOG.debug(""Dep inserted : "" + retDep.getItem());<line38>          LOG.debug(""Dep inserted from old entries : "" + retDep.getItem());"
"<line2>      _Employee.LOG.debug(""updating lastName from "" + lastName() + "" to "" + value);"
"<line72>      log.info(""Generating metadata without signing key, KeyStore doesn't contain any default private""+ "" key, or the signingKey specified in ExtendedMetadata cannot be found"");<line78>      log.info(""Generating metadata without encryption key, KeyStore doesn't contain any default private""+ "" key, or the encryptionKey specified in ExtendedMetadata cannot be found"");"
"<line9>    logger.debug(LoggingMarkers.DUPLICATES, ""Same size: {}"", sameSize);"
"<line4>    log.info(""replicateCollectionAsynchronously()"");<line13>    log.info(""irodsCollectionAbsolutePath:{}"", irodsCollectionAbsolutePath);<line13>    log.info(""resourceName:{}"", resourceName);<line13>    log.info(""delayInMinutes:{}"", delayInMinutes);<line32>    log.info(""result of action:{}"", result.getRuleExecOut().trim());"
"<line19>        logger.info(""found domain = "" + providerDomain);<line22>        logger.info(""found gbids = "" + gbidsParameter);<line25>        logger.info(""globalOnly = "" + globalOnly);<line28>        logger.info(""expectedFailure = "" + expectedFailure);<line30>      logger.error(""failed to parse command line: "" + e);<line36>      logger.debug(""Searching for providers on domain \""{}\"", gbids \""{}\"""", providerDomain, gbids);<line38>      logger.debug(""Searching for providers on domain \""{}\"""", providerDomain);<line41>    logger.debug(""Using the following runtime module: "" + runtimeModule.getClass().getSimpleName());<line41>    logger.debug(""Searching for providers on domain \""{}\"""", providerDomain);"
"<line4>      elasticClientLogger.debug("">>> delete({})"", delete);<line13>        log.error(""Cannot delete {} documents: {}"",failureMessages.size(),String.join("" - "", failureMessages));<line22>      log.error(""Cannot delete with query {}"", delete.getQuery(), e);"
"<line11>          LOG.error(""Failed to close the connection"", e);"
"<line15>          LOGGER.error(""Error while listing containers, while parsing resource "" + next, e);"
"<line7>        LOGGER.debug(""Wrong file extension. May be only {}"", VALID_IMAGE_EXTENSIONS);<line10>      LOGGER.error(""Failed to handle image ByteArrayInputStream"", e);"
"<line10>      log.error(""Error during first step of installation"", e);"
"<line11>      log.error(""Warning: target extension point '""+ extension.getExtensionPoint()+ ""' of '""+ extension.getTargetComponent().getName()+ ""' is unknown. Check your extension in component ""+ extension.getComponent().getName());"
"<line22>      logger.warn(em.toString());<line26>      logger.warn(String.format(""%s"", LogUtil.getStackTrace(e)));"
"<line6>      LOG.info(""Waiting for Host state "" + HostState.IDLE + "" to propagate."");<line16>            LOG.error(""Failed to process command: "" + command, e);<line20>        LOG.info(""Interrupted in main loop. Exiting."", e);<line23>    LOG.info(""Partition server main thread is stopping."");"
"<line10>      LOGGER.error(""Exception in WebDriverManager while getWebDriver "", e);"
"<line11>        LOG.warn(""Cert Container conversion failed: "" + e.getLocalizedMessage(), e);"
"<line11>      logger.error(""Input/Output exception while reading from a random access file. Stack trace follows"",ioe);"
"<line2>    LOG.info(""Removing Acl: acl->"" + acls + "" resource->"" + resource);<line16>        LOG.error(""Failed to remove acls."", kex);"
"<line18>    LOGGER.debug(""IdentifiersMatch={} ({}, {})"",matches,dependency1.getFileName(),dependency2.getFileName());"
"<line24>          logger.debug(""MDL file part read: "", buffer);<line30>            logger.error(""Error while reading next molecule: "" + exception.getMessage());<line30>            logger.debug(exception);<line56>      logger.error(""Error while reading next molecule: "" + exception.getMessage());<line56>      logger.debug(exception);"
"<line9>      LOG.trace(""Registered service: {}..."", brokerPoolService.getClass().getSimpleName());"
"<line17>        log.info(""Node {} is unavailable as expected!"", HapiPropertySource.asAccountString(node.get()));<line20>        log.warn(spec.logPrefix() + this + "" failed - {}"", t);<line21>        log.warn(spec.logPrefix() + this + "" failed {}!"", t.getMessage());<line29>      log.error(message);"
"<line12>      logger.error(""error in save"", t);"
"<line4>    LOG.debug(""Loaded conf "" + conf);"
<line10>        logger.error(error);<line39>      logger.error(error, e);
"<line6>      LOG.error(""Failed to get revision {} of note {}"", revId, noteId, e);"
<line19>      LOGGER.info(Arrays.toString(counters1));<line33>      LOGGER.info(Arrays.toString(counters2));
"<line6>      logger.trace(""Serial event: {}, new value:{}"", seEvent.getEventType(), seEvent.getNewValue());<line27>      logger.warn(""RuntimeException during handling serial event: {}"", seEvent.getEventType(), e);"
"<line15>          log.warn(""SCAN: remove expired channel from ConsumerManager consumerTable. channel={},""+ "" consumerGroup={}"",RemotingHelper.parseChannelRemoteAddr(clientChannelInfo.getChannel()),group);<line20>        log.warn(""SCAN: remove expired channel from ConsumerManager consumerTable, all clear,""+ "" consumerGroup={}"",group);"
"<line7>            LOGGER.error(""Failed to create dump directory; please double-check your filesystem permissions"");<line10>          LOGGER.error(""Dump directory path does not point to a valid directory"");<line16>      LOGGER.error(""Failed to initialize networking"", e);"
"<line4>      log.debug(""logback loglevel is starting..."");"
"<line15>        LOG.debug(""UP UPDATE  \t""+ startVertex.getId()+ ""\t""+ maxVertex.getId()+ ""\t""+ startVertex.getHierarchyWeight()+ "" --> ""+ (maxVertex.getHierarchyWeight() - 1));"
"<line12>      logger.error(""Exception occurred in AAIUpdateTasks updateManagementV6AddressVnf"", ex);"
"<line53>              log.warn(""OrganizationPersistenceImpl.fetchByC_ERC(long, String, boolean) with parameters""+ "" (""+ StringUtil.merge(finderArgs)+ "") yields a result set with more than 1 result. This violates the logical""+ "" unique restriction. There is no order guarantee on which result is""+ "" returned by this finder."");"
"<line2>    logger.debug(""Adding product with coefficient: "", product, """" + coefficient);"
"<line2>      logger.debug(""All anticipated view responses received - notifying waiting thread"");<line5>      logger.debug(""Still waiting for these view replies: {}"", notRepliedYet);"
"<line1>    logger.debug(""getAuthorizedPublishers started..."");"
<line18>      logger.debug(msg);
"<line2>    LOG.debug(""KyloDatabaseConnectionInspection.inspect"");"
"<line6>      LOG.error(""Interrupted waiting for finish"", e);"
"<line1>    log.info("""");<line1>    log.info(""dependencies, and where they are available:"");<line4>      log.info(String.format(""%s%s (%s)"", StringUtils.repeat(""|"" + sep, i), rep.getId(), rep.getUrl()));<line5>    log.info(StringUtils.repeat(""|"" + sep, repCount));"
"<line8>    LOGGER.debug(""Start mass updating archive units with Dsl query {}"", queryDsl);<line64>      LOGGER.error(""An error occured while mass updating archive units"", e);<line70>      LOGGER.error(BAD_REQUEST_EXCEPTION, e);"
"<line4>      LOGGER.debug(""Registering application service MBean under object name: {}"", objectName);<line6>      LOGGER.debug(""Re-registering Application Service MBean"");"
"<line19>      LOGGER.info(""Configuring jdbcloader from "" + url.toString());"
"<line19>      logger.error(""error in end tag"", t);"
"<line5>    logger.debug(""About to unmarshal task content parameters from payload: '{}'"", payload);<line7>    logger.debug(""About to set content of a task with id '{}' with data {}"", taskId, parameters);"
"<line3>        log.debug(""Executing complete tenant extension"");<line10>        log.debug(""Complete tenant script returned:"" + output);<line13>        log.error(""Could not execute complete tenant extension"", e);"
"<line1>    log.debug(""finding MYesno instance by example"");<line8>      log.debug(""find by example successful, result size: "" + results.size());<line10>      log.error(""find by example failed"", re);"
"<line3>    logger.info(""STANDALONE mode is set..."");"
"<line7>    LOG.info(""Deleting all segment files from gs location [bucket: '%s' prefix: '%s']"",accountConfig.getBucket(), accountConfig.getPrefix());<line15>      LOG.error(""Error occurred while deleting task log files from gs. Error: %s"", e.getMessage());"
"<line2>    LOG.trace(""Running job: {}"", job);<line4>      LOG.error(""Unexpected no (null) main worker on job: {}"", job);<line11>      LOG.error(""Direct Exception (not failed Future) when executing job, won't even retry: {}"", job, e);<line19>            LOG.trace(""Job completed successfully: {}"", job.getKey());<line24>            LOG.error(""Job {} failed {}"", job.getKey(), cause.getMessage());<line28>    LOG.trace(""Finished running job: {}"", job);"
"<line4>      log.info(""Invalid map to add the entries"");"
"<line10>          logger.warn(format(""Creating missing gene product with id ''{0}'' because reaction ''{1}'' uses this""+ "" id in its gene-product association."",id, reactionId));<line15>        logger.info(format(""Updating the id of gene product ''{0}'' to ''{1}''."", gp.getId(), id));"
"<line3>    LOG.debug(String.format(""started opertation %s. polling until complete."", operation.getName()));"
<line6>      logger.error(e.getMessage());
"<line53>      LOGGER.error(""Exception:"", e);"
"<line17>      LOG.error(""Failed fetching using indexQuery: "" + e.getMessage());"
"<line9>    LOGGER.info(""Created multibranch pipeline: "" + pipelineName);"
"<line18>      log.debug(""Adding object {}: {}"", i++, file.getName());"
"<line3>      LOG.warn(""{} is not configured. We recommend adding this setting. ""+ ""Falling back to {} instead."",definition.getLocationConfigKey(),HddsConfigKeys.OZONE_METADATA_DIRS);"
"<line14>      logger.debug(""there are {} constraint violations at method {}."", violations.size(), method);<line18>        logger.debug(""added message {}={} for contraint violation"", category, msg);"
"<line11>      logger.debug(""Could not get version from GitHub"", e);<line24>      logger.warn(""Error while parsing the Opencast version from GitHub"", e);"
"<line2>    LOG.info(""Trying to start the ZooKeeper container"");<line4>    LOG.info(""ZooKeeper instance running at {}"", getConnectionString());"
"<line2>    log.debug(""NunchukRemovedEvent {}"", arg0);"
"<line6>        LOG.debug(""detected DelayOperator having APPLICATION_WINDOW_COUNT not equal to 1"");<line15>            LOG.debug(""detected DelayOperator does not immediately output to a visited operator""+ "" {}.{}->{}.{}"",om.getName(),downStream.getSource().getPortName(),successor.getName(),sink.getPortName());"
"<line6>        logger.trace(""\tFound "" + children.size() + "" files in the repository"");<line13>              logger.warn(messages.getString(""PentahoMetadataDomainRepository.WARN_0001_FILE_WITHOUT_METADATA"",child.getName()));<line20>            logger.trace(""\tprocessing file [type=""+ type+ "" : domainId=""+ domainId+ "" : locale=""+ locale+ ""]"");"
"<line3>      logger.warn(this + "" is already closed, but close() was invoked!"");<line9>          logger.debug(""Stack trace when close() no. {} of {}: "", i, numCloses);<line9>          logger.debug(""Thread name: {}"", closeEvent.threadName);<line18>      logger.debug(""Method close() invoked, usage decremented to {} for {}"", _usageCount, this);<line23>        logger.debug(""Closing {}"", this);"
"<line5>    LOG.debug(name + "" cancelling ongoing future"");<line8>      LOG.error(""error cancelling future for "" + name, e);"
<line13>      logger.warn(e.getMessage(), e);
"<line14>        log.error(""Failed to end transaction"", e);"
"<line3>      log.info(""privateKey: {}"", privateKey);<line3>      log.info(""publicKey: {}"", privateKey.getPublicKey());"
"<line3>        logger.debug(""Creating a new Solr Client."");<line5>        logger.debug(""Returning the solr client owned by Solr6Index."");<line8>      logger.debug("" No Solr6Index available. Will return null"");"
"<line2>    logger.warn(""Schema validation error parsing Flow Configuration at line {}, col {}: {}"",e.getLineNumber(),e.getColumnNumber(),e.getMessage());"
"<line1>    log.info(""Starting to clean up internal topics {}."", topicsToCleanUp);<line5>      log.info(""Going to cleanup internal topics: "" + topicsStillToCleanup);<line24>                log.info(""Internal topic {} to clean up is missing"", topicName);<line25>                log.info(""The leader of internal topic {} to clean up is not available."", topicName);<line26>                log.info(""Cleaning up internal topic {} timed out."", topicName);<line27>                log.error(""Unexpected error during cleanup of internal topics: "", cause);<line56>    log.info(""Completed cleanup of internal topics {}."", topicsToCleanUp);"
"<line22>        logger.error(""commit index exception, commitMap: {}, app: {}"", entry.getValue(), app, e);"
"<line13>      LOG.error(""Error while checking validity of the document"", e);"
<line40>      log.error(UNEXPECTED_ERROR_OCCURRED, exception);
"<line3>      logger.debug(""Client Http Session received OK"");"
"<line31>        logger.info(""Concat video job {} started on a remote composer"", r.getId());"
"<line7>    log.info(""Avvio elaborazione tracciato ""+ formato+ "" [""+ tracciato.getId()+ ""] per il Dominio [""+ codDominio+ ""]"");<line35>      log.error(""Errore durante l'elaborazione del tracciato ""+ formato+ "" [""+ tracciato.getId()+ ""]: ""+ e.getMessage(),e);"
"<line3>    log.info(""[{}]: Got publish request from {} on {}"",containerId,connection.getRemoteContainer(),target.getAddress());<line49>            log.info(""[{}]: Failed creating publisher {} for address {}"",containerId,connection.getRemoteContainer(),receiver.getRemoteTarget().getAddress(),result.cause());"
"<line4>        log.debug(""Loading "" + sourceURL);"
"<line12>      logger.debug(""Unable to find beans exposing the `BindableService` interface - not starting the gRPC""+ "" server"");"
"<line7>      logger.error(""Error reading the content from version {} "", versionId, e);"
"<line5>      log.trace(""table:"" + triStateName + "" got "" + sharedTriState + "" from "" + sharedTriStates);<line5>      log.trace(""table:""+ triStateName+ "" checking to see if sharedTriState ""+ sharedTriState+ "" is the same as expected value:""+ expectedValue);"
"<line5>      LOG.debug(""cache hit for {}"", pid);<line8>      LOG.debug(""cache miss for {}"", pid);"
"<line12>        log.error(""Cartridge group not found group-name: "" + groupCtxt.getName());"
"<line4>      logger.trace(""DoLock.execute() : do workaround for user agent '"" + userAgent + ""'"");"
"<line8>      log.info(""Cassandra Sink Task trying to put()"");<line24>              log.error(""parseArray error, fieldValue:{}"", fieldValue);<line29>          log.info(""Cassandra Sink Task trying to call updater.push()"");<line31>            log.error(""push data error, keyspaceName:{}, cfName:{}, entryType:{}, fieldMap:{}"",keyspaceName,cfName,fieldMap,entryType);<line35>      log.error(""put sinkDataEntries error, {}"", e);"
"<line2>      LOG.debug(""Creating communication channel..."");<line3>      LOG.debug(""Reconnecting communication channel..."");<line6>    LOG.debug(""\tCreating channel to listening for incoming connections in port {}"", config.getPort());<line8>    LOG.debug(""\tListening channel created and connected: {}"", listeningChannel);"
"<line8>    LOG.debug(""Created cache key; {}"", cacheKey);"
<line6>      log.error(msg);
"<line10>            log.trace(""No listeners registered to topic {} so closing down MessageListenerWrapper"",subsribedToTag.getTopicName());<line14>              log.error(""Failed to unregister properly from a Tag update; subscriptions will be""+ "" refreshed."");"
"<line7>      logger.info(""Processing: ["" + data + ""]"");"
"<line11>        logger.warn(""A lifecycle error observer threw an exception"", ex);"
"<line5>        log.debug(""ACCESS GRANTED ["" + object.toString() + ""]"");<line8>    log.debug(""ACCESS DENIED ["" + object.toString() + ""]"");"
"<line13>        logger.error(""[monitor] sentinel: {} : {}"", sentinel, e.getMessage());<line14>        logger.error(""[monitor] sentinel: {}"", sentinel, e);"
"<line2>      logger.trace(""getDiscoveryInitialWaitTimeout()"");"
"<line3>      LOG.debug(""Closing source driver for resource \""{}\"" in process \""{}\"""",getName(),script.getName());<line6>        WGLOG.error(""E13001"", profile.getResourceName(), script.getName(), path);<line12>      WGLOG.error(e, ""E13001"", profile.getResourceName(), script.getName(), path);<line18>        WGLOG.warn(e, ""W13001"", profile.getResourceName(), script.getName(), path);"
"<line7>    LOG.debug(""[testScheduleCampaign] {}"", rs);"
"<line3>      LOG.debug(""Securing route {} using Shiro policy {}"", route.getRouteId(), this);"
"<line1>    LOG.debug(""Config loaded"" + authConfig);"
"<line4>      log.debug(""Get {}: {} {} {}"",new String[] {o != null ? ""Hit"" : ""Miss"", getLogLayerId(layer), category.toString(), key});"
"<line42>      log.error(""Error during find All , Caused by: ."", ioex);"
"<line2>    log.info(""Setting up disk quota periodic enforcement task"");"
"<line5>      logger.debug(""asyncTraceContext.continueAsyncTraceObject() AsyncTrace:{}"", asyncTrace);<line9>        logger.warn(""Duplicated async trace scope={}."", oldScope.getName());<line13>        logger.debug(""start async trace scope"");"
"<line3>    logger.info(""registered scripting gauge:"" + name);"
"<line2>    logger.debug(""Getting reactionScheme count: "", super.getReactionSchemeCount());"
<line17>      LOGGER.error(ERROR_ADDING_DATA, e);
"<line7>        log.info(""Starting to terminate all members in cluster [""+ getClusterId()+ ""] ""+ ""Network Partition [""+ instanceContext.getNetworkPartitionId()+ ""], Partition [""+ partitionContext.getPartitionId()+ ""]"");<line9>        log.info(String.format(""Terminating all remaining members of partition [partition-id] %s""+ "" [application-id] %s"",partitionContext.getPartitionId(), getAppId()));<line16>        log.info(""Sending instance cleanup event for the active member: [member-id] "" + memberId);<line27>          log.debug(""Moving pending member [member id] "" + memberId + "" to obsolete list"");"
"<line7>      LOGGER.warn(""No event on flow completion"", messagingException);"
"<line22>    logger.debug(""final cluster spec: "" + manifest);"
"<line1>    logger.debug(""activate()"");"
"<line4>      LOG.error(errorMessage);<line8>        LOG.debug(""Acquired {} {} lock on resource {}"", lockType, resource.name, resourceName);"
"<line39>      LOGGER.info(""discovered {} ctors, {} methods for class {}"",declaredConstructors.length,methods,clazz);"
"<line16>        log.error(""Something went wrong with fetching the project"", e);<line26>        log.error(""Could not fetch linked projects or linked releases in projects view."", e);<line43>          log.error(""Could not put empty linked projects or linked releases in projects view."", e);"
<line13>        LOGGER.warn(e.toString(), e);
"<line3>    LOG.trace(""postNotice"");"
"<line4>      log.info(""Stopping Talend API server (port {})"", handler.getPort());<line7>        log.warn(e.getMessage(), e);<line21>                log.error(e.getMessage(), e);"
"<line8>          LOG.warn(""{}; {}; {}; {}"",loggable.data.length,page.getData().length,ph.getDataLength(),loggable.size);<line24>      LOG.warn(""An IOException occurred during redo: {}"", e.getMessage(), e);"
<line35>            LOGGER.warn(e.getMessage(), e);
"<line2>    log.debug(""Reading CAs for Automation document [{}] ({}) in project [{}] ({})"",aDocument.getName(),aDocument.getId(),aDocument.getProject().getName(),aDocument.getProject().getId());"
"<line7>      log.error(""Error while reading traits"", e);"
"<line8>      LOGGER.debug(""File's own path: {}"", filePath);"
"<line3>    log.info(""Portlet [""+ (portletName != null ? portletName : this.getClass().getSimpleName())+ ""] has been MODIFIED."");"
"<line5>      LOGGER.info(""Attempt to delete mailbox {} for user {} that does not exists"",mailboxPath.getName(),mailboxPath.getUser());"
<line2>    log.debug(event);
"<line10>        LOGGER.info(""Catalog sync interrupted early, exiting"");"
"<line4>      ActiveMQRALogger.LOGGER.trace(""createConnectionConsumer("" + destination + "", "" + pool + "", "" + maxMessages + "")"");"
"<line51>              log.warn(""ObjectFieldPersistenceImpl.fetchByODI_N(long, String, boolean) with parameters (""+ StringUtil.merge(finderArgs)+ "") yields a result set with more than 1 result. This violates the logical""+ "" unique restriction. There is no order guarantee on which result is""+ "" returned by this finder."");"
"<line1>    log.debug(""deleting TmpBauSel instance"");<line3>      log.debug(""delete successful"");<line4>      log.error(""delete failed"", re);"
<line2>    logger.info(json);<line6>      logger.error(e.getMessage());
"<line11>            LOG.info(""Model contains Equipment Core data profile in model {}"",m.get(CgmesNames.FULL_MODEL));"
"<line21>                    LOG.info(""File Metadata resourceId is {} "", file.getMetadata().resourceId());"
"<line3>      LOG.debug(""Not setting up non-writable index set <{}> ({})"", getConfig().id(), getConfig().title());<line6>      LOG.info(""Found deflector alias <{}>. Using it."", getWriteIndexAlias());<line7>      LOG.info(""Did not find a deflector alias. Setting one up now."");<line9>        LOG.info(""Pointing to already existing index target <{}>"", currentTarget);<line12>        LOG.info(msg);"
"<line29>      logger.error(""Unable to update index"", e);"
"<line31>          LOG.warn(""Ignoring unsupported Workflow node type {}"", node.getType());"
"<line6>    LOG.trace(""Processing PUBLISH message, topic: {}, messageId: {}, qos: {}"", topicName, messageID, qos);<line10>      LOG.debug(""Drop connection because of invalid topic format"");<line28>        LOG.error(""Unknown QoS-Type:{}"", qos);"
"<line3>      LOG.debug(""discarding unexpected response [correlation-id: {}]"", correlationId);"
"<line5>        logger.debug(""Uninstalled '{}'"", name);<line9>      logger.debug(""Failed uninstalling '{}': {}"", name, e.getMessage());"
<line19>      log.error(systemException, systemException);
"<line15>      logger.error(""Error skipping to: "" + pos + "": "" + e.getMessage(), e);"
"<line5>    LOG.info(""---------------- Response with content received from '{}' ----------------\n""+ ""---------------- START Response-Body ----------------\n""+ ""{}\n""+ ""---------------- END Response-Body ----------------"",request.getURI(),response);"
"<line8>    LOGGER.info(""Move paragraph {} {} {}"", noteId, paragraphId, newIndex);"
"<line13>      logger.info(""Loading file: "" + inputFile);<line31>        logger.error(""Error loading file: "" + inputFile + "". Nested exception is: "" + ex.getMessage(), ex);"
"<line2>      log.debug(""sendWindowChange({}) cols={}, lines={}, height={}, width={}"",this,columns,lines,height,width);"
<line21>      log.error(systemException, systemException);
"<line11>      logger.debug(""There is currently no Active Event File for {}. Will not purge oldest events until the""+ "" Active Event File has been established."",this);<line19>        logger.info(""{} Deleted {} event file ({}) due to storage limits"",this,eventFile,FormatUtils.formatDataSize(fileSize));<line21>        logger.warn(""{} Failed to delete oldest event file {}. This file should be cleaned up manually."",this,eventFile);"
"<line7>        LOG.debug(""succeeded in binding server to port "" + port);<line9>        LOG.debug(""failed to bind to port "" + port);"
"<line7>        logger.warn(""Failed to stop command with pid {}"", pid.getPid());"
<line2>    LOGGER.debug(String.format(Messages.Log.RECEIVING_REMOTE_REQUEST_S, iq.getID()));
"<line11>          log.error(""Invalid Command name in SimpleCommand: {}, skipping"", cmd);"
"<line12>      LOG.error("""", e);"
"<line2>    LOG.info(""Deleting all completed log files..."");<line8>      LOG.info(""Deleting completed log: {}"", log);<line10>    LOG.info(""Finished deleting all completed log files."");"
"<line13>        logger.debug(""Failed to create snapshot for region: {}. Continuing with next region."",subRegion.getFullPath(),e);"
"<line26>      logger.error(""Error in processing input row:"", e);"
"<line3>      LOGGER.trace(format(""Encode raw '%s' tuple2 object %s"", fieldName, o));"
"<line2>      log.warn(""Alreadying calling sync!  Skipping this request"");<line5>    log.warn(""================================ sync !!! =============================="");<line16>      log.error(""sync threw"", e);<line18>    log.info(""Sync completed"");"
"<line4>      logger.debug(""writeElement: "" + sbmlElementToWrite.getClass().getSimpleName());"
<line6>      log.warn(message, e);
"<line29>            logger.warn(fex.getMessage());<line29>            logger.warn(ExceptionUtils.getStackTrace(fex));<line30>          logger.debug(""Committing batch after {} objects"", count);"
<line24>      log.error(exception.getMessage(), exception);
"<line2>    LOGGER.debug(""=======Pinging HBase For:"" + key);<line13>          LOGGER.debug(""=======Found intel from source: "" + k);"
"<line4>      log.info(""Register resource and observer for config resource uri {}"", configResourceUri);<line8>      log.warn(""No configuration resource manager and/or no configuration resource uri and/or no schema ""+ ""resource name defined. Not using this feature in this case"");"
"<line4>    LOG.info(""Created new process file and writer over {} "", processFile.getAbsolutePath());"
"<line24>        LOG.debug(""await async execution"");<line29>      LOG.debug(""Exception while onResourceChange is invoked"", e);<line30>    LOG.debug(""group={}, changed={}"", group.getName(), isChanged);"
"<line18>    log.info(""in build(local) with archive="" + archive + "", and consolidate="" + consolidate);<line41>      log.error(""execption in build:"", any);"
"<line15>        LOGGER.warn(""Interrupted while requesting repository statistics."", e);"
"<line3>      ActiveMQRALogger.LOGGER.trace(""setFloat("" + name + "", "" + value + "")"");"
"<line28>          logger.warn(""unknown produce code {}"", code);"
"<line25>            LOG.debug(String.format(""Assigned Java system property for property field name=%s value=%s"",propertyField.getName(), propertyField.getValue()));<line30>      LOG.error(String.format(""SecurityException while processing Java system property for propertyField=%s"",AtlasModelFactory.toString(propertyField)),e);"
"<line4>        LOG.debug(OBJECT_MAPPER.writeValueAsString(metrics));<line6>      LOG.error(""Unable to marshal data to JSON"", e);"
"<line30>            log.debug(""initChannel("" + ch + "") listener="" + listener + "" ignoring abort event exception"",exc);"
"<line1>    logger.trace(""runReboot() called on {}"", getThing().getUID());<line6>              logger.info(""Reboot command {}sucessfully sent to {}"",bcp.isCommunicationSuccessful() ? """" : ""un"",getThing().getUID());"
"<line1>    log.info(""Sample point called "" + x + "" "" + y);"
"<line10>      LOG.info(""The amount of cpu cores must be a positive integer on Yarn. Rounding {} up to the""+ "" closest positive integer {}."",cpuCoresDouble,cpuCoresLong);"
"<line18>        log.warn(""Subqueries in from clause not supported by {} Query : {}"", this, this.query);<line21>        log.warn(""Join in from clause not supported by {} Query : {}"", this, this.query);<line26>      log.warn(""Union queries are not supported by {} Query : {}"", this, this.query);<line32>      log.warn(""Having queries are not supported by {} Query : {}"", this, this.query);<line38>      log.warn(""Order by queries are not supported by {} Query : {}"", this, this.query);<line46>        log.info(""Rewritten query from build : "" + rewritternQueryText);"
"<line13>        LOG.debug(""Allocation to dirIndex {} rejected: {}"", dirIndex, dir.toBlockStoreLocation());"
"<line2>      LOG.trace(""clearing JMX Cache"" + StringUtils.stringifyException(new Exception()));"
"<line2>    log.error(""Request: {} raised following exception."", req.getRequestURL(), exception);"
"<line12>        LOGGER.error(e.getMessage(), e);<line13>          LOGGER.debug(Thread.currentThread().getName()+ "" points:""+ uploadCounter.get()+ "" uploaded before error, now release the lock."");<line19>      LOGGER.debug(Thread.currentThread().getName()+ "" no more points, total points:""+ uploadCounter.get()+ ""  uploaded"");"
"<line1>    log.info(""[{}] Clearing backlog for cursor {} in the topic."", topic, cursorName);"
"<line2>      log.debug(""Executing command "" + request.getURI());<line29>        log.debug(""This request is not a CommandHostRequest. Ignoring request for URI {}"",request.getURI());<line30>        log.debug(""This request is not a CommandHostRequest. Ignoring null request"");"
"<line11>      LOGGER.warn(""Cannot find histogram for coverage '"" + coverageName + ""'"");"
"<line2>    AttributePathServiceTest.LOG.debug(""start simple attribute path test 2"");<line3>    AttributePathServiceTest.LOG.debug(""end simple attribute path test 2"");"
"<line3>    logger.info(""Performing upgrade to the Vorto Repository and its content..."");<line5>        logger.info(""Executing task - "" + task.getShortDescription());<line8>          logger.error(""Problem executing upgrade task"", problem);<line10>        logger.info(""NOT Executing task - "" + task.getShortDescription() + "". Conditions not met."");"
"<line7>    logger.debug(""checkUniqueConstraintByConsumerSystemIdAndServiceIdAndProviderSystemId started..."");"
"<line17>              logger.error(""Error occured while generating JSON!"");"
"<line6>    log.info(""servoSetVelocity {} id {} velocity {}"", servo.getName(), getDeviceId(servo), speed);<line8>      log.error(""{} has null deviceId"", servo);"
"<line11>                LOG.error(""Null {}"", ItemTO.class.getSimpleName());"
<line21>        LOG.debug(e.getMessage());
"<line12>      LOG.warn(""Unable to delete repository lock file"");<line17>        LOG.error(""Unable to clear system property: "" + identifier, e);"
"<line2>      log.warn(""Closing session for you"");"
"<line18>        LOG.error(""Suppressing exception from closing tables"", t);"
<line5>      log.error(msg);
"<line12>            LOG.warn(""Node is no longer leader for partition {}, tried to respond on request with id {}"",partitionId,requestId);<line17>              LOG.trace(""Send response to request {} for topic {}"", requestId, topicName(partitionId));<line20>            LOG.trace(""Wasn't able to send response to request {} for topic {}"",requestId,topicName(partitionId));"
"<line3>    logger.info(""Registering error: {}, code: {}, body:\n{}"", key, code, error);"
"<line12>      logger.debug(""Using evaluator with index {} (wnc:{}, rcs:{}, rst:{}, rs:{}) "",new Object[] {evaluatorIndex, wnc, restrictsConnectionStates, restrictsSocketTypes, restrictsSockets});"
"<line7>      log.debug(e, ""SQLException when abort"");"
"<line6>        logger.debug(""RowKeyComparisonFilter: "" + (this.keepRow ? ""KEEP"" : ""FILTER"") + "" row "" + inputTuple);"
"<line3>      logger.error(""getDoubleProperty(): argument 'name' must be non-null"");"
"<line2>      log.trace(""add margins to extent "" + extent);"
"<line5>      LOG.warn(""No state retention interval configured for a query which accumulates state. Please""+ "" provide a query configuration with valid retention interval to prevent excessive""+ "" state size. You may specify a retention time of 0 to not clean up the state."");"
"<line5>        LOGGER.info(""Dropped a pending dataverse: "" + dataverse.getDataverseName());"
"<line12>    LOGGER.debug(""Creating a Dataset<Row> from path {} with option mergeSchema=true"", store.getGraphPath());"
"<line8>      log.debug(""Found more than one domain in repository {}, using first one."",session::getRepositoryName);<line11>    log.debug(""Using default domain {}"", defaultDomainPath);"
<line10>      Logger.debug(response.getBody().toString());
"<line10>    LOG.info(""Opening split "" + split);"
"<line2>    LOG.debug(""Client connected"");"
"<line33>      LOG.debug(""Failed to delete {} flows"", deviceFlowRegistry.size(), ex);"
"<line32>    LOG.info(""{} removed from Flow Controller"", service, this);"
"<line1>    log.debug(""Starting scheduler service ..."");<line9>          log.error(""Failed to schedule task for class "" + taskDefinition.getTaskClass(), e);"
"<line1>    log.info(""CFDP sending data [{}, {}]"", start, end);<line6>      log.warn(""Error reading from file"", e);"
"<line2>      log.info(""Recieved ping for [{}]"", agentId);"
"<line1>    logger.info(""activate..."");"
"<line7>    log.debug(""deleteLayout() - id={}"", id);"
"<line10>        log.info(""Expected exception: "" + e);"
"<line8>      logger.info(getClass().getName()+ "".freeInstance: ignoring exception ""+ e+ "" on bean instance ""+ bean);"
"<line33>      LOGGER.error(""Exception while processing message in RelayStatisticsCollectingHandler"");"
"<line9>      LOG.info(""Task {} not active, aborting..."", taskKey);<line25>      LOG.error(""While executing task {}"", taskKey, e);"
"<line8>      logger.warn(""Error shutting down user group refresh scheduler due to {}"", e.getMessage(), e);"
"<line9>          LOG.warn(""Invalid stream rule type. Skipping matching for this rule. "" + e.getMessage(), e);"
"<line23>      LOG.warn(""Unexpected key type "" + key + "" ("" + key.getClass() + "") in "" + bo + ""; ignoring value"");"
"<line1>    logger.info(""Moving to done file. ""+ indexRecord.filePath+ "", queueName=""+ FILE_QUEUE_PROVIDER_NAME+ "", consumer=""+ consumerProvider.getName());<line16>      logger.info(""Moving logFile "" + logFile + "" to "" + archiveFile);<line18>        logger.error(""Error moving log file to archive folder. Unable to rename""+ logFile+ "" to archiveFile=""+ archiveFile);<line20>      logger.error(""Error moving log file to archive folder. logFile=""+ logFile+ "", archiveFile=""+ archiveFile,t);<line43>                logger.info(""Deleting archive file "" + archiveFile);<line45>                  logger.error(""Error deleting archive file. archiveFile="" + archiveFile);<line48>                  logger.info(""Deleted "" + filesDeletedCount + "" files"");<line58>      logger.error(""Error deleting older archive file. archiveFile="" + archiveFile, t);"
"<line3>    log.debug("">> IoT Hub responded to message {} with status {}"", msg.getMessageId(), status.name());"
"<line2>    LOG.info(""Enabling availability zone {} in preparation for agent {}"",availabilityZone,agent.getAgentId());"
"<line4>      log.debug(""handle message - for operational environment notification received: {}"", notification);<line41>      log.debug(""handle message for operational environment failed for notification: {} with error :{}"",notification,e.getMessage(),e);"
"<line3>    log.debug(""XML export output at "" + outputFile.getAbsolutePath());"
"<line1>    log.info(""Generating UDT POJOs"");<line5>        log.error(""Error while generating UDT POJO "" + udt, e);"
"<line10>        log.info(""Copying "" + f.getPath().toString());<line14>      log.error(e.getMessage(), e);"
"<line7>      log.trace(""setting sensor {}={} for {}"", new Object[] {path, newValue, entity});"
"<line3>    logger.info(""Starting WatchlistLeftoversCleaner on wiki [{}]."", context.getWikiId());<line15>    logger.info(""End of WatchlistLeftoversCleaner on wiki [{}]."", context.getWikiId());"
"<line14>      LOG.debug(""Exiting safe mode."");"
"<line3>      log.debug(""Assert container asserting exceptions of type "" + exception);<line9>      log.debug(""Validating caught exception ..."");<line35>        log.debug(""Asserted exception is as expected: "" + e.getClass() + "": "" + e.getLocalizedMessage());<line36>      log.info(""Assert exception validation successful: All values OK"");"
"<line39>      LOG.info(""LongHybridHashTable: Use dense mode!"");"
"<line23>        log.warn(""Abort creating a new thread pool for destination ""+ getName()+ "" and reuse previous one"");"
"<line29>          Log.warn(""An exception occurred while trying to update the identity store configuration for""+ "" connection type '""+ type+ ""'"",e);"
"<line1>    log.debug(""Entered WML folder"");"
"<line4>      log.error(this.getType().typeName() + "": index set has been set to null."");<line6>        log.debug(this.getType().typeName() + "": no fields have been set to index."");<line12>      log.error(this.getType().typeName() + "": reverse index set has been set to null."");<line14>        log.debug(this.getType().typeName() + "": no fields have been set to reverse index."");"
"<line9>        log.error(""Error while reading authorization header from APIM configurations"", e);"
<line21>      log.error(systemException, systemException);
"<line25>      logger.error(""Error while checking page for widget '{}'"", viewerWidgetCode, t);"
"<line3>    LOG.info(""handleNewSession. sessionId: {}."", sessionId);"
"<line16>      logger.error(""Unable to get the event count"", e);"
"<line5>      LOG.warn(""Property '{}' cannot be set in the current SchemaFactory: {}"",XMLConstants.ACCESS_EXTERNAL_DTD,factory.getClass().getName(),e);<line9>      LOG.warn(""Property '{}' cannot be set in the current SchemaFactory: {}"",XMLConstants.ACCESS_EXTERNAL_SCHEMA,factory.getClass().getName(),e);"
<line12>    LOGGER.info(String.format(Messages.Log.ASYNCHRONOUS_PUBLIC_IP_STATE_S,asyncRequestInstanceState.getOrderInstanceId(),AsyncRequestInstanceState.StateType.CREATING_FIREWALL_RULE));
"<line17>        logger.warn(""{}"", ex.getMessage());"
"<line2>    logger.info(String.format(""%s. Returning %s response."", exception, Response.Status.CONFLICT));<line3>      logger.debug(StringUtils.EMPTY, exception);"
"<line2>    LOG.debug(""Completing session: {}"", id);"
<line21>      log.error(systemException, systemException);
"<line7>        LOGGER.error(""Fail to reload notes from repository"", e);"
"<line11>    logger.debug(""Number of cell params: "", params.length);"
"<line3>      logger.info(""Execute leaf-node '{}'"", node);<line4>      logger.info(""Execute leaf-node '{}' for {} samples"", node, includeSamples.size());<line22>    logger.info(""Found {} sample in leaf '{}'"", samples.size(), node);"
"<line11>      log.warn(""Failed to read topic policies data, will apply the namespace backlog quota:""+ "" topicName={}"",topicName,e);"
"<line6>      log.error(""Can't deserialize json object (may-be incompatible ProjectForge versions): ""+ ex.getMessage()+ "" json=""+ json,ex);"
"<line14>      log.debug(""The batch ID '{}' of the received reply from bitarchives does not correspond to any""+ "" pending batch job. Ignoring and deleting RemoteFile '{}'.Only knows batchjob with""+ "" IDs: {}"",bitarchiveBatchID,remoteFile,runningBatchJobs.keySet());"
"<line39>              log.warn(""AccountRolePersistenceImpl.fetchByRoleId(long, boolean) with parameters (""+ StringUtil.merge(finderArgs)+ "") yields a result set with more than 1 result. This violates the logical""+ "" unique restriction. There is no order guarantee on which result is""+ "" returned by this finder."");"
"<line1>    log.info(""Splitting the data"");<line7>    log.info(""Growing a forest with m={}"", m);<line12>    log.info(""Growing a forest with m=1"");"
<line6>    log.info(xml);
"<line15>        LOG.debug(""Fetching 'robots.txt' from redirect URL: {}"", redirURL);<line20>        LOG.debug(""Fetched and parsed robots.txt: {}"", robotsURL);<line21>        LOG.info(""No robots.txt found for {}. ({} - {})"",robotsURL,response.getStatusCode(),response.getReasonPhrase());<line24>      LOG.warn(""Not able to obtain robots.txt at: {}"", robotsURL, e);"
"<line2>    log.info(""In region ["" + event.getRegion().getName() + ""] destroyed key ["" + event.getKey() + ""] "");"
"<line17>      log.error(""Could not find relation in [{}] with id [{}]"", document, recommendationVid);<line17>      aTarget.getPage().error(""Could not find relation"");"
"<line5>    LOGGER.info(""cron job of noteId {} executed with result {}"", note.getId(), result);<line12>      LOGGER.warn(""No Timer.Sample for NoteId {} found"", note.getId());"
"<line1>    log.debug(""STARTING TEST FRAMEWORK"");<line6>    log.debug(""STARTED TEST FRAMEWORK"");"
"<line4>        LOGGER.info(""Spawning member bouncing thread"");"
"<line15>        LOGGER.debug(""Exception in parsing date  "" + receiptEndDate + "" - "" + e.getMessage());"
<line16>      log.error(exception, exception);
"<line10>          LOG.fatal(""yarn container launch event handler is interrupted. "" + e);<line26>          LOG.info(""Setting ContainerLauncher pool size to ""+ newPoolSize+ "" as number-of-nodes to talk to is ""+ numNodes);"
"<line29>    ResourcesResource.LOG.debug(""try to apply configuration for resource with uuid '{}'"", uuid);<line29>    ResourcesResource.LOG.debug(""try to recieve resource with uuid '{}' for csv json configuration preview"", uuid);<line34>    ResourcesResource.LOG.debug(""found resource with uuid '{}' for csv json configuration preview "", uuid);<line35>      ResourcesResource.LOG.trace(""= '{}'"", ToStringBuilder.reflectionToString(resource));<line36>    ResourcesResource.LOG.debug(""try to apply configuration to resource with uuid '{}'"", uuid);<line38>      ResourcesResource.LOG.error(""couldn't apply configuration to resource with uuid '{}'"", uuid);<line41>    ResourcesResource.LOG.debug(""applied configuration to resource with uuid '{}'"", uuid);"
<line19>        logger.error(e.getMessage());
"<line2>      LOGGER.debug(""Unbekanntes Element."");"
"<line5>    LOGGER.info(""amsPacket:\n{} has \n{}bytes\nHexDump:\n{}"",amsTCPPacket,bytes.length,amsTCPPacket.dump());"
"<line1>    log.info(""publish uri: {}"", registerDTOList);"
"<line2>    log.debug(""MappingAndDelegatingCommandConsumer receiver link [tenant-id: {}] closed locally"",tenantId);"
"<line12>                        log.trace(""Starting with backup attributes for context: ""+ session.getServletContext().getContextPath());<line20>                          log.trace(""Finished backup of attribute: "" + attrName);"
"<line8>        LOG.debug(""Getting free connection, hostConfig="" + hostConfiguration);<line11>      LOG.debug(""There were no free connections to get, hostConfig="" + hostConfiguration);"
"<line6>    log.info(""findUserNameLike {}"", userName);<line6>    log.info(""case insensitive?:{}"", caseInsensitive);<line29>      log.error(""query exception for query"", e);"
<line13>        log.debug(searchException, searchException);
"<line8>    log.info(""Job {} reached terminal state {}."",archivedExecutionGraph.getJobID(),archivedExecutionGraph.getState());"
"<line5>      log.debug(""cannot set pageType for {}"", file, ex);"
"<line1>    logger.error(""Insane record: "" + Arrays.toString(flatRow), ex);"
"<line55>    log.debug(""Type ""+ tupleExpr.getClass().getSimpleName()+ "" not supported for computing free vars. If you run into this, please report a bug."");"
"<line20>      logger.debug(""test case duration = "" + tc.getDuration());"
"<line25>          logger.trace(""Failed to connect to "" + possibleLive.getA());"
"<line2>      logger.warn(""resolving lock"");"
"<line26>      log.error(""Unsupported calibrator  type "" + calibrator.getClass());"
<line15>      logger.warn(msg, e);
"<line12>        logger.debug(""problem processing policy"", pdme);"
"<line11>      LOG.error(""Can't find ExternalizableBusinessObject implementation class for ""+ inquiryBusinessObjectClass.getName());"
"<line8>      log.error(""Task could not be initialized hence not be executed."", e);"
"<line15>      log.error(""IO exception, Caused by {}."", e);<line17>      log.error(""Class not found exception, Caused by {}."", e);"
"<line5>      logger.error(""error in getWidgetTypeApiMappings"", t);"
"<line12>      LOG.warn(""Convert to CGMES a change on IIDM {}.{}"",change.getIdentifiable().getClass().getSimpleName(),change.getAttribute());"
"<line20>      LOGGER.error(""back up failed"", e.getMessage());<line24>    LOGGER.debug(""backup complete for {}"", resourceId);"
"<line20>      LOG.debug(""Refine query on documents is {}"", refine_query_on_doc ? ""enabled."" : ""disabled."");<line39>          LOG.debug(e);"
"<line19>                log.error(""Error occurred"", e);"
"<line3>        Log.debug(""AuthorizationManager: Trying "" + am.name() + "".map("" + principal + "")"");"
<line23>        log.error(e);
"<line18>    LOGGER.info(""Job Return Status {} "", status);"
"<line38>      LOGGER.error(""Error in fetchVPCtoNatIPInfo"", e);"
"<line20>        LOG.info(""End of Log stream for running pod: {}"", podName);<line21>        LOG.info(""End of Log stream for terminated pod: {}"", podName);"
"<line4>    LOGGER.info(instanceName + "" becomes MASTER from SLAVE for "" + partitionName);"
"<line1>    log.debug(""find() - username: {}"", username);"
"<line2>    LOG.trace(""Creating connection"");<line3>    LOG.trace(""Creating channel"");<line9>    LOG.debug(""Using temporary queue name: {}"", result.getQueue());<line15>                LOG.debug(""Temporary queue name {} was changed to {}. Updating replyTo."",oldName,newName);<line16>                LOG.debug(""Trying to rebind the new temporary queue to update routingKey"");<line20>                  LOG.warn(""Failed to bind or unbind a queue. This exception is ignored."", e);"
<line5>        log.debug(_NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey);
"<line7>        LOG.info(""Loaded event filter: {}"", filterClassName);"
"<line4>      LOG.info(""Stack {} is already registered"", stack.getName());<line8>      LOG.info(""Stack {} is registered with different runner count, first removing the old stack"",stack.getName());<line10>    LOG.info(""Registering new stack {}..."", stack.getName());<line11>    LOG.info(""Starting setup stack thread of {}..."", stack.getName());"
"<line3>      log.warn(""No events found at {}"", eventFolder);<line22>        log.warn(""Caught UncheckedIOException. Could not read file {}"", p);"
"<line21>      LOGGER.error(""Failed to parse "" + item.getPath(), e.toString());"
"<line6>        LOG.debug(""Not trying to connect to ""+ remoteId.getAddress()+ "" this server is in the failed servers list"");<line12>        LOG.debug(""Connecting to "" + remoteId.getAddress());"
"<line16>      LOGGER.error(""Error getting ancestors when creating AIP"", e);"
"<line10>            LOGGER.trace(""Handler auth method: {} - desired auth method {}"", handlerAuthMethod, authMethod);<line19>    LOGGER.trace(""Returning {} handlers that support desired auth methods for path {}"",handlers.size(),path);"
"<line18>      logger.error(""ThingHandler not found for {}"", thingTypeUID);"
"<line7>    log.debug(""Cleared shard records : shardRecovery - {}, shard id = {}"",shardRecoveryDeleted,shard.getShardId());"
"<line10>        log.error(""Unable to register synchronization: no TransactionManager"");<line13>      log.error(""Unable to register synchronization"", e);"
"<line2>    logger.debug(""test-auth: moduleTokens: trying to decode '{}'"", modPermJson);<line22>    logger.debug(""test-auth: module tokens for {}: {}"", modPermJson, alltokens);"
<line9>      log.error(exception, exception);
"<line3>      logger.trace(""getXAResources("" + Arrays.toString(specs) + "")"");"
"<line2>    logger.info(""Warming up bill cache."");<line7>          logger.info(""Caching Bill instances for current session year: {}"", sessionYear);<line9>          logger.info(""Caching Bill Info instances for session year: {}"", sessionYear);<line14>    logger.info(""Done warming up bill cache."");"
"<line2>      LOG.debug(""selectLink() called ..."");"
"<line4>      log.info(LOG_PREFIX + ""Create file: creating a file with 1024 bytes ..."");<line7>      log.info(LOG_PREFIX + ""Invalid FileID Test: passed! Caught expected exception = "" + e);"
"<line9>      logger.info(""The index "" + indexName + "" already exists"");"
"<line35>        LOG.warn(""Error releasing exchange due to "" + e.getMessage() + "". This exception is ignored."",e);"
"<line25>        LOG.debug(""Exception: "" + e, e);"
"<line2>      logger.debug(""Unrecoverable failure, forcing disconnect"", ex);<line5>        logger.warn(""Unable to Disconnect..."");<line7>      logger.debug(""Ignoring failure from old connection"", ex);"
<line3>      LOGGER.info(task);
<line18>      log.error(message, ex);
"<line5>    LOG.trace(""Adding composite phenomenons {} to procedure {}"", compositePhenomenon, procedure);"
"<line1>    logger.error(""Event Bus Exception thrown during event handling within "" + context.getSubscriberMethod(),exception);"
"<line6>      LOG.info(""starting final check"");"
"<line1>    log.debug(""attaching clean BrAttribut instance"");<line3>      log.debug(""attach successful"");<line4>      log.error(""attach failed"", re);"
"<line2>    logger.info(""Document servlet started"");"
"<line4>        log.debug(""Scenario {} doTransaction called"", scenario);<line7>          log.debug(""empty transaction occurred"");<line8>        log.debug(""Sleep between invocations for {}"", delay);<line12>      log.error(""Error during the invocation"", error);"
"<line4>    logger.debug(marker, requestId + "" "" + format, arg);"
<line9>      log.error(exception, exception);
"<line1>    LOG.debug(""Received: {}"", record.value());"
"<line4>      log.info(""Service group name "" + serviceGroup[0]);<line6>      log.error(""Service group name cannot be null"");"
"<line3>      log.warn(""Oups, task shouldn't be null."");"
"<line4>      log.error(""Unable to get descriptive name for group "" + group.getGroupId(), portalException);"
"<line2>      log.debug(""Clear local statistics [key="" + key + "", columns="" + colNames + ']');"
<line29>    logger.info(sql);
"<line61>            log.error(""Cannot write stressor last operation"", e);<line72>        log.error(""Request failed due to SuspectException: "" + e.getMessage());<line73>        log.error(""Cache operation error"", e);"
<line6>        LOG.debug(e.getMessage(), e);<line8>    IndicatingAjaxButtonPage.this.info(this);
"<line3>      log.warn(""Empty list of documents received."");<line17>      log.warn(""Exception in Solr onDocuments."", e);"
"<line2>    log.info(""test method: testQueryEmptyResults"");"
"<line1>    log.debug(""<pipeline name = '{}' layer = '{}'>"", pipeline.getPipelineName(), pipeline.getLayerId());<line4>    log.debug(""</pipeline>"");"
"<line2>    Log.debug(""Test"");<line32>          Log.info(""\t""+ veff+ ""\n\t\tEFF    : ""+ veff.getEffectsStr()+ ""\n\t\tHGVS_C : ""+ hgvsCactual+ ""\n\t\tHGVS_P : ""+ hgvsPactual+ ""\n"");"
"<line26>    LOG.trace(""Deleting temporary file: {}"", tmpTar);"
"<line5>      logger.error(""Could not close the metrics service, some metrics may have not been written"", e);"
"<line9>      logger.trace(""scheduling appendUpdateRecordTransactional::txID=""+ txID+ "",id=""+ id+ "", userRecordType=""+ recordType+ "", record = ""+ record);<line23>                logger.trace(""appendUpdateRecordTransactional::txID=""+ txID+ "",id=""+ id+ "", userRecordType=""+ recordType+ "", record = ""+ record+ "", usedFile = ""+ usedFile);<line26>              logger.error(""appendUpdateRecordTransactional:"" + e.getMessage(), e);"
"<line20>        LOGGER.debug(""Error parsing Integer"");"
"<line1>    LOG.info(""Warming up data server..."");<line13>        LOG.error(""Failed to warm up data server"", e);<line17>    LOG.info(""Warming up data server took "" + warmupDurationMs + "" ms"");"
"<line3>    AbstractTableConfigHelperTest.logger.info(""AbstractTableConfigHelperTest.testSetCombinerConfigurationIfNecessary() called."");<line10>      AbstractTableConfigHelperTest.logger.info(""AbstractTableConfigHelperTest.testSetCombinerConfigurationIfNecessary() completed."");"
"<line3>        logger.warn(""no filters"");"
<line12>      log.error(e);
"<line9>    LOGGER.debug(""Connected devices: "".concat(connectedDevices.toString()));"
<line31>        logger.error(ex);<line40>      logger.info(msg);
"<line3>      logger.info(""ASM BCI engine"");<line5>    logger.info(""Unknown BCI engine"");"
"<line9>    LOG.warn(""Searching for a trigger named \""{}\"" but it doesn't exist"", name);"
"<line1>    LOGGER.info(""Loading metadata definitions"");<line11>          LOGGER.debug(""reading metadata definition "" + inputDef.getName());"
"<line2>      LOGGER.debug(""Reading taskana configuration from {} with separator {}"", propertiesFile, separator);"
"<line17>        LOG.debug(""Unresolved reference"", ignore);"
"<line3>      LOG.debug(""snapshotState() called on closed source; returning null."");<line5>        LOG.debug(""Snapshotting state ..."");<line28>          LOG.debug(""Snapshotted state, last processed sequence numbers: {}, checkpoint id: {},""+ "" timestamp: {}"",lastStateSnapshot,context.getCheckpointId(),context.getCheckpointTimestamp());"
"<line2>    logger.info(""{} destroying started."", ClassUtils.simpleClassName(this));<line5>    logger.info(""{} destroying completed."", ClassUtils.simpleClassName(this));"
"<line5>    LOG.warn(""@@@@@@@@@@@@@@@@@@ Benchmark for student section view: {}"",(System.nanoTime() - startTime) * 1.0e-9);"
"<line4>      log.debug(""  query[""+ queryStr+ ""]  start(""+ YMD_DateFormat.format(startEndDate[0])+ "")  end(""+ YMD_DateFormat.format(startEndDate[1])+ "")"");"
<line32>      log.error(bundleException, bundleException);
"<line5>        LOGGER.debug(""apparentlyTo set to: {}"", (Object) internetAddresses);"
"<line15>          log.warn(""Trying to create cache location using property {} which is undefined, using {}""+ "" instead."",property,value);<line18>        log.warn(""Cache location {} looks like a property reference but closing } is missing."",location);"
"<line6>        logger.debug(""{}: Received remove interest message of length ({} bytes)"",this,clientMessage.getPayloadLength());<line21>          logger.debug(""{}: Region named {} does not exist"", this, regionName);"
"<line9>      log.info(""Initiating recovery 'forget' processing for Xid:\n"" + xidString);<line13>      log.info(""Finished recovery 'forget' processing for Xid:\n"" + xidString);"
<line6>      log.error(e.toString());
"<line2>      LOG.debug(""#The candidate row keys for "" + partitionPathFilePair + "" => "" + candidateRecordKeys);<line7>    LOG.info(String.format(""Total records (%d), bloom filter candidates (%d)/fp(%d), actual matches (%d)"",totalKeysChecked,candidateRecordKeys.size(),candidateRecordKeys.size() - matchingKeys.size(),matchingKeys.size()));"
"<line7>      log.warn(""documentAttribute for hitTerm:"" + hitTerm + "" is null in document:"" + document);"
"<line7>      logger.debug(""recording bulkOp event {} {} {} op={}"",threadID.expensiveToString(),eventID,tag,event.getOperation());"
"<line9>          logger.debug(""Sending my leave request to {}"", coords);"
"<line1>    logger.debug(""Updating router network ref"");<line25>        logger.debug(""Added reference for router id="" + routerId + "" and network id="" + networkId);<line32>    logger.debug(""Done updating router/network references"");"
"<line8>      log.debug(""Opening store directory [{}]"", path);"
"<line3>      LOG.info(""partition path is null for "" + partitionToWriteStats.get(null));<line6>      LOG.info(""partition path is null for "" + partitionToReplaceFileIds.get(null));"
"<line3>      Log.debug(""requiresNewLauncher - byClass "" + workflowClass);<line8>      Log.debug(""requiresNewLauncher - byEngine or Type "" + workflowEngine + "" "" + workflowType);<line10>    Log.debug(""requiresNewLauncher - fall-through"");"
"<line3>      log.error(""line "" + line.getObjectId() + "" : missing network"");<line6>      log.error(""line "" + line.getObjectId() + "" : missing company"");"
"<line4>    logger.debug(""appContext -> findModules."");<line7>    logger.debug(""appContext -> doInitialize."");<line15>    logger.debug(""appContext -> doBind."");<line18>    logger.debug(""appContext -> doInitializeCompleted"");<line19>    logger.debug(""appContext -> doStart"");<line20>    logger.debug(""appContext -> fireSyncEvent ,eventType = {}"", ContextEvent_Started);<line22>    logger.info(""Hasor StartCompleted!"");"
<line11>      log.error(e);
<line7>    logger.info(events);
"<line14>      log.error(""Unable to create Output Event Adapter : "" + DeviceTypeConstants.MQTT_ADAPTER_NAME, e);"
"<line6>        LOG.warn(""Volume : {} already exists in VolumeMap"", hddsRoot);<line14>        LOG.info(""Added Volume : {} to VolumeSet"", hddsVolume.getHddsRootDir().getPath());<line17>      LOG.error(""Failed to add volume "" + volumeRoot + "" to VolumeSet"", ex);"
"<line2>      LOG.trace(""[{}] Cancelling timeout"", logPrefix);"
"<line7>      logger.debug(""Registering reflective access to ""+ typename+ "": ""+ (flags == null ? """" : Arrays.asList(flags)));<line12>        logger.debug(typename + "" discarded due to access check by "" + accessChecker.getClass().getName());"
"<line12>                    logger.warn(""Exception at getRwtweetsOfMe"", e);"
<line7>        log.debug(_NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey);
"<line5>      LOGGER.info(""success delete "" + envName + ""-snapshot"");<line6>      LOGGER.info(""fail delete "" + envName + ""-snapshot, "" + e.toString());"
"<line5>      log.trace(""Document {} is not a user workspace, it cannot be adapted as a FileSystemItem."",doc::getId);"
"<line31>        logger.debug(""Unknown event type: {} in state {}"", event.encodeType(EventTypes.class), state);"
"<line4>      LOG.error(""Cannot create folder: "" + pathToDir.toAbsolutePath(), e);"
"<line5>    LOG.debug(""opps = {}."", opps);"
"<line11>      logger.debug(""Created ServerTypeDesc for type ["" + typeName + ""]."" + getClientAddressAddition());"
"<line4>      LOG.trace(""not found: {}"", path, e);"
"<line5>    logger.info(""will submit "" + warmUpIterations + "" jobs"");<line12>    logger.info(""warmup jobs done, starting benchmark"");"
"<line5>    logger.info(""Note name changed: {} -> {}"", oldName, note.getName());"
"<line43>    LOGGER.debug(""Updated json serialiser to use: {}, and modules: {}"",jsonSerialiserClass,moduleFactories);"
"<line14>      LOGGER.trace("""", x);<line43>        LOGGER.error(L.m(""Fehlerhaftes WM()-Bookmark: \""%1\"""", name), x);"
"<line3>      logger.debug(""Step: ""+ info+ "" memory: free / total / max MB ""+ runtime.freeMemory() / (1000 * 1000)+ "" / ""+ runtime.totalMemory() / (1000 * 1000)+ "" / ""+ runtime.maxMemory() / (1000 * 1000));"
"<line12>            log.warn(""Cancel processing scheduled task. %s"", e);<line14>            log.warn(String.format(""Failure processing scheduled task. %s"", e.getMessage()), e);<line15>            log.error(String.format(""Failure processing scheduled task. %s"", e.getMessage()), e);"
"<line21>        LOG.error(""Error JSON-ifying environment {}: {}"", environment.getId(), e.getMessage());"
"<line2>    LOG.trace(""Consumer {} drain request timed out"", this);"
"<line2>    LOGGER.info(""{} hits {}."", mir.getClass().getSimpleName(), this.getClass().getSimpleName());"
"<line14>        log.debug(""An exception occured when checking if the publish "" + ""action should be displayed"",portalException);"
"<line10>          LOGGER.error(L.m(""Fehler in Dokumentkommando '%1': Die Farbe HIGHLIGHT_COLOR mit dem Wert '%2' ist""+ "" ungÃ¼ltig."","""" + this, highlightColor));<line15>          LOGGER.error(""Couldn't set background color."", e);"
"<line7>        LOG.trace(""[{}] Got result but the request has been cancelled, ignoring"", logPrefix);<line12>          LOG.trace(""[{}] Got result"", logPrefix);<line14>          LOG.trace(""[{}] Got error response"", logPrefix);"
"<line2>    logger.info(""[OneHourPeriodTask] do alert"");<line4>        logger.info(""[OneHourPeriodTask] alert system is on, stop task"");<line9>      logger.error(""[run] {}"", e);"
"<line36>      log.info(""result: numLinesBefore=""+ numLinesOrig+ ""; numLinesAfter=""+ numLinesAfter+ ""; first=""+ grepFirst+ ""; last=""+ grepLast);"
"<line1>    log.debug("""");"
"<line8>      log.debug(""addForward"");"
"<line5>      logger.debug(""Writing last partial histo log:"" + this);<line7>      logger.debug(""Not writing last partial histo log <1s:"" + this);"
"<line2>    LOGGER.trace(""ENTER :: validatePluginOptions()"");<line7>    LOGGER.debug(""Default console echo is {}, Password console echo is {}"",new Object[] {defaultEchoEnabled ? ""enabled"" : ""disabled"", passwordEchoEnabled ? ""enabled"" : ""disabled""});<line46>              LOGGER.error(""An error occurred validating plugin options for [""+ this.getClass().getName()+ ""]: ""+ ex.getLocalizedMessage(),ex);"
"<line1>    log.info(""B headers "" + exchange.getIn().getHeaders());"
"<line3>      LOG.debug(""Getting all enterprise events occurring between {} and {} {}"",after == null ? ""unspecified date"" : DateFormat.getDateTimeInstance().format(after),before == null ? ""unspecified date"" : DateFormat.getDateTimeInstance().format(before),position == null ? """" : ("" starting at "" + position));"
"<line20>      logger.error(""failed to create class "" + getController().getEntityClass() + "": "" + e);"
"<line4>        logger.info(""Creating saved clients list in "" + file);<line10>      logger.error(""Could not write to output file"", e);"
"<line3>        log.error(""device is null"");"
"<line5>    LOG.debug(""starting operator annotation processor: {}"", this);<line18>      LOG.error(Messages.getString(""AbstractOperatorAnnotationProcessor.logFailCompile""), e);"
"<line8>        logger.info(""FileSystem's output stream doesn't support""+ "" getNumCurrentReplicas; --HDFS-826 not available; fsOut=""+ wrappedStreamClass.getName()+ ""; err=""+ e);<line9>        logger.info(""Doesn't have access to getNumCurrentReplicas on ""+ ""FileSystems's output stream --HDFS-826 not available; fsOut=""+ wrappedStreamClass.getName(),e);<line13>      logger.debug(""Using getNumCurrentReplicas--HDFS-826"");"
"<line41>          logger.debug(""step is FORCE_INTERFACE"");<line50>              logger.debug(""step is APPLICATION_FIRST shouldMigrate true get serviceInvokers"");<line55>          logger.debug(""step is APPLICATION_FIRST ""+ (serviceInvokers.isEmpty() ? ""serviceInvokers is empty"" : ""shouldMigrate false"")+ "" get interfaceInvokers"");<line61>          logger.debug(""step is FORCE_APPLICATION"");"
"<line2>    log.error(""error - {} {}"", tokenToString(token), throwable);"
"<line14>      log.error(""Could not find type of narrow expression"");"
"<line16>    log.info(""[{}] Successfully unsubscribed {} on namespace bundle {}/{}"",clientAppId(),subscription,namespaceName,bundleRange);"
"<line9>    log.warn(String.format(""Lookup failed for %s in cache"", reference));"
<line1>    LOG.warn(DEFAULT_EXCEPTION_RESPONSE, e);
"<line8>    logger.debug(""Strategies : {}"", bufferSizes);"
"<line9>        logger.debug(type + "" is a Tapestry resources or service"");<line12>      logger.debug(type + "" is not a known service of the tapestry registry"");"
"<line11>      logger.warn(""create dump directory:{} failed."", dumpDir.getAbsolutePath());<line30>      logger.warn(""dump class:{} to file {} failed."", className, dumpClassFile, e);"
"<line11>    LOG.debug(""adding command [name: {}, request-id: {}] to response for device [tenant-id: {},""+ "" device-id: {}]"",command.getName(),command.getRequestId(),command.getTenant(),command.getGatewayOrDeviceId());"
"<line1>    LOGGER.info(""Creating resources before the test class"");"
"<line34>        log.debug(""getMostRecentByJobId for {}:{}"", jobId, sji);<line39>      log.warn(message, e);"
<line8>    LOGGER.error(message);
"<line10>    logger.info(""Locator restart: initializing TcpServer"");<line17>      logger.info(""Locator restart: attempt to restart location services failed"", e);<line26>      logger.info(""Locator restart: starting TcpServer"");<line28>    logger.info(""Locator restart: initializing JMX manager"");<line30>    logger.info(""Locator restart completed"");"
"<line6>          log.info(""Start plugin '{}'"", getPluginLabel(pluginWrapper.getDescriptor()));<line17>          log.error(""Error while starting plugins "" + e.getMessage(), e);"
"<line6>      logger.warn(""Channel {} not found."", name);"
"<line2>    log.info(""=== TEST for SOLUTION GENERATION of HILLCLIMB optimizer SINGLE ADP - STARTED ==="");<line4>    log.info(""=== TEST for SOLUTION GENERATION with POLICIES of HILLCLIMB optimizer SINGLE ADP FINISEHD""+ "" ==="");"
"<line1>    log.debug(""Altering '"", object, ""'"");"
"<line6>      LOGGER.warn(""Could not find action ''{}'' in collection: {}"", action, collectionName);"
"<line2>    logger.info(""[Update Cluster]{},{}"", clusterName, cluster);"
"<line3>      log.info(""Refreshing Planet subscriptions"");<line6>      log.error(""ERROR refreshing planet"", e);"
"<line8>      logger.error(""Error extracting dataObjects id"", t);"
"<line16>      log.warn(""Oracle leadership watcher has been interrupted unexpectedly"");"
"<line2>    logger.debug(""Activating Cloud Subscriber Wire Component..."");<line6>    logger.debug(""Activating Cloud Subscriber Wire Component... Done"");"
"<line21>      LOG.warn(""Error deploying '""+ url+ ""' to ""+ targetName+ "" on ""+ toString()+ ""; rethrowing..."",e);"
"<line1>    log.debug(""attaching clean MbBaust instance"");<line3>      log.debug(""attach successful"");<line4>      log.error(""attach failed"", re);"
"<line8>      LOG.info(""Async Upload Aborted. Dataidentifer [{}], file [{}] removed from AsyncCache."",identifier,file.getAbsolutePath());<line9>      LOG.warn(""Cannot remove pending file upload. Dataidentifer [ ""+ identifier+ ""], file [""+ file.getAbsolutePath()+ ""]"",ie);"
"<line2>    logger.debug(""WorkManager set."");"
"<line2>    LOGGER.info(""StorageSystems returned to client : "" + storageSystems.toJsonString());"
<line16>      log.error(exception, exception);
"<line14>                log.error(""Failed to replicate entry: {}"", indexed, commitError);"
"<line7>      logger.debug(""Successfully initialized index provider '{0}' in repository '{1}'"",provider.getName(), repository.name());"
"<line2>    logger.debug(""channelUnlinked: {}"", channelUID);<line17>            logger.warn(""Channel param error, reason: {}."", e.getMessage(), e);"
"<line22>      logger.error(""Error on COM_BINLOG_DUMP: file = "" + fileName + "", position = "" + filePosition);"
"<line1>    LOGGER.debug(""create a VCPENetwork: "" + logicalInfrastructure);<line4>    LOGGER.debug(""Polling for building task to finish"");<line8>        LOGGER.warn(""Interrupted while waiting for VCPE build to finish"", e);<line11>    LOGGER.debug(""Retrieving build result"");"
"<line26>            log.debug(InventoryConstants.ACCOUNT+ accountId+ "" Type : EC2 Egress Gateway ""+ region.getName()+ "" >> ""+ egressGatewayList.size());<line32>        log.warn(expPrefix + region.getName() + InventoryConstants.ERROR_CAUSE + e.getMessage() + ""\""}"");"
"<line2>    log.debug(""PARTICIPANT {}: Leaving room {}"", user.getName(), this.name);"
"<line8>        logger.warn(msg, event.getCode());<line9>        logger.debug(msg, event.getCode());<line12>    logger.debug(""Got event: {}"", event);<line29>        logger.debug(""Unexpected event value for channel {}: {}"", channel, eventValue);"
"<line7>            LOG.debug(""Check if IntegrationStatus {} is already in progress for key: {} (keys: {})"",id,scheduledKey,scheduledChecks);<line10>              LOG.debug(""A check for IntegrationDeployment {} is already configured with key {}"",id,scheduledKey);<line12>            LOG.debug(""No IntegrationDeployment with id: {}"", id);"
"<line10>      LOGGER.error(""Predicate path is null.Please check predicates "" + ""config root.path and path."");<line14>      LOGGER.info(""Predicate path: {}"", hdfsPath);<line15>        LOGGER.info(""Predicate path: "" + hdfsPath + "" doesn't exist."");<line17>      LOGGER.info(""Predicate path: "" + hdfsPath + "" exists."");"
"<line1>    log.debug(""Adding network "" + network.getName() + "" to Quantum model."");<line6>    log.debug(""Network "" + network.getName() + "" added to Quantum model."");"
<line9>      log.error(exception, exception);
"<line4>      LOG.error(""Error creating a tar file"");"
"<line1>    LOGGER.debug(""XACML policies will be looked for in the following location(s): {}"",xacmlPolicyDirectories);"
"<line11>      log.error(""No printer found"");<line13>      log.error(""print"", e);"
"<line5>      LOGGER.error(""No HTTP URL has been set for the Gateway: {}"", SmsParameters.GATEWAY);<line5>      LOGGER.error(""Please check Skebby.properties file"");"
"<line7>    logger.info(actorMessage.getRequestContext(), ""Calling method to save inside Es=="");<line31>            logger.info(actorMessage.getRequestContext(),""Exception occurred while converting orgLocation to List<Map<String,String>>."");"
"<line2>    LOG.info(""Running testRunAllParagraph_FirstFailed"");"
"<line3>    LOG.trace(""LocalFileSystemStore.getResourceContent("" + uri + "")"");<line8>      LOG.error(new TextI18n(""LocalFileSystemStore.getResourceContent("" + uri + "") failed""));"
<line4>      log.debug(s);
"<line10>              LOG.trace(""Configuring client-auth on SSLEngine [{}] to [{}]."", engine, clientAuthValue);"
<line16>        log.warn(portalException, portalException);
"<line3>    log.trace(""Executing findByTenantId [{}]"", tenantId);"
"<line16>      log.trace(""processing directory : "" + entry.getName());"
"<line3>      log.info(""...Setting executeStatusLogLevelInfo: "" + executeStatusLogLevelInfo);"
<line9>      logger.error(NO_REQUEST_FIND_FOR_SEQUENCE_NUMBER + pduHeader.getSequenceNumber());
"<line7>      LOGGER.error("""", exc);"
"<line17>    log.info(""Aggregator started."");<line28>          log.debug(""skipping: "" + query[i]);<line39>            log.error(""Invalid query:"" + query[i]);<line51>    log.info(""Longest running query: "" + longQuery + "" ("" + (double) longest / 1000 + "" seconds)"");<line51>    log.info(""Total running time: ("" + (double) (aggregatorEnd - aggregatorStart) / 1000 + "" seconds)"");<line51>    log.info(""Aggregator finished."");"
"<line4>      LOG.trace(""ReplyTo is disabled on endpoint: {}"", endpoint);<line16>        LOG.debug(""Disabling JMSReplyTo: {} for destination: {}. Use preserveMessageQos=true to force""+ "" Camel to keep the JMSReplyTo on endpoint: {}"",new Object[] {jmsReplyTo, to, endpoint});<line31>      LOG.debug(""Using JMSReplyTo destination: {}"", replyTo);<line33>      LOG.trace(""Not using JMSReplyTo"");<line35>    LOG.trace(""Created javax.jms.Message: {}"", answer);"
"<line14>      log.debug(""getReferralsByFacility: clientId="" + clientId + "",# of results="" + results.size());"
<line23>      log.error(systemException, systemException);
"<line5>      logger.error(""Exception occured while counting series."", e);"
"<line23>          log.error(""Problem reading config file for Godiva3 config.  Defaults will be used"", e);<line25>        log.info(""No godiva3.properties file found.  Defaults will be used."");<line54>          log.error(""Problem writing default godiva3.config"");"
"<line19>    LOGGER.info(""did remote the repository source mirror [{}]"", request.code);"
"<line14>        log.info(""Put key: "" + i);"
"<line1>    log.debug(""Loading Person object from database"");<line2>    log.debug(""Loaded Person - user name = "" + user.getUsername());<line2>    log.debug(""Setting the authority to ROLE_READER"");<line3>    log.debug(""Updating Person in database"");"
"<line8>      logger.error(""Error deleting item"", t);"
"<line14>    logger.debug(""document:{}"", document);<line15>    logger.debug(""val:{}"", stringStringValue);"
"<line10>      LOG.info(""Sending message failed on first try due to a connection failure with the leased""+ "" connection. Will retry with a new connection."",e);<line14>        LOG.error(""Sending failed with the second try"", e1);"
"<line4>    LOG.trace(""Adding parentOfferings {} to offering {}"", parentOfferings, offering);"
<line3>    logger.fatal(alertMessage);
<line13>      LOG.error(e.getMessage(), e);
"<line2>    logger.debug(""Caching audio file {}"", cacheFile.getName());<line6>    logger.debug(""Caching text file {}"", textFileName);"
<line15>      LOGGER.error(e);
<line8>        LOG.warn(e.getMessage());
"<line8>      log.info(""Ranger Plugin : Unable to find Ranger Hive policy ""+ rangerHivePolicyName+ "" ... Ignoring"");<line15>          log.error(""Unable to delete policy"", e);"
"<line15>      LOG.debug(""[RequestBody]  "" + httpreq.getRequestURL());<line15>      LOG.debug("""" + builder.toString());"
"<line2>      LOG.info(""{} Minor Compaction back to normal since bookie has enough space now."",Thread.currentThread().getName());"
"<line2>    log.debug(""Emulating close file channel"");"
"<line48>      logger.error(""StudyController - reOrderComprehensionTestQuestion - ERROR"", e);"
"<line7>    log.info(""********** duration :{} second"", (System.nanoTime() - startTime) / 1.0E9);<line7>    log.info(""result: {}"", result);"
"<line3>    logger.info(""{} resource {} in cluster {}."", enabled ? ""Enable"" : ""Disable"", resourceName, clusterName);"
"<line9>    logger.warn(String.format(""management node[uuid:%s] becomes unavailable, reply %s to message[%s]. Message""+ "" metadata dump: %s"",mgmtNodeId, err, rmeta.messageName, JSONObjectUtil.toJsonString(rmeta)));"
<line5>        log.debug(_NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey);
"<line7>        LOG.debug(""Adding {} {} to the set of runner-executed transforms"",PTransformNode.class.getSimpleName(),consumer.getId());"
"<line4>    LOG.info(""testNoteCreate \n"" + postResponse);<line8>    LOG.info(""newNoteId:="" + newNoteId);<line11>    LOG.info(""new note name is: "" + newNoteName);"
"<line8>        LOGGER.warn(""Unable to load plugin: {} due to: {}"", plugin.getKey(), ex.getMessage());"
"<line31>      log.error(""Error in fetching status of system"", e);"
"<line12>            logger.debug(""Make {} immutable because it lastUpdate[{}] exceed wait duration."",segment.getSegmentName(),segment.getLastUpdateTime());<line24>          logger.info(""found cube {} segments:{} are immutable, retention policy is: {}"",cubeName,segments,retentionPolicyInfo.getName());<line29>        logger.error(""error when handle cube:"" + cubeName, e);"
"<line4>    log.trace(""storeSecretStorage 1, secureStoragePathCopy = '{}'"", secureStoragePathCopy);<line7>      LOG.error(e.getMessage(), e);"
"<line2>    LOG.info(""Starting GbpIseAdapterProvider .."");<line20>    LOG.info(""Started"");"
"<line5>    Log.debug(""Could not retrieve basic test db password, using default from unit tests"");"
"<line7>      logger.error(""Error while last modified date of "" + this, fex);"
"<line14>      LOG.debug(""Loading Alert Scheme : {}"", alertSchemeType);"
<line9>    logger.debug(result);
"<line3>      logger.warn(""è¦å é¤çåå¤ä¿¡æ¯å·²ä¸å­å¨ï¼id={}"", replyId);"
"<line1>    log.debug(""attaching dirty StgSysExport instance"");<line3>      log.debug(""attach successful"");<line4>      log.error(""attach failed"", re);"
"<line2>      log.info(""Entering:*********waitForElementPresent()******"");<line4>      log.info(""Waiting:*************until the element is visible  ***********"");<line5>      log.info(""-----waitForElementPresent timeup------"");"
"<line5>    logger.debug(""-----getEditors--- "");"
"<line14>      logger.debug(""added update string {}"", update);<line18>          logger.warn(""ambiguous dataset spec for SPARQL endpoint: default graphs and default remove graphs""+ "" both defined but not equal"");<line22>              logger.warn(""ambiguous dataset spec for SPARQL endpoint: default insert graph ({}) and""+ "" default remove graph ({}) both defined but not equal. "",dataset.getDefaultInsertGraph(),graphURI);<line32>            logger.warn(""ambiguous dataset spec for SPARQL endpoint: default insert graph ({}) and default""+ "" graphs both defined but not equal. "",dataset.getDefaultInsertGraph());"
"<line32>      this.logger.warn(""Failed to compute access level for [{}] on [{}]: {}"",userOrGroup,entity.getId(),ex.getMessage());"
"<line11>    Logger.trace(""< "" + sb.toString());"
"<line46>              else log.error(""QueryMetricsBean JNDI lookup returned null"");<line47>              log.error(""Unable to record metrics for ""+ queryCall.methodType+ "" method: ""+ e.getLocalizedMessage(),e);<line50>          log.error(""RunningQuery instance not in the cache!, queryId: "" + queryCall.queryID);<line52>        log.error(""Query cache not injected! No metrics will be recorded for serialization times."");"
"<line4>      LOG.error(""Error deploying '"" + url + ""' on "" + toString() + ""; rethrowing..."", e);"
"<line1>    log.debug(""deleting StgMsUnjTxt instance"");<line3>      log.debug(""delete successful"");<line4>      log.error(""delete failed"", re);"
"<line4>      logger.error(""Assume concurrent modification happened, perform remapping: {}"", mapExactMatches);"
"<line14>          log.info(i + "": Got the same results for two Stream tasks runs"");<line15>          log.error(i + "": Did not get the same results for two Stream tasks runs"");<line23>    log.info(""Executed stream stage on worker"");"
"<line1>    log.debug(""Accepting"");<line2>    log.debug(""Accepted: {}"", socket);"
"<line37>      LOGGER.error(""Updating ObservedProperty {} caused {} rows to change!"", opId, count);<line43>    LOGGER.debug(""Updated ObservedProperty {}"", opId);"
"<line2>    LOG.info(""Retrieving data for project #{}"", idProject);<line7>        LOG.warn(""Unable to retreive project {}"", idProject);<line10>      LOG.error(""Database error encountered {}"", sqex.getMessage());<line12>      LOG.error(""Unexpected exception retrieving a project record"");<line12>      LOG.error(""Error Message: {}"", ex.getMessage());<line14>    LOG.info(""Cleaning project {} blocks"", record.getId());<line16>      LOG.info(""Hmm, Something broke the project record"");<line18>    LOG.info(""Returning project {}."", prjRecord.getId());"
"<line2>      LOG.error(""minEvictableIdleTimeMillis should be greater than 30000"");"
<line10>      log.error(exception, exception);
"<line33>    log.trace(name + "" has authorizations union "" + list.getAllAuths());"
"<line32>        LOG.error(""Unable to execute query : "" + exception.toString());<line34>      LOG.error(""Unable to execute query : "" + exception.toString());<line35>      LOG.error(ex.toString());"
"<line7>            .forEach(buffer -> logger.trace(""Request: {}"", new String(buffer.array())));<line10>        logger.trace(""Re-Auth needed."");<line15>        logger.trace(""Received response: {}"", response.getContentAsString());"
"<line2>    LOG.info(""Starting Test Cluster."");<line29>    LOG.info(""Test Cluster ready and test table created."");<line29>    LOG.info(""==================================================="");"
"<line25>      LOG.error(""Error while submitting extension job: "", e);"
"<line2>    log.info(""Initialisation complete."");"
"<line9>      LOG.warn(""Wrong type of {} attribute. Expected {}, got {}"",PaxWebConstants.CONTEXT_PARAM_BUNDLE_CONTEXT,BundleContext.class.getName(),attribute.getClass().getName());"
"<line2>      log.error(""No documents in version {}:{}"", version.getProject().getSlug(), version.getSlug());"
"<line8>      LOGGER.debug(""No context with alias "" + alias + "" found, nothing to remove"");"
"<line38>      log.error(""Error while retrieving records for entity {}, row keys {}"", entityClass, rowIds);"
<line13>      log.info(msg);
"<line10>      logger.error(""PropertyNotSetException while deleting nominal label "" + e.getMessage());"
<line7>        log.warn(portalException, portalException);
"<line3>      log.debug(""Registering model listeners for broker "" + _broker);<line6>        log.debug(""Considering virtualhostnode "" + vhostNode);<line16>      log.debug(""Registered model listeners"");"
"<line2>      log.warn(""Kafka TLS sidecar container has been removed and the environment variable {} is not used""+ "" anymore. You can remove it from the Strimzi Cluster Operator deployment."",STRIMZI_DEFAULT_TLS_SIDECAR_KAFKA_IMAGE);"
<line5>    log.info(expectedJavascript);<line5>    log.info(generatedJavascript);
"<line24>      LOG.debug(""Using active port range: {}-{}"", min, max);<line49>      LOG.debug(""Created FTPClient[connectTimeout: {}, soTimeout: {}, dataTimeout: {}, bufferSize: {}""+ "", receiveDataSocketBufferSize: {}, sendDataSocketBufferSize: {}]: {}"",client.getConnectTimeout(),getSoTimeout(),dataTimeout,client.getBufferSize(),client.getReceiveDataSocketBufferSize(),client.getSendDataSocketBufferSize(),client);"
"<line8>      LOG.info(""Subtask {} {} is requesting a file source split"", subtask, hostInfo);<line13>      LOG.info(""Assigned split to subtask {} : {}"", subtask, split);<line15>      LOG.info(""No more splits available for subtask {}"", subtask);"
"<line3>      LOGGER.debug(""Adding key [spatialOp Name: {}]"", spatialOp.getName());<line4>      LOGGER.debug(""spatialOps Map: {}"", spatialOps);"
"<line4>        logger.debug(""Outbound local IP.\""{}\"""", localAddress);"
"<line23>      Log.error(""Input file was not found"", e);<line25>      Log.error(""IOException"", ex);"
"<line6>    logger.debug(""Adding jobID {} ({} batches) to the queue at {} with for organization {}."",firstBatch.getJobID(),jobBatches.size(),firstBatch.getSubmitTime().orElseThrow(() ->new JobQueueFailure(firstBatch.getJobID(),firstBatch.getBatchID(),""The batches have not been prepared for submission"")).format(DateTimeFormatter.ISO_OFFSET_DATE_TIME),firstBatch.getOrgID());<line14>        logger.error(""Cannot add job batches to database"", e);"
"<line10>          LOG.trace(""Waiting for producer to finish starting: {}"", producer);<line17>                LOG.trace(""Waiting {} ms for producer to finish starting: {} state: {}"",watch.taken(),producer,ss.getStatus());<line21>            LOG.debug(""Waited {} ms for producer to finish starting: {} state: {}"",watch.taken(),producer,ss.getStatus());"
"<line1>    logger.trace(""resetAuthentication() called."");"
"<line7>        logger.debug(""Failure trying to obtain (Terminal-Information) Software-Version AVP value"", ex);"
"<line9>          log.error(""Error closing SQL statement"", e);<line16>          log.error(""Error closing SQL Connection"", e);"
"<line1>    logger.info(""[{}] Cluster node {} begins to set up with {} mode"",getServerClientName(),thisNode,ClusterDescriptor.getInstance().getConfig().isUseAsyncServer() ? ""Async"" : ""Sync"");<line8>    logger.info(""[{}] Cluster node {} is up"", getServerClientName(), thisNode);"
"<line1>    LOG.debug(""Inside Message Monitoring API sendSuccessEdgeNotification() method."");<line24>      LOG.error(errorMsg, ex);<line25>    LOG.debug(""Exiting Message Monitoring API sendSuccessEdgeNotification() method."");"
"<line3>      LOGGER.debug(""Collecting stats from integrationId: {}"", integrationId);<line32>      LOGGER.error(""Collecting stats from integrationId: {}"", integrationId);<line32>      LOGGER.debug(""Collecting stats from integrationId: {}"", integrationId, e);"
"<line5>              logger.info(""Deleting HBase table {}"", htable);<line10>                logger.info(""Deleted HBase table {}"", htable);<line11>                logger.info(""HBase table {} does not exist."", htable);<line14>            logger.error(""Deleting HBase table failed"");"
"<line2>    LOG.info(""Hadoop version: "" + org.apache.hadoop.util.VersionInfo.getVersion());<line2>    LOG.info(""HBase version: "" + org.apache.hadoop.hbase.util.VersionInfo.getVersion());<line4>    LOG.info(""Got platform bridge: "" + bridge.getClass().getName());"
"<line11>            logger.debug(""findDeviceFirewallOpenPorts() :: adding new Open Port Entry: {}"",((FirewallOpenPortConfigIP4) netConfig).getPort());"
"<line17>    log.info(""Start put [key="" + key + ']');"
"<line9>    LOG.info(""Configuring stats with csv output to directory [{}]"", outputDir.getAbsolutePath());"
"<line3>    log.debug(""[{}] evict device profile from cache: {}"", profileId, oldProfile);"
<line19>      log.error(systemException, systemException);
<line2>    log.trace(text);
"<line3>      logger.warn(""Tried to report FP Complications of a non-existing EC, with submission: "" + submission);"
"<line2>    LOGGER.trace(""Visiting PropertyName expression"");"
"<line12>        logger.info(""Starting warm up for new cache of site '{}'"", siteName);<line22>        logger.info(""Warm up for new cache of site '{}' completed (switched with old cache) in {} secs"",siteName,stopWatch.getTime(TimeUnit.SECONDS));<line24>        logger.error(""Cache warm up failed"", e);<line26>      logger.info(""Starting warm up for cache of site '{}'"", siteName);<line29>      logger.info(""Warm up for cache of site '{}' completed in {} secs"",siteName,stopWatch.getTime(TimeUnit.SECONDS));"
"<line2>    LOG.trace(""Start the unmarshalling of the node: ""+ nodeToUnmarshal.toString()+ "" to clazz: ""+ destinationClazz.toString());"
"<line8>        logger.warn(String.format(""Failed to close file %s"", olf.f.getAbsolutePath()), e);"
"<line8>      log.error(""Error while saving the report .. {}"", e.getMessage());"
"<line2>      log.warn(""No .zip suffix[%s], putting files from [%s] into it anyway."", outputZipFile, directory);"
"<line4>      log.debug(""created subreports thread executor ""+ threadExecutor+ "" for ""+ fillContext.getMasterFiller().getJasperReport().getName());"
"<line11>            log.trace(""operationComplete({}) available={}"", this, available);<line14>        log.error(""operationComplete({}) invalid available count: {}"", this, available);<line16>        log.error(""operationComplete({}) Error ({}) signalled: {}"",this,err.getClass().getSimpleName(),err.getMessage());<line18>      log.error(""operationComplete({}) Incomplete future signalled: {}"", this, future);<line22>      log.warn(""operationComplete({}) unexpected ({}) due to close: {}"",this,e.getClass().getSimpleName(),e.getMessage());"
"<line3>          logger.warn(""No repo loaded warning."");"
"<line3>    logger.debug(""Updating user with id : "" + userId);"
"<line23>      logger.debug(""failed to reset"", cx);"
"<line8>    log.debug(""new SHARD datasource'{}' is ADDED in {} ms"",shardDataSourceCreateHelper.getShardName(),System.currentTimeMillis() - start);"
"<line13>        log.error(pe.getMessage());<line14>        log.error(""Unable to numeric argument "" + start + "": "" + nfe.getMessage());"
"<line9>    log.debug(""cacheKey:{} LessEqualsFilter: {} percentiles[{}] = {} multiplier: {}"",cacheKey,lessEqualFilter,minBound,percentiles[minBound],result);"
"<line4>      log.debug(""Stylesheet is "" + xslFile);<line7>      log.debug(out.toString());<line10>      log.error(""Failed to configure transformer"", tce);"
"<line5>      logger.info(""Land[Follower] -> server mast be Follower, but ->"" + object.getStatus());<line12>    logger.info(""Land[Follower] -> initiate the election."");"
"<line12>      LOGGER.info(""Source "" + getSourceName() + "" provides no token data."");<line17>      LOGGER.warn(""Could not find data for token "" + name + "", set "" + set + ""."");<line24>        LOGGER.warn(""Not enough images variants for token with type number ""+ type+ "", name ""+ name+ "", set ""+ set+ '.');"
<line18>      log.debug(e.toString());
"<line11>        logger.trace(""HttpSubjectSecurityFilter skipped because we are already allowed or logged in."");<line19>        logger.trace(""HttpSecurityStandardFilter skipped because no realm is configured."");"
"<line2>    logger.info(""Running: testSpdyServerSessionHandlerGoAway v3"");<line3>    logger.info(""Running: testSpdyServerSessionHandlerGoAway v3.1"");"
<line14>      logger.info(ex.getMessage());
"<line11>    logger.debug(""About to marshal task attachment with id '{}' {}"", attachmentId, attachment);"
"<line14>      log.debug(""Falling back to describe '{}' table by querying {}"", tableId, db);"
"<line5>      LOG.debug(""Using "" + impl.getName() + "" for "" + protocol.getName());"
"<line1>    LOG.debug(""Registering loggers..."");<line2>      LOG.info(""Registering logger: "" + logger.getClass().getName());"
"<line5>      logger.error(""Unable to access "" + properties_file, ex);"
"<line10>                logger.warn(""Link extractor ({}) returned invalid URI: {}"", name, e.getMessage());"
"<line2>    logger.debug(""Restoring vm ""+ vm.getUuid()+ ""from backup ""+ backup.getUuid()+ "" on the Dummy Backup Provider"");"
"<line2>    log.info(""Start of downPhysicalInterface call"");<line5>    log.info(""End of downPhysicalInterface call"");"
"<line2>      log.debug(""going ""+ MemberExpression.DIRECTION.DOWN+ "" by ""+ (expression.isWildcard() ? ""wildcard"" : ""key: ["" + expression.getObjectKey() + ""]"")+ "" on ""+ jrJsonNode.getDataNode());"
"<line2>      log.warn(""Detected failure in monitor accessing "" + getUrl() + "": "" + problem);"
"<line10>        log.debug(StringBundler.concat(""Unable to locate status for background task "",backgroundTaskId,"" to process "",message));"
"<line10>      logger.debug(""deleting account "" + "" csid="" + id);<line13>      logger.debug(""deleted account "" + "" csid="" + id);"
"<line8>            logger.error(""Session not found : "" + sessionId);"
"<line5>      log.info(""using AWS for salt tests because named:SaltTests does not exist"");"
<line24>      logger.error(e.getMessage(), e);
"<line3>    LOGGER.debug(""Get all criteria={}"", criteria);"
"<line6>      logger.error(""TTransportException writing to internal frame buffer"", e);<line8>      logger.error(""Exception writing to internal frame buffer"", e);"
"<line26>      LOGGER.error(""ActivityMetaDataDao - getFrequencyRunsDetailsForActiveTasks() :: ERROR"", e);"
"<line2>      LOG.debug(""truncating Cassandra table {}"", mapping.getCoreName());"
<line18>      LOG.error(msg, e);
"<line2>    LOGGER.debug(""ExtendedRandomLength: "" + msg.getExtendedRandomLength().getValue());"
"<line7>        logger.debug(""Pipelines [id={} , pipeline={} ]"", line[0], line[1]);"
"<line10>            LOG.error(""Unable to extract the underlying Sequence Iterator: {}. Falling back to""+ "" EMPTY_ITERATOR"",xpe.getMessage(),xpe);"
"<line15>      log.debug(""Worlflow Detail ====  Mojo {}  Loaded Successfully"", deployedMojoName);<line36>      log.error(e);"
"<line2>      logger.trace(LogMarker.DLS_VERBOSE, ""[DLockGrantor.handleLockQuery] {}"", query);"
"<line2>    logger.info(""Get all modules from registry"");"
"<line23>    LOG.info(""derby repository is set to "" + conf.get(CatalogConstants.CATALOG_URI));<line40>    LOG.info(""Mini Tajo cluster is up"");<line42>    LOG.info(""=                           MiniTajoCluster starts up                              ="");<line44>    LOG.info(""= * Master Address: "" + tajoMaster.getMasterName());<line44>    LOG.info(""= * CatalogStore: "" + tajoMaster.getCatalogServer().getStoreClassName());<line44>    LOG.info(""------------------------------------------------------------------------------------"");<line44>    LOG.info(""= * Warehouse Dir: "" + TajoConf.getWarehouseDir(c));<line44>    LOG.info(""= * Worker Tmp Dir: "" + c.getVar(ConfVars.WORKER_TEMPORAL_DIR));<line44>    LOG.info(""===================================================================================="");"
"<line1>    logger.debug(""handleMessage started..."");<line8>      logger.debug(""Error while communicating with an other gatekeeper: {}"", ex.getMessage());<line8>      logger.debug(""Exception:"", ex);"
"<line3>      LOGGER.debug(""Listing topics"");"
"<line9>      LOGGER.error(""Failed to get revision {} of note {}"", revId, noteId, e);"
"<line2>    logger.debug(""RestartFreezePosition="" + seekTo);"
"<line1>    logger.debug("">> deleting server tags..."");<line15>        logger.warn(ex, "">> could not delete tag: %s"", tag);"
"<line3>      log.warn(""addGrid {} already exists"");"
"<line13>    log.info(""in build...."");<line24>      log.warn(""build unauthorized!"");<line28>      log.info(""validating dates...."");<line29>      log.info(""dates valid"");<line58>        log.warn(message);<line64>        log.error(""exception in build:"", any);<line67>      log.warn(""something went wrong with validation"");"
"<line41>        logger.warn(""Cannot find a parser for the "" + attribute.getName().getNamespaceURI() + "" namespace"");"
"<line7>      logger.error(""Unable to determine if user "" + string + "" exists"", fex);"
<line18>      log.error(systemException, systemException);
"<line1>    log.debug(""Clearing resource definitions"");"
"<line2>      LOG.info(""Username was not supplied."");<line4>      LOG.info(""Password was not supplied."");"
"<line4>      LOG.debug(""Removed Scout server session from cache [scoutSessionId={}, httpSessionId={}]."",scoutSessionId,httpSessionId);"
"<line3>    logger.info(""coeff = "" + coeff.getClass().getSimpleName());<line4>    logger.info(""vars = "" + Arrays.toString(vars));<line8>    logger.info(""tord = "" + tord);<line11>    logger.info(""m = "" + m);"
"<line2>    logger.debug(""Received command {} for channel {}"", command, channelUID);<line14>          logger.debug(""Unhandled command {}."", command);"
"<line2>    log.debug(""Migration1707ArtifactUuidFix  fix group:  resource id {}, group name {} "",resource.getUniqueId(),group.getName());"
"<line24>                  logger.info(""Schedule job \""{}\"" for node template \""{}\"" (instance={}) because""+ "" annotation \""{}\"" is attached..."",plugin.getClass().getSimpleName(),nodeTemplate.getId(),nodeTemplateInstance.getId(),policyTemplate.getType());<line33>          logger.info(""Job statistics: {}"", context.getDeploymentTest().getStatistics());"
"<line5>      logger.warn(String.format(""Class[%s] doesn't has EO."", VOClazz));"
"<line3>      LOGGER.warn(""Failed to find segment ZK metadata for segment: {}, table: {}"",segment,_offlineTableName);<line14>        LOGGER.warn(""Failed to find valid end time for segment: {}, table: {}"", segment, _offlineTableName);"
"<line14>          log.error(illegalArgumentException.getMessage(), illegalArgumentException);<line37>            log.error(""Invalid CSRF token"", principalException);<line42>        log.error(""The first parameter of the method signature must be an ""+ ""ActionRequest or ResourceRequest"");<line44>      log.error(""The method signature must include (ActionRequest, ""+ ""ActionResponse) or (ResourceRequest, ResourceResponse) ""+ ""as parameters"");"
"<line6>      LOG.info(""Using YAML configuration!"");<line8>      LOG.info(""Using CLI configuration!"");"
"<line2>    LOG.info(""Catalog Server ("" + bindAddress + "") shutdown"");"
"<line23>        LOG.info(""Found new partition {} of table {}, generating splits for it"",partSpec,tablePath.getFullName());"
"<line2>    log.debug(""Getting recommendations for user with OS ID: "" + openSocialId);<line9>    log.debug(recommendations.size() + "" recommendations found"");<line13>      log.debug(""There are "" + people.size() + "" Person objects available for recommendations."");"
"<line1>    logger.debug(""Recommending to Peer {} that Protocol Version {} be used"",peerDescription,protocolVersion);"
"<line4>      log.warn(""MD5 calculation disabled"");"
"<line16>      log.debug(""no deployment artifacts are configured for generated artifacts"");<line21>      log.debug(""no env type {} are configured for generated artifacts"", envType);<line48>        log.debug(""failed to create heat env artifact on resource instance"");"
"<line7>    LOGGER.info(""Experiment created successfully"");"
"<line17>      LOGGER.debug(""Created voltagelevel {}"", voltageLevel.getId());"
"<line2>      LOG.info(""Building suggester index for: "" + suggester.getName());<line5>      LOG.info(""Built suggester "" + suggester.getName() + "", took "" + timeTakenMillis + "" ms"");<line6>      LOG.error(""Exception in building suggester index for: "" + suggester.getName(), e);"
"<line5>        logger.trace(""DISCOVERY lookup #{} for domains: {}, interface: {}, {}, gbids: {} returned""+ "" DiscoveryError {}, continuing"",arbitrationCnt,domains,interfaceName,interfaceVersion,Arrays.toString(gbids),error);<line15>        logger.trace(""DISCOVERY lookup #{} for domains: {}, interface: {}, {}, gbids: {} returned""+ "" DiscoveryError {}, giving up"",arbitrationCnt,domains,interfaceName,interfaceVersion,Arrays.toString(gbids),error);"
"<line7>        logger.warn(""Exception at getFavorites"", e);"
"<line3>      LOGGER.info(""dispose: "" + this.getId());"
"<line1>    logger.debug(""Get Acl"");"
"<line16>          LOGGER.info(""!!! IMPORTANT !!! Create admin@imeji.org as system administrator with password""+ "" admin. !!! CHANGE PASSWORD !!!"");<line17>          LOGGER.info(""Created admin user successfully:"" + Imeji.adminUser.getEmail(), e);<line19>        LOGGER.info(""Admin user already exists:"");<line20>          LOGGER.info(admin.getEmail() + "" is admin + ("" + admin.getId() + "")"");<line23>      LOGGER.warn(Imeji.adminUser.getEmail() + "" already exists"", e);<line25>        LOGGER.warn(Imeji.adminUser.getEmail() + "" already exists"");"
"<line8>    LOGGER.info(""Finding devices which have no owner for organisation: {}."", organisationIdentification);"
"<line9>        LOG.debug(""Key Factory is null"");<line11>      LOG.error(e.getLocalizedMessage(), e);"
"<line7>      logger.error(""Failed to process heartbeats"", e);"
<line25>      logger.warn(e.getMessage(), e);
"<line47>              log.warn(String.format(""Ignore property %s"", name));<line66>            log.warn(e);"
<line38>        LOG.error(ERROR_TRANSLATING_POSTGRES_EXC_MSG, pSqlException);
"<line4>      LOGGER.debug(""Added object {} to the commit for updating"", obj.getOID());"
"<line1>    logger.debug(""getCloudsByCloudRequestDTOs started..."");"
"<line3>      log.warn(""No service to register for nacos client..."");<line11>      log.info(""nacos registry, {} {} {}:{} register finished"",group,serviceId,instance.getIp(),instance.getPort());<line12>      log.error(""nacos registry, {} register failed...{},"", serviceId, registration.toString(), e);"
"<line4>      LOGGER.debug(""attempting to transfer node to {} with ACL {}: {}"",zooKeeperEndpointConfig,node.getAcls(),node);<line6>      LOGGER.info(""transferred node {} in {}"", node, zooKeeperEndpointConfig);"
"<line10>      LOG.info(getName() + "": readAndProcess caught InterruptedException"", ieo);<line12>      LOG.info(getName() + "": readAndProcess threw exception "" + e + "". Count of bytes read: "" + count,e);<line16>        LOG.debug(getName()+ "": disconnecting client ""+ c+ "". Number of active connections: ""+ numConnections);"
"<line7>      log.error(""InvocationTargetException or InterruptedException during conversion"", e);"
"<line6>      logger.error(""Failed to instantiate asr driver (className = "" + clazz + "")"", e);"
"<line11>        LOGGER.debug(""Enabling ""+ instance.target()+ "" since the profile value does not match the active profile."");<line12>        LOGGER.debug(""Disabling ""+ instance.target()+ "" since the profile value matches the active profile."");"
"<line1>    LOG.error(""TestReadListener error"", t);"
"<line7>        logger.warn(""Exception at createSavedSearch"", e);"
"<line7>    LOG.info(""TESTING TOPICS ""+ prod_broker_url+ "" -> ""+ cons_broker_url+ "" (""+ num_msg+ "" messages)"");<line11>    LOG.trace(""Removing existing Topic"");<line12>    LOG.trace(""Creating Topic, "" + topic_name);"
"<line2>      log.error(""Unexpected end state: {}"", states.peek());"
"<line6>    LOG.info(""Result of CalcMath.calcSum() method is {}"", result);"
"<line11>      logger.error(""query logs from db error"", e);"
"<line1>    log.debug(""persisting RechteRolleBericht instance"");<line3>      log.debug(""persist successful"");<line4>      log.error(""persist failed"", re);"
"<line2>    logger.debug(""========SSLCertificateExpiryRule started========="");<line14>      logger.info(PacmanRuleConstants.MISSING_CONFIGURATION);<line33>          logger.debug(""========SSLCertificateExpiryRule ended with annotation {} : ========="", annotation);<line37>        logger.info(""Elb with SSL validity not expired"");<line39>    logger.debug(""========SSLCertificateExpiryRule ended========="");"
"<line19>    log.info(""Removing all confirmationConfiguration objects"");"
"<line4>      logger.debug(""Not timed out: "" + maxIdleTime + "" "" + timeoutPeriod);<line6>      logger.debug(""Timed out: "" + maxIdleTime + "" "" + timeoutPeriod);"
"<line17>      logger.info(""Resolved function from provided [routing-expression]  "" + routingExpression);"
"<line20>      logger.error(""StudyDAOImpl - getComprehensionTestQuestionById() - Error"", e);"
"<line15>        logger.debug(""sending email to-->"");<line30>      logger.error(""error sending email"", e);"
"<line2>      log.info(""Host ""+ host.getLabel()+ "" is compatible for vCenter cluster operation due to type ""+ host.getType()+ "" and OS version ""+ host.getOsVersion());<line4>      log.info(""Host ""+ host.getLabel()+ "" is not compatible for vCenter cluster operation due to type ""+ host.getType());"
"<line7>      logger.info(""Partition group {}/node {} receive vote response from {}, ""+ ""current term is {}, term is {}"",topicPartitionGroup,localNode,node.getNodeId(),currentTerm,term);<line16>      logger.warn(""Partition group {}/node {} handle vote response fail"", topicPartitionGroup, localNode);"
"<line2>    log.debug("""");"
"<line14>        logger.debug(""Sending request %s: %s"", request.hashCode(), request.getRequestLine());<line18>        logger.debug(""Receiving response %s: %s"", request.hashCode(), response.getStatusLine());"
<line6>        log.debug(_NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey);
"<line3>    log.error(""Found unresolved type in the VDM AST"");"
"<line5>    LOGGER.info(""Starting the importData process"");<line13>      LOGGER.error(""Error importing RINGGOLD data"", e);<line15>      LOGGER.warn(""Ringgold import completed"");"
"<line22>      logger.info(""local avatar not available"", t);"
"<line21>      LOG.debug(""Updating endtime of coord {} to {} on cluster {}"",coord.getId(),SchemaHelper.formatDateUTC(endTime),cluster);<line23>        LOG.info(""Nothing is materialized for this coord: {}"", coord.getId());<line24>          LOG.info(""Setting end time to START TIME {}"",SchemaHelper.formatDateUTC(coord.getStartTime()));<line26>          LOG.info(""Setting end time to START TIME {}"", SchemaHelper.formatDateUTC(endTime));<line29>        LOG.info(""Actions have materialized for this coord: {}, last action {}"",coord.getId(),SchemaHelper.formatDateUTC(lastActionTime));<line31>          LOG.info(""Setting pause time on coord: {} to {}"",coord.getId(),SchemaHelper.formatDateUTC(pauseTime));"
"<line4>      logger.debug(""Entered enforceCompareDatastreamChecksum"");<line26>      logger.debug(""Exiting enforceCompareDatastreamChecksum"");"
"<line4>      logger.debug(this.getClass().getName() + ""/handleRequest!"");<line7>      logger.error(""Not enough path components on the URI: {}"", request.getRequestURI());<line39>      logger.error(e.getMessage(), e);"
"<line3>    log.info(""Request to delete "" + bucket + "" sent"");<line4>    log.info(""Bucket "" + bucket + "" is deleted"");"
"<line2>    log.info(""Validating failing in configuration with explicit total flink and managed memory size."");"
"<line7>        LOG.error(""Exception in WebDriverManager while getWebDriver "", e);"
"<line33>        log.error(""Interrupted while waiting for XML tag threads to terminate - no datatags were added!"");"
"<line9>          logger.debug(format(""Some annotations would get lost because there was no metaid defined on {0}. To""+ "" avoid this, an automatic metaid ''{0}'' has been generated."",getElementName(), getMetaId()));<line11>          logger.warn(format(""Some annotations can get lost because no metaid is defined on {0}."",getElementName()));<line41>          logger.warn(format(""Plugin for namespace {0} is null!"", key));"
"<line11>      this.logger.info(""{} events were saved in the new store because they did not already exist"",eventsToSave.size());"
"<line6>      log.debug(""committing in TestTransactionUtil"");"
"<line19>      logger.debug(""Nothing to refresh"");"
"<line5>    LOG.info(""Hive source({}}) {} use time: {} ms, result: {}"",tablePath,operationName,System.currentTimeMillis() - startTimeMillis,result);"
<line24>      log.error(e);
"<line7>      log.info(""Exception in retrieve"", e);<line8>      log.info(""Interrupted in retrieve"", e);"
"<line1>    LOGGER.info(""Resuming consumer "" + getFlowName() + "" ["" + creationTimestamp + ""]"");<line2>      LOGGER.info(""Resuming action ""+ currentAction.getName()+ "" in flow ""+ getFlowName()+ "" [""+ creationTimestamp+ ""]"");"
"<line1>    logger.debug(""handleRoomScene called for channel: {}, command: {}"", channelUID, command);<line16>        logger.warn(""Unexpected UPB Room scene: {}"", channelUID);"
"<line1>    LOGGER.info(""Searching cases in "" + folder.getPath());"
"<line11>      logger.debug(""getGroup (by id) group not found: "" + identifier);<line12>      logger.debug(""getGroup (by id) found group: {} for id: {}"", group.getName(), identifier);"
<line15>    log.info(sb.toString());
"<line1>    log.debug(""finding MbMassPhase instance by example"");<line8>      log.debug(""find by example successful, result size: "" + results.size());<line10>      log.error(""find by example failed"", re);"
"<line1>    log.info(""Testing attributes"");<line5>    log.info(""Attributes tested successfully"");"
"<line8>      log.error(""Tika.detect failed:"" + e.getMessage());<line14>          log.warn(""Exception closing TikaInputStream. This leaves tmp-files: "" + e.getMessage());<line30>        log.error(""TikaExtractor.parse(): "" + tikaType + "" : "" + o.getMessage());<line31>        log.error(""TikaExtractor.parse(): "" + tikaType + "" : "" + r.getMessage());<line36>            log.warn(""Exception closing TikaInputStream. This leaves tmp-files: "" + e.getMessage());<line42>      log.debug(""Tika Exception: "" + e.getMessage());"
"<line27>    LOG.debug(""Successfully created default index set: {}"", savedConfig);"
"<line33>              Logger.info(String.format(""Bienvenue, Bienvenido, Willkommen, Hello, Namaskar, Welkom, Bonjour to""+ "" OpenPnP version %s."",Main.getVersion()));"
"<line7>    log.debug("" {}  Received  {} : {}"", consumerTag, routingKey, message);<line10>      log.debug(""This is normal json object for webhook health "");<line16>        log.debug(""Webhook Health Record update status {} ==== "", isRecordUpdate);<line20>        log.error("" Data List is empty for webhook health record "");<line26>      log.error(e);"
<line15>      LOGGER.warn(String.format(GENERIC_INSECURE_DEFAULTS_MSG, keystorePath), e);
"<line30>    LOG.info(""NCSARequestlogging is using directory {}"", lc.getLogNCSADirectory());"
"<line61>      log.error(""BPMN Analytics Core - Date Vs Task Instance Count TaskLevelMonitoring error."", e);<line63>      log.debug(""Date Vs Task Instance Count Result:"" + sortedResult);"
"<line40>                log.trace(""Sending WebSocket PING"");<line48>            log.error(""Error while processing incoming message"", e);"
"<line3>    logger.info(""Attempting to terminate all cloud resources (timeout set to ""+ duration+ "" ""+ unit+ "")"");"
"<line10>      logger.warn(""no entity manager found with code {}"", entityManagerCode);"
"<line12>    LOGGER.debug(""Using album title '"" + title + ""'"");<line17>      LOGGER.info(""[+] Creating directory: "" + Utils.removeCWD(this.workingDir));<line19>    LOGGER.debug(""Set working directory to: "" + this.workingDir);"
"<line1>    LOG.info(""Disconnecting sessions"");"
"<line5>        logger.warn(""Exception when de-serializing "" + clazz + "" with "" + jsonString, ex);"
"<line3>      Log.info(""HTTP ERR(""+ session.getStreamID().getID()+ ""): ""+ bindingError.getErrorType().getType()+ "", ""+ bindingError.getCondition()+ ""."");<line15>        Log.debug(""Closing session due to error: {}. Affected session: {}"", bindingError, session);"
"<line3>      logger.debug(""{} onNext:{}"", listener, value);"
"<line8>    log.info(""Looking for matching Object names..."");<line52>    log.info(String.format(""Found %s matching Objects"", objectNames.size()));"
"<line18>          logger.info(""the connection has resumed."");<line39>      logger.warn(""the connection to the server has been interrupted ..."");<line49>          logger.error(""listener threw exception: "" + t);"
"<line18>    log.trace(""Started blockchain Constants"");<line20>    log.trace(""blockchain Constants result : {}"", dto);"
"<line1>    LOG.trace(""Seeking offset for topic-partition [{}] with [{}] and committed offset [{}]"",newTp,firstPollOffsetStrategy,committedOffset);"
<line16>      log.error(portalException, portalException);
"<line2>    LOG.info(""Closing SourceCoordinator for source {}."", operatorName);<line13>    LOG.info(""Source coordinator for source {} closed."", operatorName);"
"<line42>              logger.warn(String.format(""update vms priority failed on host[%s],because %s"",inv.getUuid(), reply.getError()));"
"<line2>    logger.debug(""Removing all electron containers"");"
"<line12>    logger.info(""Initializer Jackrabbit Repository in: "" + home.getAbsolutePath());"
"<line2>    logger.debug(""Connecting to database {} at {}"",configuration.getDatabase().getDriverClass(),configuration.getDatabase().getUrl());"
"<line8>      logger.error(""event=cns_topic_delivery_policy_to_string"", e);"
"<line12>      LOG.trace(""Response body = {}"", response);<line21>      LOG.error(""Error while processing response"", exception);"
"<line12>          logger.info(""Added Airavata Gateway with Id: "" + gateway.getGatewayId());<line13>            logger.info(""Gateway with ID: {}, is now APPROVED, replicating to subscribers."",gateway.getGatewayId());<line24>      logger.error(""Error adding gateway-profile, reason: "" + ex.getMessage(), ex);"
"<line7>    logger.debug(""getting page list with request {}"", searchRequest);"
<line25>      LOGGER.error(e.getLocalizedMessage(), e);
"<line5>      log.warn(className + "" could not be found"", e);<line8>      log.warn(className + "" is not a Lucene Analyzer"");<line10>      log.warn(className + "" can not be used as a JackrabbitAnalyzer component"");<line33>    log.warn(className + "" could not be instantiated"", cause);"
"<line17>            LOGGER.error(""Unable to find photos in response"");<line21>        LOGGER.info(jsonData);<line23>          LOGGER.info(i);<line27>            LOGGER.error(""Flickr MalformedURLException: "" + e.getMessage());"
"<line3>    log.info(""Testing Job lifecycle for {} of type {}"",jobInfo.getOperation(),jobInfo.getContentType());"
"<line56>      log.trace(""Created network policy {}"", networkPolicy);"
"<line5>      log.debug(""log4j-web not available, skipping MDC setup"");"
"<line4>    log.info(""Starting getAllDisplays."");<line8>      log.error(""getAllDisplays Failure:"", e);<line26>    log.info(""Returning Json from getAllDisplays."");"
"<line4>    LOGGER.info(""id={}"", id);"
"<line8>        LOG.error(""JSON Error Exception"", ex);"
"<line11>      LOGGER.warn(""Failed to save HexviewerPlus settings file. Error:{}"", ex.toString());"
"<line9>          LOG.trace(""Tracing: finish server span={}"", span);<line15>        LOG.warn(""Tracing: could not find managed span for exchange={}"", exchange);<line17>      LOG.warn(""Tracing: Failed to capture tracing data"", t);"
"<line2>    LOG.debug(""testClassLdapConnectionPool"");"
"<line8>      logger.info(""reconfigureDisks() extend data disk #""+ vdKey+ "" space to ""+ newDiskSpace+ "" (""+ vdDataDisk.getDeviceInfo().getLabel()+ "")"");<line9>        logger.error(""Cannot reduce size of data disk "" + vdDataDisk.getDeviceInfo().getLabel());<line9>        logger.error(""Current disk space: ""+ vdDataDisk.getCapacityInKB()+ "" new disk space: ""+ newDiskSpace);<line17>        logger.debug(""Data disk size has not been changed. "" + newDiskSpace + "" KB"");"
"<line1>    logger.debug(""deleting Secuserrole instance"");<line4>      logger.debug(""delete successful"");<line5>      logger.error(""delete failed"", re);"
"<line8>      logger.debug(""Interrupted in packet parser thread shutdown join."");<line9>    logger.debug(""TelegesisFrameHandler closed."");"
<line4>      LOGGER.info(LOG_MSG_LOGIN_REQUEST, loginRequest.getEmail());<line7>      LOGGER.info(LOG_MSG_LOGIN_REQUEST_FAILURE, loginRequest.getEmail());
"<line15>    LOG.debug(""owner:"" + owner + "", group:"" + group);"
"<line14>        log.error(""Problems encountered when trying to construct local variable"");"
"<line6>      log.info(""Found previous state: "" + iterationPath);"
"<line9>        log.debug(""No measurement schedule for: "" + key);<line14>    log.debug(""Un-scheduling "" + items.length + "" metrics for "" + ent);"
"<line1>    log.info(""Opening openshift web page on route {}"", ocRoute);<line7>        log.info(""User is not logged"");"
"<line22>              LOG.error(""Error during processing of: "" + vfile.getName(), e);"
"<line14>          LOG.debug(""Loading shared default stream instance"");<line17>            LOG.warn(""Unable to load default stream, tried {} times, retrying every 500ms. Processing is""+ "" blocked until this succeeds."",i + 1);"
"<line2>      logger.debug(""Evohome Gateway not online, scanning postponed"");"
"<line14>    log.info(""added legacy, at: "" + addedLegacyLocationUri);<line15>    log.info("" contents: "" + location);"
"<line25>    logger.debug(SET_ADVERTISING_PARAMETERS_HCITOOL_MESSAGE,() -> interfaceName,() -> String.join("" "", cmd));<line25>    logger.info(""Set Advertising Parameters on interface {}"", interfaceName);"
"<line2>      LOG.info(""Stopping the StorageContainerManager"");"
<line5>        LOG.error(e);
"<line2>    logger.info(""Starting edge labeling..."");<line31>    logger.info(processedRels + "" relations labeled."");"
"<line3>      logger.debug(""validate() action="" + action.name());<line28>        logger.error(msg);"
"<line13>    LOG.debug(""From primitive type array we've created the list: "" + resultList);"
"<line1>    log.debug(""attaching dirty TmpRepZobbaumas instance"");<line3>      log.debug(""attach successful"");<line4>      log.error(""attach failed"", re);"
"<line12>      log.trace(""getPath({}): {}"", uri, p);"
"<line2>    LOG.info(""VisibilityProcessor.process starts................."");<line18>    LOG.info(""Calling ConstructVisibilityRest................."");<line20>    LOG.info(""VisibilityProcessor.process ends................."");"
"<line3>    LOGGER.info(""Updating Elasticsearch index documents, indexName={}."", indexName);<line14>          LOGGER.error(""Bulk response error={}."", bulkResponse.buildFailureMessage());<line17>      LOGGER.error(""Caught IOException while attempting to use the ElasticsearchRestHighLevelClient."",ioException);"
"<line4>      LOGGER.info(""currentAndroidVersion="" + currentAndroidVersion);<line10>        LOGGER.info(""Allow Mock config is present:"" + allowMock.isElementPresent(SHORT_TIMEOUT));"
"<line2>    logger.info(""Rollback to position: {}, left: {}, right: {}, flushPosition: {}, store: {}..."",Format.formatWithComma(position),Format.formatWithComma(left()),Format.formatWithComma(right()),Format.formatWithComma(flushPosition()),base.getAbsolutePath());<line21>      logger.info(""Rollback finished, left: {}, right: {}, flushPosition: {}, store: {}."",Format.formatWithComma(left()),Format.formatWithComma(right()),Format.formatWithComma(flushPosition()),base.getAbsolutePath());"
"<line2>    log.info(""Querying"");<line8>    log.info(""Done Querying"");<line9>    log.info(""delta="" + (endTime - startTime));"
"<line2>    logger.debug(""deactivating..."");"
<line9>      log.error(exception, exception);
"<line10>    log.info(""Response: {}"", response);"
"<line12>                    logger.info(""## stop the canal client"");<line14>                    logger.warn(""##something goes wrong when stopping canal:"", e);<line15>                    logger.info(""## canal client is down."");"
"<line16>          ServletContextInitializerBeans.logger.debug(""Created ""+ type.getSimpleName()+ "" initializer for bean '""+ beanName+ ""'; order=""+ order+ "", resource=""+ getResourceDescription(beanName, beanFactory));"
"<line4>      log.debug(tableTag + "" lang sys table: "" + langSysTag);<line15>      log.debug(tableTag + "" lang sys table reorder table: "" + lo);<line15>      log.debug(tableTag + "" lang sys table required feature index: "" + rf);<line15>      log.debug(tableTag + "" lang sys table non-required feature count: "" + nf);<line20>        log.debug(tableTag + "" lang sys table non-required feature index: "" + fi);"
"<line4>      logger.debug(getTestBanner(testName, CLASS_NAME));<line41>        logger.debug(testName + "": knownItemResourceId="" + newID + "" inAuthority="" + vcsid);"
"<line15>      LOG.error(""Exception {} detected."", ex.toString());"
"<line38>      LOGGER.warn(""Adapt DefaultDatabaseSet to ClusterDatabaseSet exception"", t);"
"<line11>              Log.error(""Error while retrieving org unit ""+ userJS.getMainOrgUnit()+ "" for user ""+ userJS.getId());"
"<line2>      LOG.error(""fail to set config. invalid config scope. Scope: {}."", scope);"
"<line12>          LOG.trace(""No delay for exchangeId: {}"", exchange.getExchangeId());"
"<line20>      LOG.warn(""exception while writing the last shutdown report"", e);"
"<line38>        LOGGER.warn(""Errored as expected"", ex);"
"<line4>        logger.trace(this + "" informing "" + listener + "" about node down = "" + nodeId);"
"<line10>          LOG.info(""remove expired segment success. segment: {}"", segment);<line11>          LOG.warn(""remove expired segment failed. segment: {}"", segment);"
"<line1>    logger.debug(""Destroying SCTP Server"");"
"<line2>    LOG.info(""Plugin download started"");<line2>    LOG.info(""Source folder: \""{}\"""", url);<line2>    LOG.info(""Destination folder: \""{}\"""", destinationFile);<line13>    LOG.info(""Plugin download completed"");"
"<line6>      logger.debug(""No method name provided, CrudRepository.saveAll will be used."");"
"<line12>      LOG.warn(""RocksDBStateBackend performance will be poor because of the current Flink memory""+ "" configuration! RocksDB will flush memtable constantly, causing high IO and CPU.""+ "" Typically the easiest fix is to increase task manager managed memory size. If""+ "" running locally, see the parameter taskmanager.memory.managed.size. Details:""+ "" arenaBlockSize {} > mutableLimit {} (writeBufferSize = {},""+ "" arenaBlockSizeConfigured = {}, defaultArenaBlockSize = {},""+ "" writeBufferManagerCapacity = {})"",arenaBlockSize,mutableLimit,writeBufferSize,arenaBlockSizeConfigured,defaultArenaBlockSize,writeBufferManagerCapacity);"
"<line2>    log.debug(""Event received"");"
"<line5>      log.info(""Waiting on the Camel Context to stop"");<line6>    log.info(""Closing JMS Session"");<line10>    log.info(""Closing JMS Connection"");<line14>    log.info(""Stopping the ActiveMQ Broker"");"
"<line69>      logger.debug(""Returning OK response with content '{}'"", response);<line82>      logger.error(""Unexpected error during processing {}"", e.getMessage(), e);"
"<line1>    log.info(""Information message text"");"
<line11>        log.debug(ioException, ioException);
"<line5>        LOG.info(""Sleep bookie {}."", addr);<line17>                  LOG.error(""Error suspending bookie"", e);"
"<line21>          logger.warn(""detected overlapped IP blocks: "" + prev + "", "" + curr);"
"<line6>      log.error(String.format(""Could not export to '%s'!"", dir), e);<line8>    log.info(""Export {} account balances registered to file {}"", accountBalances.size(), dir);"
"<line3>      LOG.warn(""Not attempting to re-login since the last re-login was ""+ ""attempted less than {} seconds before."",(MIN_TIME_BEFORE_RELOGIN / 1000));"
"<line11>          logger.debug(""Found match for string in source file: {} in line: {}"", textUnitNameInSource, line);"
"<line6>        logger.error(""ERROR # disconnect meta connection for address:{}"",metaConnection.getConnector().getAddress(),e);"
"<line2>    logger.debug(""finalizing {}"", dataDir);<line6>        logger.error(e.toString(), e);"
"<line2>    logger.info(""System benchmark result = {}"", SystemUtil.bench());"
"<line15>            log.info(MoreObjects.toStringHelper(Thread.currentThread().getName() + "" bundle skip log"").add(""thread-skip"", totalSkip).add(""file-skip"", bundlesSkipped).add(""file-to-skip"", read).add(""global-skip-estimate"", bundlesSkipped + globalBundleSkip.count()).toString());<line29>      log.debug(""mark.indx {} / {}"", mark, stream);"
"<line4>    logger.info(""----------------- Initialising and Starting the OSGi Framework -----------------"");<line4>    logger.info(""FrameworkFactory Class: {}"", factoryClass);<line4>    logger.info("""");<line7>    logger.info(""The OSGi framework has been initialised"");<line16>      logger.info(""The OSGi framework has been started"");<line16>      logger.info("""");<line17>      logger.error(""An error occurred when starting the OSGi framework: {}"", e.getMessage(), e);"
"<line5>        log.warn(""the DB have duplicated jobName({})"", jobName);"
<line31>      LOGGER.debug(ResourcesPublicUtilTest.SHOULD_HAVE_AN_EXCEPTION, e);<line38>      LOGGER.error(ResourcesPublicUtilTest.SHOULD_NOT_HAVE_AN_EXCEPTION, e);
"<line1>    LOG.info(""{}: Writing checkpoint [sequence number {}]."", mMaster.getName(), nextSequenceNumber);<line16>          LOG.error(""{}: Failed to create checkpoint"", mMaster.getName(), t);<line18>        LOG.info(""{}: Cancelled checkpoint [sequence number {}]."",mMaster.getName(),nextSequenceNumber);<line24>          LOG.warn(""{}: Checkpoint was interrupted but shutdown has not be initiated"",mMaster.getName());<line28>      LOG.info(""{}: Finished checkpoint [sequence number {}]."", mMaster.getName(), nextSequenceNumber);<line30>      LOG.error(""{}: Failed to checkpoint."", mMaster.getName(), e);"
"<line11>          LOGGER.info(""Undeploying library {}.{}"", dataverseName, libraryName);"
"<line2>      LOGGER.debug(""succeeded reconciliation {}"", name);"
"<line9>      log.debug(""Waiting for interrupt"");"
"<line4>      logger.error(""{}:replace users failed"", metaGroupMember.getName(), e);"
"<line18>      LOG.warn(""Simple expression: {} detected in header: {} of type String. This feature has been""+ "" removed (see CAMEL-6748)."",value,Exchange.FILE_NAME);<line25>      LOG.trace(""Filename evaluated as expression: {}"", expression);"
<line2>    LOGGER.debug(CleanUpJob.LOG_MARKER, format(Messages.DELETING_FILES_MODIFIED_BEFORE_0, expirationTime));<line4>      LOGGER.info(CleanUpJob.LOG_MARKER, format(Messages.DELETED_FILES_0, removedOldFilesCount));
"<line2>    logger.info(name.getMethodName() + "" - callback - got unexpected exception"");"
"<line11>      log.info(""Sending session opened  event: {}"", sessionOpenedEvent);<line12>      log.info(""Sending session restored event: {}"", sessionRestored);<line26>      log.info(""Sending generic event {}"", genEvent.getEventId());<line31>      log.info(""Sending session closed event {}"", sessionClosedEvent);<line32>      log.info(""Sending session expired event {}"", sessionExpired);"
<line13>      log.error(systemException, systemException);
"<line9>      logger.debug(""Platform: Android"");"
"<line10>      logger.trace(""ASH: RX Timer took {}ms, timer now {}ms"",(System.nanoTime() - sentTime) / 1000000,receiveTimeout);<line14>      logger.debug(""ASH: Frame acked and removed {}"", ackedFrame);"
<line9>      log.error(exception, exception);
"<line17>      logger.warn(""Invalidating this batch! The results returned do not match the addresses given."");"
"<line8>          logger.debug(""Caching "" + type.name());"
"<line15>      log.error(""main threw"", e);"
"<line3>        log.debug(""Will sleep for {} millisec"", failoverPolicy.sleepBetweenHostsMilli);<line7>        log.warn(""Sleep between hosts interrupted"", e);"
"<line6>    logger.info(""Serializer = "" + serializerType + "", UseRawLocalFileSystem = "" + useRawLocalFileSystem);"
"<line7>      log.warn(""query["" + debugKey + ""]: match="" + match + "" miss="" + miss);"
<line28>        log.debug(exception, exception);
"<line6>      log.warn(""getSplits() was called more than once and the 'seed' is set, ""+ ""this can lead to no-repeatable behavior"");"
"<line1>    LOGGER.info(""Waiting for Service {} deletion in namespace {}"",serviceName,kubeClient().getNamespace());<line6>    LOGGER.info(""Service {} in namespace {} was deleted"", serviceName, kubeClient().getNamespace());"
"<line13>      LOGGER.warn(""saveDataHostIndexToZk err:"", e);"
"<line5>      logger.debug(""storing response message {}"", message.toStringForDebug(false));"
"<line3>      LOGGER.info(""Ordered Scan:"");<line14>            LOGGER.info(rec);"
"<line27>        log.info(""Could not read replication table, waiting and will retry"");"
"<line9>        LOG.warn(""Writing a request: {} to channel {} failed : cause = {}"",new Object[] {getBasicInfoFromRequest(request), channel, channelFuture.getCause().getMessage()});"
"<line10>                  log.trace(""SyncRequest+{}"", request);<line12>            .doOnSubscribe(ignore -> log.trace(""Index not found. Resyncing.""))<line15>    return Flowable.error(error);"
"<line5>      log.warn(""Credential reset notification failed"", e);"
"<line15>                    log.warn(""{}: The resource needs to be upgraded from version {} to 'v1beta1' to use""+ "" the status field"",reconciliation,connect.getApiVersion());<line28>                                  log.debug(""{}: Completed status update"", reconciliation);<line30>                                  log.error(""{}: Failed to update status"",reconciliation,updateRes.cause());<line34>                      log.debug(""{}: Status did not change"", reconciliation);<line38>                  log.error(""{}: Current Kafka ConnectS2I resource not found"", reconciliation);<line41>                log.error(""{}: Failed to get the current Kafka ConnectS2I resource and its status"",reconciliation,getRes.cause());"
"<line3>    log.info(""setAVUMetadata()"");<line9>    log.info(""set avu metadata for user: {}"", avuData);<line9>    log.info(""userName {}"", userName);<line10>      log.error(""irods version does not support set avu"");<line15>    log.debug(""sending avu request"");<line21>      log.error(""jargon exception adding AVU metadata"", je);<line23>    log.debug(""metadata added"");"
<line9>      logger.error(NONE_RESOURCE_FOUND_MESSAGE);
"<line3>    LOGGER.debug(""Providing {} docs"", persistQueue.size());"
"<line5>    logger.info(""[Redis-Change] {}, master: {}"", redisMeta, redisMeta.isMaster());"
"<line7>      log.info(""Binding "" + filterParser);"
"<line4>    logger.trace(""wrote: {}, got: {} "", requestPacket, returnPacket);"
"<line4>      logger.debug(""OW connection state: set to failed as max retries exceeded."");"
"<line7>      log.error(""Error creating Drools base message validator."", ex);"
"<line9>        LOG.debug(""Failed to set 'identifier': "" + identifier);<line13>        LOG.debug(""Failed to set 'infoID': "" + infoID);<line34>        LOG.debug(""Failed to set 'Parameter "" + tagName + ""': "" + strValue);"
"<line9>    LOGGER.debug(""Loaded User: "" + result);"
"<line4>    logger.debug(""Constructing validation url: {}"", validationUrl);<line5>      logger.debug(""Retrieving response from server."");<line9>      logger.debug(""Server response: {}"", serverResponse);"
"<line7>      LOG.error(""Update failed"", e);"
"<line37>      LOG.trace(""Cannot convert field {} to type {}. Content was: {}"",fieldName,Type.getTypeName(type),content);"
"<line4>    log.info(""Running load shedding task as leader broker, overload threshold {}, comfort loadlevel {}"",overloadThreshold,comfortLoadLevel);<line14>            log.warn(""Null bundle stats for bundle {}"", lr.getName());<line18>            log.warn(""HIGH USAGE WARNING : Sole namespace bundle {} is overloading broker {}. ""+ ""No Load Shedding will be done on this broker"",bundleName,overloadedRU.getResourceId());<line24>              log.info(""Namespace bundle {} will be unloaded from overloaded broker {},""+ "" bundle stats (topics: {}, producers {}, ""+ ""consumers {}, bandwidthIn {}, bandwidthOut {})"",bundleName,overloadedRU.getResourceId(),stats.topics,stats.producerCount,stats.consumerCount,stats.msgThroughputIn,stats.msgThroughputOut);<line26>              log.info(""Unable to shed load from broker {}, no brokers with enough capacity available ""+ ""for re-balancing {}"",overloadedRU.getResourceId(),bundleName);"
"<line1>    log.info(""Enter project ID {}"", projectId);"
<line8>        log.error(exception, exception);
"<line9>              logger.warn(String.format(""[GC] cannot find a job[name:%s, id:%s], assume it's deleted"", NAME, uuid));<line16>              logger.debug(String.format(""[GC] the job[name:%s, id:%s] is being executed by another trigger,""+ ""skip this event[%s]"",NAME, uuid, path));<line18>            logger.debug(String.format(""[GC] the job[name:%s, id:%s] is triggered by an event[%s]"", NAME, uuid, path));"
"<line1>    logger.trace(""[{}] isDirectory() -> {}"", name, isDirectory);"
"<line2>    log.debug(""Retuning OAuth bearer error response: "" + header);"
"<line11>          logger.warn(""Unsupported mode '{}'"", mode);<line16>      logger.warn(""Prevented deployment '{}' while another deployment is active."", mode);"
"<line14>                LOGGER.warn(""Unit field points to a null value: {}"", field);<line18>              LOGGER.error(""The unit field '{}' seems to be not accessible"", field, e);<line20>            LOGGER.warn(""There is a unit field defined which has no generic type parametrization: {}"",field);"
<line10>        LOG.warn(message);
"<line4>    LOGGER.info(""ServerProfileCompliancePreview object returned to client : ""+ JsonPrettyPrinter.print(compliance));"
"<line5>      logger.error(""TTransportException writing to internal frame buffer"", e);<line7>      logger.error(""Exception writing to internal frame buffer"", e);"
"<line22>      LOG.debug(""<<<< {}"", entry);"
"<line14>              logger.warn(""Invalid commit. Restarting."");<line18>          logger.warn(""SqlException: "" + sqlex.getMessage());<line27>          logger.error(""Error while commiting exception. Terminating server."");"
<line6>      logger.warn(e.getMessage());
"<line12>                LOGGER.error(""**** Error!! Error running privileged action."", ex);"
"<line10>      logger.error(""Fehler beim Ermitteln der DateigrÃ¶Ãe: {}"",datenDownload.arr[DatenDownload.DOWNLOAD_ZIEL_PFAD_DATEINAME]);"
"<line2>    logger.debug(""convertFromString: values={}, toClass={}"", values, toClass);"
"<line38>        logger.debug(""Created JAAS subject with principals: {}"", subject.getPrincipals());<line39>          logger.debug(""Caching assertion for principal {}"", this.assertion.getPrincipal());"
"<line1>    log.debug(""attaching clean StgMbCss instance"");<line3>      log.debug(""attach successful"");<line4>      log.error(""attach failed"", re);"
"<line5>            LOGGER.error(""An Unknown Error Occurred in Thread {}: {}"", t, e.toString());<line5>            LOGGER.error("""", e);"
"<line1>    logger.debug(""checkTokenGenerationRequest started..."");"
"<line5>          log.info(""name server address changed, old: {} new: {}"", this.nameSrvAddr, addrs);<line11>      log.error(""fetchNameServerAddr Exception"", e);"
"<line15>      log.debug(""Expected error while shutting down Hibernate Search, caused by the deletion of an index"",e);"
"<line1>    log.debug(""Stopping {}"", getName());<line8>    log.debug(""Stopped {}"", getName());"
"<line6>    logger.debug(String.format(""Created SR (mount point:%1$s)"", d.path));"
"<line7>        LOG.error(""Cannot find ShareStudy by shareStudyID "" + shareStudyId);<line7>        LOG.error(exception.getMessage());"
"<line6>      LOG.trace(""Added custom request header: {}: {}"", header.getValue(), header.getValue());"
"<line6>      log.info(""Starting JMS adaptor: ""+ adaptorID+ "" started on brokerURL=""+ brokerURL+ "", topic=""+ topic+ "", selector=""+ selector+ "", offset =""+ bytesReceived);"
"<line11>      logger.info(""The restart policy of JobManager pod will be overwritten to 'always' ""+ ""since it is controlled by the Kubernetes deployment."");"
<line18>    logger.info(actual.toJSONString());
"<line23>      log.info(""Client ready."");<line24>      log.info(""Client closing..."");<line26>      log.info(""Client is done."");<line27>      log.error(""Client failed."", e);"
"<line5>      logger.info(yarnCmd);<line8>        logger.warn(""Failed to get yarn logs. "", ex);<line10>      logger.info(""Skip this application {}."", applicationId);"
"<line2>    LOGGER.info(""  test19AnonUpdate"");<line8>      LOGGER.trace(""This should happen."", ex);"
<line8>      LOG.error(Freedomotic.getStackTraceInfo(e));
"<line1>    log.info(format(""Updating category %s to parent %s and name %s"", categoryUrl, parentUrl, name));"
"<line1>    logger.info(""Stopping thread {}..."", thread.getName());"
"<line13>        logger.trace(LogMarker.PERSIST_RECOVERY_VERBOSE, ""bad disk region id!"");"
"<line16>    LOGGER.debug(""Inserted FeatureOfInterest. Created id = {}."", generatedId);"
"<line2>    log.info(""Automatic with 250 with same prefix"");"
<line29>      LOGGER.debug(txt.toString());
"<line2>    LOG.debug(""OFDatagramPacketFramer"");<line15>        LOG.debug(""skipping bytebuf - too few bytes for header: {} < {}"",readableBytes,LENGTH_OF_HEADER);<line15>        LOG.debug(""bb: {}"", ByteBufUtils.byteBufToHexString(bb));<line19>    LOG.debug(""length of actual message: {}"", length);<line21>        LOG.debug(""skipping bytebuf - too few bytes for msg: {} < {}"", readableBytes, length);<line21>        LOG.debug(""bytebuffer: {}"", ByteBufUtils.byteBufToHexString(bb));<line24>    LOG.debug(""OF Protocol message received, type:{}"", bb.getByte(bb.readerIndex() + 1));<line26>      LOG.debug(""detected version: {}"", version);<line30>      LOG.warn(""detected version: {} - currently not supported"", version);"
"<line4>      logger.debug(func.getIdentifier() + "" is not activated"");<line6>    logger.debug(func.getIdentifier() + "" is activated"");<line15>    logger.debug(func.getIdentifier() + "" has a delegation policy"");<line30>      logger.warn(errorMessage);"
"<line12>        log.error(""Error getting projects!"", e);"
"<line4>    SystemUtils.LOG.info(""aggregateTemporal "" + ctx.getIndex() + "" onmaxvar contribution="" + value);"
"<line2>    LOGGER.debug(""dequeueGetPQValuesResponse called with correlation uid {}"", correlationUid);"
"<line35>      logger.warn(""Could not perform search query"", e);"
"<line12>      LOGGER.info(""Generator discovery performed, found [{}]"", generatorMessages);"
"<line14>            LOG.debug(""Bitfinex websocket API version: {}."", version.intValue());<line19>            LOG.error(""Authentication error: {}"", message.get(MESSAGE));<line21>            LOG.info(""Authenticated successfully"");<line31>              LOG.debug(""Register channel {}: {}"", subscriptionUniqueId, channelId);<line32>              LOG.error(e.getMessage());<line43>            LOG.error(""Error with message: "" + message.get(""symbol"") + "" "" + message.get(""msg""));<line46>            LOG.warn(""Already subscribed: "" + message.toString());"
"<line2>      log.debug(""Attempting to acquire leader lease..."");<line13>                  log.debug(""The tryAcquireOrRenew result is {}"", success);<line16>                log.info(""Processing tryAcquireOrRenew successfully canceled"");<line31>      log.error(""LeaderElection acquire loop gets interrupted"", e);"
"<line6>        log.warn(""got multiple titles for document "" + id + "", storing first title only"");"
"<line2>    log.info(""Saving CSV to "" + file.getAbsolutePath());<line8>      log.warn(e.getMessage());<line14>        log.warn(""There was problem closing file stream"", ex);"
"<line17>      LOGGER.error(""Unable to create SimpleFeatureType"", e);"
"<line25>      logger.warn(""Unable to get Processor of type {}; its default properties will be fingerprinted instead""+ "" of being ignored."",className);"
"<line8>    log.info(""{}"", parsed);"
"<line13>      this.logger.info(""Failed to initialize logging using {} or {}"", this.logConfig, defaultLogLocation, e);"
"<line19>        logger.error(""Unexpected lines reached while parsing health check response. Skipping line:- ""+ line);"
"<line1>    LOG.debug(""identifyDuplicateGroupNames"");"
"<line12>      this.logger.warn(""Failed to query patient documents with label [{}] and corresponding external ID [{}]:""+ "" {}"",label,id,ex.getMessage(),ex);"
"<line5>          log.warn(""Timeout during shutdown of async job executor. ""+ ""The current running jobs could not end within ""+ secondsToWaitOnShutdown+ "" seconds after shutdown operation."");<line7>        log.warn(""Interrupted while shutting down the async job executor. "", e);"
"<line4>    log.debug(""numOwners="" + numOwners + "", selfRequests="" + selfRequests + "", config="" + configFile);<line5>      log.info(""Loading JGroups form: ""+ org.jgroups.Version.class.getProtectionDomain().getCodeSource().getLocation());<line5>      log.info(""JGroups version: "" + org.jgroups.Version.printDescription());"
"<line19>        LOG.warn(""Created subscription {} to topic {}.""+ "" Note this subscription WILL NOT be deleted when the pipeline terminates"",subscriptionPath,topic);"
"<line1>    logger.debug(""Starting connection monitor job for thing {} at IP {}"",thingID(),commandConnection.getIP());"
<line39>      logger.debug(sb.toString());
"<line6>      LOG.info(""Skipping attempt to back-fill plugin metadata after 10 failures."");<line9>    LOG.info(""Starting back-fill process(attempt {}) for plugin metadata"", backfillAttempts);<line13>    LOG.debug(""Back-filling plugin metadata for {} namespaces"", namespaces.size());<line21>      LOG.debug(""Back-filling plugin metadata for namespace '{}' with {} applications"",namespace,apps.size());<line25>        LOG.warn(""Failed to write plugin metadata updates for namespace '{}': {}"", namespace, e);<line28>      LOG.info(""Successfully back-filled plugin metadata for {} namespaces."", namespaces.size());"
"<line13>    LOG.info(""Result message: {}"", createResult.getMessage());<line15>    LOG.info(""Transaction done - id={}"", createResult.getTarget().getId());<line20>    LOG.debug(""Transaction submitted for settlement - id={}"", result.getTarget().getId());"
"<line5>        LOGGER.debug(""Restored universe: "" + Thread.currentThread());"
"<line36>          logger.warn(""Failed to generate a seed from SecureRandom due to an InterruptedException."");<line40>          logger.warn(""Failed to generate a seed from SecureRandom within {} seconds. ""+ ""Not enough entropy?"",timeoutSeconds);"
"<line22>        logger.warn(""Could not handle {} for {}"", device.getStatus(), device);"
"<line5>    logger.info(""> Available Process definitions: "" + processDefinitionPage.getTotalItems());<line6>      logger.info(""\t > Process definition: "" + pd);"
"<line15>      LOG.warn(""Unexpected error when accessing file handle limit"", t);"
"<line3>      logger.info(""Executing resource DestroyCommand to evict template from storage pool: ""+ getHumanReadableBytesJson(_gson.toJson(cmd)));<line11>          logger.info(""Destroy template volume "" + vol.getPath());<line18>          logger.info(""Template volume "" + vol.getPath() + "" is not found, no need to delete."");<line23>        logger.warn(""Encounter remote exception to vCenter, invalidate VMware session context"");<line26>      logger.error(msg, e);"
"<line5>      logger.debug(""{}: Sending error message header type: {} transaction: {}"",servConn.getName(),messageType,origMsg.getTransactionId());<line10>      logger.debug(""{}: Sending error message chunk: {}"", servConn.getName(), message);"
"<line7>      logger.debug(""Removed live pub channel from archived media package {}"", mp);"
"<line25>          LOGGER.error(""Could not list resources under representation data directory"", e);<line60>            LOGGER.error(""Could not list resources under AIP"", e);"
"<line20>        LOG.info(""Setting restore state in the FlinkKinesisConsumer. Using the following offsets: {}"",sequenceNumsToRestore);<line22>      LOG.info(""No restore state for FlinkKinesisConsumer."");"
"<line14>        LOGGER.warn(""Spatial filtering for lat/lon is not yet implemented!"");"
"<line3>    LOG.trace(""NM: Container started: "" + containerId);"
"<line2>    LOG.info(""Adding command {}"", command);"
"<line8>      log.error(""Failed to prepare JSON from ImportPersonConfig: '{}'"",oxTrustImportPersonConfiguration,ex);"
"<line1>    logger.debug(""Stopping Call Monitor thread..."");"
"<line2>      logger.debug(""==> RangerKeyStoreProvider.getFromJceks()"");<line15>          logger.info(""Credential keystore password not applied for KMS; clear text password shall be""+ "" applicable"");"
"<line5>        LOGGER.debug(""Waiting for flushing primary index {} to complete..."", selectedIndex);"
"<line11>    logger.debug(""Device notification query requested for device {}"", deviceId);"
"<line8>      LOG.info(null, ""Hooked into "" + plugins + ""!"");"
"<line8>            logger.info(""Retry {} {}"", remainingRetryCount, request);<line14>      logger.debug(""retry fail {}"", e.getCause(), e);"
"<line1>    log.debug(""attaching clean StgG20SclMapping instance"");<line3>      log.debug(""attach successful"");<line4>      log.error(""attach failed"", re);"
"<line1>    log.debug(""passivate connection: {}"", connection);"
"<line5>      LOG.warn(""Admin Dist request body was null"");"
"<line20>      LOG.error(""Cannot find file: {}"", e.getMessage(), e);<line21>      LOG.error(""IO Exception: {}"", e.getMessage(), e);"
"<line4>    LOG.info(""Created queue named: "" + name);"
"<line7>          logger.debug(""Processing of event listener yielded."");<line9>        logger.info(""Processing of event listener could not yield. Executor refused the task."");"
"<line2>    logger.info(""[ testFindObjectsXML01 ]"");<line6>        logger.debug(""http response:\n"" + response);<line11>        logger.debug(ade.getMessage());"
"<line4>    log.info(""Deleting avro files {}"", deletePath);<line8>    log.info(""Moving files with pattern {} to {}"", filter, movePath);<line10>    log.info(""Files moved to {} directory"", movePath);"
"<line8>      logger.error(""Error unmarshalling activity stream info config. xml: {}"", xml, t);"
"<line9>        LOGGER.info(""Connection released"");"
"<line1>    LOG.info(""==> RangerChainedPlugin.init("" + serviceType + "", "" + serviceName + "")"");<line2>    LOG.info(""<== RangerChainedPlugin.init("" + serviceType + "", "" + serviceName + "")"");"
"<line5>      LOG.error(""Message queue overflow, dropping message: "" + e.getMessage());"
"<line1>    LOG.error(""Unable to execute query : "" + exception.toString());"
"<line8>            log.info(""{} {}"", action, path);<line11>            log.info(""E {} {}"", path, e.toString());"
"<line18>    LOG.info(""Create a new firehose for [%d] objects"", objects.size());"
"<line16>    LOGGER.info(""inside4 AddBusinessObjectDataStorageFiles"");"
"<line2>    LOGGER.debug(""Invalidate cache."");"
"<line1>    log.debug(""finding StgMUmsetzStatTxt instance by example"");<line8>      log.debug(""find by example successful, result size: "" + results.size());<line10>      log.error(""find by example failed"", re);"
"<line33>          LOG.debug(""replaceBookie for bookie: {} in ensemble: {} ""+ ""is not adhering to placement policy and chose {}"",oldBookie,ensemble,newBookie);"
"<line5>        logger.debug(""Ignoring non-existent path assuming replay : {}"", path);<line19>        logger.debug(""File {} Uploaded at {}"", keyName, result.getETag());<line21>      logger.debug(""Ignoring non-existent path assuming replay : {}"", outputMetaData.getPath());<line22>      logger.error(""Unable to create Stream: {}"", e.getMessage());"
"<line4>        logger.info(""Missing test input file. mvn test -DcreateInputTestFiles=true ""+ ""or temporarilly: System.setProperty(\""createInputTestFiles\"", \""true\"");"");"
"<line12>              logger.warn(""No property widget for 'num tokens' found!"");"
"<line28>          LOG.error(""Error running system job"", e);"
"<line18>    logger.info(msg + "", "" + count + "" rows, "" + speed(t) + ""K row/sec"");"
"<line10>      LOG.warn(""Trying to scrape prometheus, while it is disabled, set""+ "" \""secor.monitoring.metrics.collector.micrometer.prometheus.enabled\"" to""+ "" \""true\"""");"
"<line6>    Freedomotic.logger.info(""Scene: "" + group.address + "" was renamed to "" + group.name);"
"<line4>    LOGGER.debug(""Got request on /experimentoverview(experimentType={}, matching={}"",experimentType,matchingString);"
<line11>      logger.warn(ex.getMessage());
"<line10>                        log.warn(""No pong from server "" + remoteNodeId + "" - will consider it dead"");"
<line24>      log.error(systemException, systemException);
"<line1>    LOGGER.info(""testOneFail"");<line21>      LOGGER.error(e);"
"<line17>            LOG.debug(""YANG library cleared successfully"");<line21>            LOG.warn(""Unable to clear YANG library"", throwable);"
<line16>      LOG.error(msg, e);<line16>      getListener().warn(msg);
"<line4>      log.debug(String.format(""Calculate checksum with header %s"", header));"
"<line3>      logger.info(""Not starting the API..."");"
"<line24>        log.debug(""Output from Server .... \n"");<line37>      log.error(errMsg, e);<line44>          log.error(errMsg, e);"
"<line5>        LOG.error(""Transaction rollback failed: "" + ex.getLocalizedMessage());<line5>        LOG.debug(""Exception follows."", ex);"
"<line18>        LOG.info(""Plan to choose {} blocks for block deletion, "" + ""actually deleting {} blocks."",blockLimitPerInterval,totalBlocks);<line20>      LOG.warn(""Failed to initiate block deleting tasks, ""+ ""caused by unable to get containers info. ""+ ""Retry in next interval. "",e);<line22>        LOG.debug(""Unexpected error occurs during deleting blocks."", e);"
"<line9>      log.debug(""expected exception: "" + e.getMessage());"
<line5>        log.debug(sqlException, sqlException);
"<line25>      log.warn(""Can't find segments from inputSpec[%s], nothing to do."", ioConfig.getInputSpec());<line29>      log.info(""Generated [%d] compaction task specs"", totalNumSpecs);<line34>          log.info(""Task is asked to stop. Finish as failed."");<line38>            log.info(""Running indexSpec: "" + json);<line41>              log.warn(""Failed to run indexSpec: [%s].\nTrying the next indexSpec."", json);<line44>            log.warn(""indexSpec is not ready: [%s].\nTrying the next indexSpec."", json);<line47>          log.warn(e, ""Failed to run indexSpec: [%s].\nTrying the next indexSpec."", json);<line49>      log.info(""Run [%d] specs, [%d] succeeded, [%d] failed"",totalNumSpecs, totalNumSpecs - failCnt, failCnt);"
"<line7>        LOG.info(""Deleted directory: "" + parent.getAbsolutePath());<line8>        LOG.error(""Failed to delete directory "" + parent.getAbsolutePath() + "": "" + e.getMessage(), e);"
"<line8>      log.info(""Rollover segment [""+ idx+ "" to ""+ res.getSegmentId()+ ""], recordType=""+ (rec == null ? null : rec.type()));"
"<line1>    LOG.info(""Submitting monitor thread !!"");<line11>                LOG.info(""Monitoring thread(s) !!"");<line13>                LOG.error(""Monitor noticed one or more threads failed. Requesting graceful shutdown of""+ "" other threads"",ex);<line15>                LOG.error(""Got interrupted Monitoring threads"", ie);"
"<line8>      LOG.debug(String.format(""==> RangerServiceDefValidator.isValueUnique(%s, %s, %s, %s, %s)"",value, alreadySeen, valueName, collectionName, failures));<line32>      LOG.debug(String.format(""==> RangerServiceDefValidator.isValueUnique(%s, %s, %s, %s, %s): %s"",value, alreadySeen, valueName, collectionName, failures, valid));"
"<line32>      logger.info(leavingMember + "" will be removed from "" + changes);<line33>      logger.info(leavingMember + "" will be removed from "" + leavingGroupIds);"
"<line3>    LOG.info(""Test Dir = {}"", schedulerStoreDir);"
"<line15>      logger.error(""Failed to start application"", e);"
"<line2>    LOGGER.debug(""Forgetting transaction branch {}"", xid);"
"<line2>    this.logger.info(""Getting user info from: "" + path);<line17>      this.logger.info(""Could not fetch user details: "" + ex.getClass() + "", "" + ex.getMessage());"
"<line2>    logger.info(""x"");"
"<line8>      this.logger.debug(""No registered environment, keep default Groovy setup"", e);"
"<line34>    logger.debug(""Fetched group by result from {} of [{}, {}]: {}"",source,curStartTime,curEndTime,results);"
<line4>      LOGGER.error(String.format(Messages.Log.ERROR_WHILE_GETTING_TEMPLATES_S, response.getErrorMessage()));
"<line2>    log.debug(""Reloading..."");"
"<line19>      LOG.error(""init schedule task failed !"");"
<line19>      log.error(systemException, systemException);
"<line3>      this.logger.info(""  Performing bulk migration of %d content entities"", contentRows.size());<line34>        this.logger.error(errmsg);<line36>      this.logger.info(""  Migrated %d content"", count);"
"<line6>      logger.debug(""{}: IAS CIE state is currently {}[{}]"",iasZoneCluster.getZigBeeAddress(),currentStateEnum,currentState);<line7>        logger.debug(""{}: IAS CIE is already enrolled"", iasZoneCluster.getZigBeeAddress());<line10>      logger.debug(""{}: IAS CIE failed to get state"", iasZoneCluster.getZigBeeAddress());<line14>    logger.debug(""{}: IAS CIE address is currently {}"",iasZoneCluster.getZigBeeAddress(),currentIeeeAddress);<line18>        logger.debug(""{}: IAS CIE address is confirmed {}"",iasZoneCluster.getZigBeeAddress(),currentIeeeAddress);<line19>        logger.warn(""{}: IAS CIE address is NOT confirmed {}"",iasZoneCluster.getZigBeeAddress(),currentIeeeAddress);<line24>      logger.debug(""{}: IAS CIE zone ID request failed"", iasZoneCluster.getZigBeeAddress());<line25>      logger.debug(""{}: IAS CIE zone ID is currently {}"", iasZoneCluster.getZigBeeAddress(), currentZone);<line30>      logger.debug(""{}: IAS CIE zone type request failed"", iasZoneCluster.getZigBeeAddress());<line31>      logger.debug(""{}: IAS CIE zone type is 0x{}, {}"",iasZoneCluster.getZigBeeAddress(),String.format(""%04X"", zoneType),ZoneTypeEnum.getByValue(zoneType));"
"<line52>                  LOGGER.debug(""Transport not those allowed: {}"", allowedTransports);<line56>                LOGGER.debug(""Origin check failed for origin {}"", origin);"
<line9>      log.error(e.getMessage());
"<line15>          logger.warn(""Failed to close a socket."", t);"
<line19>        log.debug(exception, exception);
"<line25>          log.debug(""Expected close tag for image "" + _html.substring(curIndex));<line37>          log.warn(""Unable to find image source "" + text);<line47>            log.warn(""Unable to obtain image URL from file entry "" + imageFileEntry.getFileEntryId(),portalException);"
<line6>        log.debug(_NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey);
"<line10>      logger.info(""VSM could not be deleted"");"
"<line2>    logger.debug(""Activating Regex Filter..."");<line8>    logger.debug(""Activating Regex Filter... Done"");"
"<line14>      logger.warn("""", ex);"
"<line35>              LOGGER.debug(""Invalidating flow from caches named {}"", notification.getResourceIdentifier());"
"<line3>    LOGGER.info(""MCRJob is Checked for dead Entries"");<line18>                    LOGGER.debug(""->Resetting too long in queue"");<line22>                    LOGGER.debug(""->ok"");<line42>    LOGGER.info(""MCRJob checking is done"");"
"<line33>            log.debug(""CVE data of bug [""+ _b.getBugId()+ ""] changed, triggering update of local database"");<line36>            log.debug(""CVE data of bug [""+ _b.getBugId()+ ""] did not change, no update of local database needed"");<line39>        log.error(""Cache exception when refreshing CVE data of bug [""+ _b.getBugId()+ ""]: ""+ e.getMessage());<line40>        log.error(""Cannot save bug ["" + _b.getBugId() + ""] with refreshed CVE data: "" + e.getMessage());<line42>      log.debug(""Bug ["" + _b.getBugId() + ""] does not need a refresh of cached CVE data"");"
"<line8>      logger.warn(""Cannot set internet address"", e);"
"<line3>      log.debug(""Deleting topics: {} "", topicsToBeDeleted);<line3>      adminClient.deleteTopics(topicsToBeDeleted).all().get();"
"<line6>      LOG.error(""sync db to tasks error"", e);"
"<line40>          LOG.warn(""Failed to acquire lock for '{}'"", FileUtils.fileName(index.db.getFile()), e);<line41>          LOG.error(""{} in '{}'"", e.getMessage(), FileUtils.fileName(index.db.getFile()), e);"
"<line10>          LOG.trace(""Found binary attribute: ""+ attributeName+ "". Is defined in LDAP config: ""+ getOperationService().isBinaryAttribute(attributeName));<line22>              LOG.trace(""Binary attribute: ""+ attribute.getName()+ "" value (hex): ""+ org.apache.commons.codec.binary.Hex.encodeHexString(attributeValues[i])+ "" value (base64): ""+ attributeValueStrings[i]);"
"<line17>      LOG.error(""Failed to convert the Ipv6 subnetmask from integer to mask value "", e);"
"<line2>    logger.error(""Error occurred during Application Key Mappings discovery"", throwable);"
"<line4>      LOGGER.error(""error trying to deserialize column at index: "" + columnIndex);<line4>      LOGGER.error(""raw value:"" + resultSet.getObject(columnIndex));"
"<line22>      logger.error(""error updating user stats to cloud_usage db"", ex);"
"<line5>    logger.debug(""ajaxChangeDiagnosis"");"
"<line6>      LOGGER.trace(""Versioning updated metacards: {}"", updateResponse.getUpdatedMetacards());<line13>      LOGGER.trace(""No updated metacards applicable to versioning"");<line26>      LOGGER.trace(""Successfully created metacard versions under ids: {}"",response.getCreatedMetacards().stream().map(Metacard::getId).collect(TO_A_STRING));"
"<line4>      log.warn(String.format(""Missing database %s"", database));<line20>        log.error(String.format(""Failure verifying host key entry %s. %s"", entry, e.getMessage()));"
"<line1>    log.debug(""merging MbSchicht instance"");<line3>      log.debug(""merge successful"");<line5>      log.error(""merge failed"", re);"
"<line10>          log.error(""Unrecognized state for table with tableId={}: {}"", tableId, sState);"
"<line2>    log.debug(""Client started"");<line7>    log.debug(""Response:"" + result);<line10>    log.debug(""Client finished"");"
"<line1>    logger.debug(""JDBC::createNewEntryInItemsTable"");"
"<line5>        logger.error(""KeyStore location is empty, cannot find keyStore {} in resources"",KEYSTORE_RESOURCE_KEY);<line12>      logger.debug(""Opening keystream from location: {}"", keyStorePath);<line15>        logger.error(""Could not find keystore at location: {}""+ Paths.get(keyStorePath).toAbsolutePath().toString());"
"<line2>      logger.debug(""Validating FESLPOLICY datastream"");<line3>      logger.debug(""FESLPOLICY datastream is valid"");"
"<line7>      LOGGER.warn(String.format(Locale.ROOT, ""Could not delete %s."", file));"
"<line19>      log.debug(StringBundler.concat(""Adding interest terms \"""",StringUtil.merge(interestTerms),""\"" to asset query for user ID "",userId));"
"<line35>        LOG.info(""{} succeeded"", this);<line42>      LOG.debug(""{} : {} moves failed, start a new iteration"", this, remaining);<line47>    LOG.info(""{} : failed with {} moves"", this, failedMoves);"
"<line5>      LOG.debug(""query = {} {}."", query, folderId);"
"<line8>        log.error(""error executing shutdown hook"", e);"
"<line6>      log.trace(""#{} {} <- {}"", impl.uniqueId(), key, prev + delta);"
"<line17>            LOGGER.info(""Received a SOAP fault on setSchedule"");"
"<line9>            logger.debug(""sleep:{}"", name);<line10>            logger.debug(""messageReceived thread-{} message:"", Thread.currentThread().getName());"
"<line6>      log.error(""Unable to check is back order allowed"", portalException);"
"<line1>    logger.info(""Loading cloud configuration"");<line2>    logger.info(""Cloud Configuration: "" + cloudConfigurationHolder);<line27>    logger.info(""Successfully loaded cloud configuration file from management space"");"
"<line2>      log.debug(""Raising WakeupException in response to user wakeup"");"
"<line5>    LOGGER.warn(""* FINISHED GeoWaveSparkSQLIT        *"");<line5>    LOGGER.warn(""*         "" + stopwatch.getTimeString() + "" elapsed.             *"");<line5>    LOGGER.warn(""*                                       *"");<line5>    LOGGER.warn(""-----------------------------------------"");"
"<line6>      log.error(""failed to redirect request to {}"", redirectTo, e);"
"<line1>    log.info(""starting"");<line3>      log.debug(XmlUtils.marshaltoString(wmlPackage.getMainDocumentPart().getJaxbElement(), true, true));"
"<line3>      logger.debug(""Correcting power state of {} ({}) to off"", deviceType, macAddress);<line6>      logger.debug(""Correcting power state of {} ({}) to on"", deviceType, macAddress);"
"<line6>      log.error(""Problem retrieving all the Vendor names"", e);"
"<line9>        LOGGER.error(""An error occurred while handling a previous error: "" + e2.getMessage(), e2);<line10>      LOGGER.error(""unexpected exception when handling another exception: "" + e.getMessage(), e);"
"<line2>    LOG.info(""Slot with allocationId {} already exist, with resource profile {}, job id {} and index {}.""+ "" The required index is {}."",taskSlot.getAllocationId(),taskSlot.getResourceProfile(),taskSlot.getJobId(),taskSlot.getIndex(),index);"
"<line3>        logger.debug(""Request matched by universal pattern '/**'"");"
"<line3>      log.debug(""Meta changed from {} to {}"", original.getMetadata(), addressSpace.getMetadata());<line6>      log.debug(""Spec changed from {} to {}"", original.getSpec(), addressSpace.getSpec());<line9>      log.debug(""Status changed from {} to {}"", original.getStatus(), addressSpace.getStatus());"
"<line6>          logger.error(""Configured application directory ""+ applicationDirectoryPath+ "" is not a directory"");<line9>        logger.error(""Configured application directory "" + applicationDirectoryPath + "" is not valid"", e);<line17>    logger.info(""Application directory: {}"", applicationRoot);<line24>    logger.info(""Application initialized."");"
<line21>      log.error(systemException, systemException);
"<line6>      log.error(""No message object of type org.projectforge.mail.Mail given. E-Mail not sent."");<line10>      log.error(""No to address given. Sending of mail cancelled: "" + composedMessage.toString());<line15>      log.error(""No e-mail host configured. E-Mail not sent: "" + composedMessage.toString());"
"<line19>      log.warn(""Got an invalid SAML Single Logout response (XML is broken)"", e);"
<line4>          Version.fromString(client.info(RequestOptions.DEFAULT).getVersion().getNumber());<line11>      LOGGER.warn(esVersionCompatibilityWarn);<line17>      LOGGER.error(VERSION_CHECKING_ERROR_MESSAGE, e);
"<line8>      logger.error(""Error loading groups"", t);"
"<line2>      log.debug(""Synchronizing the project routers"");<line6>      log.error(""Error while handling synchronizing projects during topology update event."", e);"
"<line2>    logger.debug(""getInterDirectPingMeasurementFromQoSMonitor started..."");"
"<line13>      LOGGER.warn(""Found plugin {} for executable plugin type {} that is not itself executable."",uncastResult.getId(),uncastResult.getPluginType());"
"<line3>    LOG.info(""Bean: Other -> XOrder"");"
"<line3>    logger.info(""jsonResponse: "" + jsonResponse);<line4>    logger.info(""wordsJSONArray.length(): "" + wordsJSONArray.length());"
"<line2>    logger.debug(""About to send POST request to '{}' with payload '{}'"", uri, body);"
"<line6>      log.error(""pin definition null"");"
"<line8>              logger.debug(String.format(""Removing property: '%s' from scope: '%s'"", matchedValue, getScopeName()));<line17>        logger.info(""Key expression return null, no property will be removed"");"
"<line3>    LOG.info(""Repopulating the deposit queue, {} items in backlog"", depositStatuses.size());<line12>            LOG.info(""Skipping resumption of deposit {} because it already is in the queue"", uuid);<line27>            LOG.debug(""Skipping resumption of queued deposit {} because it already is in the queue"",uuid);"
"<line3>    logger.debug(""Handling: "" + message);"
"<line2>      logger.error(""Cannot publish the failure event as analytics publisher is null."");<line11>      logger.debug(""Analytics event for failure event is published."");<line12>      logger.error(""Error while publishing the analytics event. "", e);"
<line6>      logger.error(e.getMessage(), e);
"<line24>          logger.debug(String.format(""file MD5 changed, src[%s, md5:%s] dest[%s, md5, %s]"",sourceFilePath, srcMd5, destFilePath, destMd5));"
"<line5>    LOGGER.debug(""check exist accessContract={}"", accessContractDto);"
<line12>      log.error(e.getMessage());
<line3>    logger.trace(marker, msg);
"<line6>      LOGGER.error(""Failed to create "" + Conf.class.getPackage().getName() + "" for "" + loaderClass.getName(),e);"
"<line22>      logger.info(""Using distributed configuration management for {}"", component);<line24>    logger.info(""Using standalone configuration management for {}"", component);"
"<line11>      LOGGER.debug(""ExperimentRun parentId updated successfully"");"
"<line2>    LOG.debug(""  nodeCreationType {} getNodeType {} isContainerNode {} "",nodeCreationType,nodeOriginalData.getNodeType(),nodeOriginalData.isContainerNode());<line29>      LOG.error("" Error while parsing dynamic response template"", e);"
"<line10>          logger.debug(""got all link records."");<line17>      logger.debug(""bad field handling link records {}"", e.getMessage());<line18>      logger.debug(""got IO exception handling link records {}"", e.getMessage());<line19>      logger.debug(""got exception requesting link records {}"", e.getMessage());<line20>      logger.warn(""invalid message "", e);"
"<line2>    logger.debug(""Reloading DataModel from folder ""+ store.getReadableResourcePath(ResourceStore.DATA_MODEL_DESC_RESOURCE_ROOT));<line10>        logger.error(""Error to load DataModel at "" + path, e);<line13>    logger.debug(""Loaded "" + dataModelDescMap.size() + "" DataModel(s)"");"
"<line8>      log.info(""Retrieved {} file_data rows for address book processing"", fileDataList.size());<line17>    log.info(""Processed {} historic address books"", fileDataEntries);"
"<line20>            log.warn(this,""JsHelper eval Exception: script=""+ script+ "", StrategyDesc=""+ stra.getDesc()+ "", StrategyName=""+ stra.getName());<line30>      log.debug(this, ""Judgement: FireSize="" + map.size() + "", conditions="" + JSONHelper.toString(crs));"
"<line4>      logger.info(""Using instance ID {} from setting {}"", result, SETTING_SOURCE_INSTANCE);<line6>        logger.info(""Detect instance set to AWS, trying to determine AWS instance ID"");<line10>          logger.info(""Unable to detect AWS instance ID for this machine, sending metrics without an""+ "" instance ID"");<line12>        logger.info(""Detect instance set to GCE, trying to determine GCE instance ID"");<line18>          logger.info(""Unable to detect GCE instance ID for this machine, sending metrics without an""+ "" instance ID"");<line21>        logger.info(""No source instance ID passed, and not set to detect, sending metrics without an""+ "" instance ID"");<line23>    logger.info(""Detected instance ID as {}"", result);"
"<line1>    logger.info(""Checking the "" + this.action + "" request"");"
<line11>          log.debug(_NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey);
"<line8>      LOG.error(""Error while serializing object for dnd"", e);"
"<line31>      logger.error(""{} Failed to process data due to {}"", new Object[] {this, e});<line32>        logger.error("""", e);"
"<line10>        LOG.info(""Updated {} entity: name={}, guid={} "",ret.getEntity().getTypeName(),ret.getEntity().getAttribute(ATTRIBUTE_QUALIFIED_NAME),ret.getEntity().getGuid());<line11>        LOG.info(""Entity: name={} "",entity.toString() + "" not updated as it is unchanged from what is in Atlas"");<line14>      LOG.info(""Entity: name={} "",entity.toString() + "" not updated as it is unchanged from what is in Atlas"");"
<line27>      log.error(systemException, systemException);
"<line1>    log.info(""getAttendeeRequests: "" + user.getName());<line9>    log.info(""getAttendeeRequests: found requests: "" + list.size());"
"<line2>    log.debug(""Recovering upload, object-id: {}"", objectId);"
"<line4>        logger.debug(""event=deleting_publish_job_from_cqs message_id=""+ message.getMessageId()+ "" queue_url=""+ queueUrl+ "" receipt_handle=""+ receiptHandle);<line5>        logger.error(""event=failed_to_kill_message"", ex);"
"<line29>              logger.error(""Failed to parse PgArray: "" + pgArray, e);"
"<line2>    LOGGER.trace(""Velocity engine initializing..."");<line4>    LOGGER.debug(""Setting Velocity runtime log: {}"", velocityLog);<line6>    LOGGER.debug(""Using Velocity configuration from {}"", propertiesUrl);<line10>    LOGGER.trace(""Velocity engine initialized."");"
"<line8>          logger.debug(""æ§è¡è®¾ç½®:response.setCharacterEncoding(\"""" + encoding + ""\"""");"
"<line2>      LOG.debug(""Stopping tracking of resources for job {}."", jobId);"
"<line9>      LOGGER.error(""I/O error while access the starting file of derivate {}!"", derivateId, e);<line10>      LOGGER.error(""No access to starting file of derivate {}!"", derivateId, s);"
"<line15>          logger.debug(""Loading batch of ""+ records.count()+ "" records: ""+ ""process=""+ process+ "", align=""+ align);<line17>          logger.debug(""Delivering batch of "" + batch.size() + "" updates"");<line20>          logger.error(""Failed to delivery updates"", e);<line32>        logger.error(exception.getMessage(), e);"
"<line13>      LOG.warn(""output directory "" + convertedModelDir + "" already exists"");"
"<line3>      LOGGER.debug(""Schedule execute solution change with previous chBestSolution: {}"", chBestSolution);"
"<line3>      log.debug(""Shutting down scheduler thread pool"");"
"<line3>      Log.debug(""Skipping permissions admin on Experiment object "" + swAccession);<line8>      Log.debug(""Checking permissions for experiment object "" + swAccession);<line9>      Log.debug(""Skipping permissions for experiment object "" + swAccession + "" , checked before"");<line18>        LOGGER.warn(""Modifying Orphan Experiment: "" + this.getName());<line20>        LOGGER.warn(""Experiment has no owner! Modifying Orphan Experiment: "" + this.getName());<line22>        LOGGER.warn(""Not modifying Orphan Experiment: "" + this.getName());<line26>      LOGGER.info(""Experiment does not give permission"");"
"<line6>      LOG.debug(String.format(""acquire permits: %s, maxPremits: %s"", numOps, maxPermits));"
"<line11>        LOG.warn(""You can not run delete command while active backup session is in progress. \n""+ ""If there is no active backup session running, run backup repair utility to ""+ ""restore \nbackup system integrity."");<line15>        LOG.warn(""Failed backup session found. Run backup repair tool first."");<line21>        LOG.warn(""Backup system table snapshot exists"");<line45>            LOG.error(""Delete operation failed, please run backup repair utility to restore ""+ ""backup system integrity"",e);<line47>            LOG.warn(""Delete operation succeeded, there were some errors: "", e);"
"<line7>    logger.info(""guestId=""+ updateInfo.getGuestId()+ "" connector=fitbit action=loadWeightDataForOneDay json=""+ json);"
<line16>      LOGGER.error(Constants.ERROR_UNIQUEHOST, e);
"<line18>      LOGGER.warn(""The variable '{}' appears more than once: {}."",duplicatedVariableId,duplicatedVariables);"
<line13>        log.debug(principalException, principalException);
"<line2>    log.error(""the default fallback for hystrix"");<line2>    return DefaultShenyuEntity.error(ShenyuResultEnum.HYSTRIX_PLUGIN_FALLBACK.getCode(),ShenyuResultEnum.HYSTRIX_PLUGIN_FALLBACK.getMsg(),null);"
"<line9>      LOGGER.error(""Cannot create policy rule assignment: {}"", e.getMessage(), e);<line9>      getSession().error(""Cannot create policyRule assignment."");"
"<line2>    log.debug(""Building Process for command: {}"", () -> String.join("" "", processBuilder.command()));"
<line14>      log.error(systemException, systemException);
"<line9>        logger.debug(""Updating basic zone id="" + zoneId + "" with correct nic count"");<line32>      logger.debug(""Basic zones are updated with correct nic counts successfully"");"
"<line14>            LOGGER.warn(""Fixed MD5 of {} to {}"", path, md5);"
"<line28>    log.info(String.format(""Produced %s recommendations for MetroPoint placement."",rpProtectionRecommendaton.getResourceCount()));"
"<line3>      logger.trace(LogMarker.SERIALIZER_VERBOSE, ""Writing File {}"", file);"
<line8>      log.warn(e.getMessage(), e);
"<line5>      Log.debug(""blocking TM request until outstanding request returns"");"
"<line3>    logger.debug(""Spotify auth callback servlet received GET request {}."", req.getRequestURI());"
"<line7>    logger.info(""RetryableHttpConnection: endpoint: %s, data.len: %d, method: %s"",endpoint, data == null ? -1 : data.length(), method);<line12>        logger.info(""connecting to %s:%s ..."", ips.get(index), ports.get(index));<line35>              logger.info(""succeed to connect to %s:%s!"", ips.get(index), ports.get(index));<line37>            logger.error(""code: %d, msg: %s"", code, conn.getResponseMessage());<line42>            logger.error(""Error when connecting to %s:%s:\n%s"", ips.get(index), ports.get(index), e);<line45>        logger.error(e);<line47>    logger.error(""Unable to connect to all IPs"");"
"<line7>            log.error(t, ""Unexpected exception while cleaning up expired transactions"");"
"<line2>      log.debug(""A new run info set "" + runInfo);"
"<line39>        LOG.debug(""compiling stylesheet {}"", stylesheet);"
"<line11>      log.info(String.format(""Redirecting to bundled app: %s"", redirectPath));<line21>    log.debug(String.format(""App page name: '%s'"", pageName));<line26>        log.debug(String.format(""Manifest context path: '%s'"", contextPath));<line36>      log.debug(String.format(""App filename: '%s'"", filename));"
"<line5>      LOG.info(""Lookup join cache has expired after {} minute(s), reloading"",reloadInterval.toMinutes());<line6>      LOG.info(""Populating lookup join cache"");<line23>        LOG.info(""Loaded {} row(s) into lookup join cache"", count);<line31>        LOG.warn(String.format(""Failed to load table into cache, will retry in %d seconds"", toSleep / 1000),e);<line34>          LOG.warn(""Interrupted while waiting to retry failed cache load, aborting"");"
"<line4>      LOGGER.debug(""MANAGER["" + name + ""] SESSION["" + branchSession + ""] "" + LogOperation.BRANCH_REMOVE);"
"<line6>      log.info(""*No namedcls directory so skip it: namedclsPath="" + namedclsPath);<line18>      log.info(""*No namedcls dfprop file in the directory so skip it: "" + namedclsPath);"
"<line5>      log.debug(""User connection configuration file not found"");<line7>    log.info(""Using connection configuration file at "" + fileLocation);"
"<line17>    LOGGER.info(""WFS filter GetFeature response:\n"" + prettyString(doc));"
"<line1>    log.debug("""");"
"<line16>    log.info(""finished processing for id "" + documentId + "" in "" + (processingTime / 1000) + "" secs"");"
"<line6>      log.warn(""Rollback failed"", e);"
"<line1>    log.trace(""Running command on the shell: {}"", Arrays.toString(command));<line2>    log.trace(""Result:"" + result);"
<line25>      log.error(systemException, systemException);
"<line7>      LOG.error(""Missing input parameters"");"
"<line2>      log.warn(""Temporal column type of table ID %s set incorrectly to %s"", tableId, type);"
<line2>    logger.info(infoMsg);<line2>    EventLogger.logger.info(infoMsg);
"<line1>    log.debug(""findAllTemplates()"");"
"<line13>      logger.error(""Failed to fetch container ids "", e);"
"<line2>    LOGGER.info(""Testing capabilities of StringImageInputStreamSpi"");<line33>    LOGGER.info(""Testing capabilities of URLImageInputStreamSpi: SUCCESS!!!"");"
"<line6>    LOG.info(""start: scheduler started"");"
"<line2>    log.info(""visitEnter()"");<line5>    log.debug(""checking control rod"");<line7>      log.info(""aborted!"");<line9>    log.info(""obtaining metadata for:{}"", node);<line13>      log.info(""pushed metadata in the stack...now filter and then delegate to visitEnterWithMetadata()""+ "" in the impl class to make any determinations"");<line14>        log.info(""not indexable by filter, short circuit"");<line19>      log.error(""error in obtaining metadata"", e);"
"<line5>      LOG.warn(""Sanity check failed for entry size of ""+ entrySize+ "" at location ""+ pos+ "" in ""+ entryLogId);<line7>      LOG.error(""Read invalid entry length {}"", entrySize);"
"<line2>    LOGGER.debug(() -> ""using prefetched list: "" + list);"
"<line7>    log.debug(""User "" + personDao.getLoggedPerson().getEmail() + "" retrieved list of people."");"
"<line5>      LOG.debug(""{}: Creating read view for subpartition {} of partition {}."",parent.getOwningTaskName(),getSubPartitionIndex(),parent.getPartitionId());"
"<line3>      logger.info(""Required a selected node"");<line7>      logger.info(""Home category not deletable"");<line11>      logger.info(""Category with children not deletable"");"
"<line21>          logger.warn(""Accessing MemoryPool '{}' threw an Internal Error: {}"",mp.getName(),ie.getMessage());"
"<line20>              logger.info(""Total item read {} from {} is less than {}, retrying reads"",totalItemRead,this.client.getReadEndpoint(),expectedNumberOfDocuments);<line23>                logger.info(""interrupted"");<line27>              logger.info(""Read {} items from {}"", totalItemRead, this.client.getReadEndpoint());"
"<line12>        logger.error(""No repository info returned"");<line18>      logger.error(msg, th);"
"<line23>      LOG.error(""BodyTrackController.getFluxtreamCapturePhoto(): Exception while trying to check""+ "" authorization."",e);<line31>        LOG.error(""BodyTrackController.getFluxtreamCapturePhoto(): "" + message, e);<line40>        LOG.error(""BodyTrackController.getFluxtreamCapturePhoto(): "" + message);<line55>        LOG.warn(""NoSuchAlgorithmException caught while trying to create an MD5 hash for photo [""+ photo.getIdentifier()+ ""].  No Etag will be specified in the response."");"
"<line8>        logger.info(""Event index does not support denial via ACL, ignoring {}"", entry);"
"<line9>        log.error(e, ""Task failed"");"
"<line12>          log.debug(""Found ""+ Iterables.size(resultL)+ "" matches in catalog for type ""+ typeName+ ""; returning the result with preferred version, ""+ resultI);<line17>            log.warn(""Unable to find catalog item for type ""+ typeName+ "". There is an existing catalog item with ID ""+ resultI.getId()+ "" but it doesn't define a class type."");"
<line10>      logger.error(e.getLocalizedMessage(), e);
"<line18>      LOG.debug(""SoftLayer VLANs private {} and public {} already configured in template options for""+ "" scope: {}"",new Object[] {privateVlanId, publicVlanId, scopeUid});<line31>        LOG.debug(""Creating new latch for scope: {}"", scopeUid);<line38>      LOG.debug(""Waiting for VLAN details for scope: {}"", scopeUid);<line48>    LOG.debug(""Looking up saved VLAN details {}"", scopeUid);<line55>      LOG.warn(message);"
<line28>      log.error(systemException, systemException);
"<line2>    LOGGER.debug(""SerializedPublicKexLength: "" + msg.getPublicKeyLength().getValue());"
"<line7>    logger.debug("">> canceling service for guest(%s) billingItem(%s)"", id, guest.getBillingItemId());"
<line2>    logger.debug(msg, thrown);
"<line10>      log.trace(""Append record id = "" + id + "" recordType = "" + recordType);"
"<line6>          logger.info(""Utente non abilitato al salvataggio del contenuto "" + currentContent.getId());<line25>        logger.info(""Salvato contenuto ""+ currentContent.getId()+ "" - Descrizione: '""+ currentContent.getDescription()+ ""' - Utente: ""+ this.getCurrentUser().getUsername());<line26>        logger.error(""Tentativo Salvataggio/approvazione contenuto NULLO - Utente: ""+ this.getCurrentUser().getUsername());<line28>      logger.error(""Error extracting saving content"", e);"
"<line3>    logger.warn(""component=metadata action=setTimeZone message=attempt to set timezone"");"
"<line3>    log.info(""{0}: Beginning Optimization #{1}"", logCacheName, timesOptimized);<line23>        log.error(""{0}: Error optimizing queued puts."", logCacheName, e);<line34>    log.info(""{0}: Finished #{1}, Optimization took {2}"",logCacheName, timesOptimized, timer.getElapsedTimeString());"
"<line40>    logger.info(""listening on port: ""+ strPort+ "" for trace data ""+ ""(Jaeger Protobuf format over gRPC)"");"
"<line26>      logger.info(""detach disks from volume-wrapper VM "" + vmName);"
"<line2>    logger.debug(""Removing all metadata for item {}"", name);"
"<line1>    LOGGER.info(""Loading Adaptors..."");<line3>      LOGGER.warn(""WARN: COMPSS_HOME not defined, no adaptors loaded."");<line8>      LOGGER.warn(""WARN_MSG = [Adaptors folder not defined, no adaptors loaded.]"");"
"<line5>      log.debug(""Access denied"", e);"
<line2>    LOGGER.info(TestcontainersConfiguration.getInstance().toString());
"<line3>      log.warn(""[{}] Dispatcher is already closed. Closing consumer {}"", name, consumer);<line16>      log.warn(""[{}] Attempting to add consumer to subscription which reached max consumers limit"",name);"
"<line3>      LOG.debug(""PortletRequestDispatcher requested for path: ""+ path+ "" at context: ""+ app.getContextPath());<line6>        LOG.info(""Failed to retrieve PortletRequestDispatcher: path name must begin with a slash '/'."");<line17>          LOG.info(""No matching request dispatcher found for: "" + path);<line21>        LOG.info(""Failed to retrieve PortletRequestDispatcher: "" + ex.getMessage());"
"<line8>      logger.debug(""Scanner (scan now) failed on server instance {} due to {}"",container.getUrl(),response.getMsg());"
"<line2>    logger.info(name.getMethodName() + "" - callback - got broadcast"");<line3>      logger.info(name.getMethodName() + "" - callback - invalid content"");<line5>      logger.info(name.getMethodName() + "" - callback - content OK"");"
"<line15>      logger.warn(""Unauthorized resource set request from bad user; expected ""+ rs.getOwner()+ "" got ""+ auth.getName());<line20>      logger.warn(""Tried to add a policy with a non-null ID: "" + p.getId());<line25>        logger.warn(""Tried to add a policy with a non-null claim ID: "" + claim.getId());<line38>      logger.warn(""Unexpected result trying to add a new policy object: "" + newPolicies);"
"<line11>      logger.warn(""createHandler({}) failed: ThingHandler not found for {}."",thingTypeUID,thing.getLabel());"
<line55>      log.error(systemException, systemException);
"<line13>      logger.info(""ExecutionError {} {}"", name, e.getMessage());<line14>      logger.info(""Timeout {} {}"", name, e.getMessage());"
"<line12>          logger.error(""Cannot open transport for client {}"", node, e);"
"<line4>      LOG.warn(""bulk/flag configuration being made to {} after deployment: may not be supported in""+ "" future versions ({})"",new Object[] {this, flags});<line46>      LOG.warn(""Unsupported flags when configuring {}; storing: {}"", this, flags);"
"<line5>    log.info(""Generating tests"");"
"<line2>      logger.warn(new IllegalThreadStateException(""connectionordered channel handler `queue size: ""+ connectionExecutor.getQueue().size()+ "" exceed the warning limit number :""+ queuewarninglimit));"
"<line5>      log.error(""Exception while executing runnable "" + task, t);"
"<line3>      log.debug(""adding a user:{}"", user);<line9>    log.debug(""executing admin PI"");<line14>      log.warn(""no more rules exception caught, will throw as duplicate data for backwards""+ "" compatibility"",nmr);<line17>    log.debug(""user added, now process other fields"");<line18>      log.info(""comment has changed"");<line21>      log.info(""info has changed"");"
"<line13>        LOG.info(""{} {} <{}>"", direction, messageType, clientID);<line16>        LOG.info(""{} SUBSCRIBE <{}> to topics {}"",direction,clientID,subscribe.payload().topicSubscriptions());<line19>        LOG.info(""{} UNSUBSCRIBE <{}> to topics <{}>"",direction,clientID,unsubscribe.payload().topics());<line22>        LOG.info(""{} PUBLISH <{}> to topics <{}>"",direction,clientID,publish.variableHeader().topicName());<line28>        LOG.info(""{} {} <{}> packetID <{}>"", direction, messageType, clientID, messageId(msg));<line32>        LOG.info(""{} SUBACK <{}> packetID <{}>, grantedQoses {}"",direction,clientID,messageId(msg),grantedQoSLevels);"
"<line9>      LOG.warn(""Could not read attribute {}."", attributeName, e);"
"<line1>    LOGGER.debug(""Loading context (transfer), uri: {}"", cmd.getCurrentSession().getContext().getUri());<line6>      LOGGER.error(""Failed loading context!"", ex);<line8>      LOGGER.error(""Cannot play context!"", ex);"
"<line12>        log.warn(""Duplicate MassTruncate RestException type: "" + alias);"
"<line2>    LOG.info(""Initializing ..."");<line6>      LOG.error(""Can not load rules from database:\n"" + e.getMessage());<line10>    LOG.info(""Initialized. Totally "" + rules.size() + "" rules loaded from DataBase."");<line12>        LOG.debug(""\t"" + info);"
"<line4>      LOG.debug(""skipping bb - too few data for header: {}"", bb.readableBytes());<line8>      LOG.debug(""skipping bb - too few data for msg: {} < {}"", bb.readableBytes(), length);<line10>    LOG.debug(""OF Protocol message received, type:{}"", bb.getByte(bb.readerIndex() + 1));"
"<line4>      log.info(""adding {}"", type.getSimpleName());"
<line5>      logger.error(e.getMessage(), e);
"<line9>    logger.info(""Executing copyTemplateFromSecondaryToPrimary. secondaryStorage: ""+ secondaryStorageUrl+ "", templatePathAtSecondaryStorage: ""+ templatePathAtSecondaryStorage+ "", templateName: ""+ templateName);<line10>    logger.info(""Secondary storage mount point: "" + secondaryMountPoint);<line23>      logger.info(""Executing command: "" + command.toString());<line26>        logger.error(msg);<line61>      logger.error(msg);"
"<line6>      logger.info(""Altered cloud_usage.cloud_usage table and added column quota_calculated"");<line8>        logger.warn(""cloud_usage.cloud_usage table already has a column called quota_calculated"");"
"<line12>        LOGGER.warn(""Ignoring unparsable DDL statement '{}': {}"", ddlText, e);<line35>                      LOGGER.info(""Skipped DDL event type {}: {}"", event.type(), ddlText);"
"<line2>    logger.debug(""WeMoMakerHandler disposed."");"
"<line5>      log.trace(""Failed to reply due to error"", e);"
"<line5>    log.trace(""Loaded settings from {}."", systemId);"
"<line6>      log.debug(""Link release ["" + releaseId + ""] to project ["" + projectId + ""]"");<line18>      log.error(""Cannot link release ["" + releaseId + ""] to project ["" + projectId + ""]."");"
"<line17>      logger.warn(""Requested torrent hash was: {}"", announceRequest.getHexInfoHash());"
"<line4>      logger.error(""VM status updator is interrupted."");"
"<line27>          logger.warn(""Failed to delete temp file, marked for deletion when VM closes ""+ oldFile.getAbsolutePath());<line30>        logger.warn(""Cannot delete temp file as it no longer exists "" + oldFile.getAbsolutePath());"
"<line15>        log.trace(""SP of entity "" + entityId + "" doesn't support SAML2 - ignoring."");<line19>        log.trace(""SP of entity "" + entityId + "" is hidden from discovery - ignoring."");<line27>          log.error(""Adding remote SPs certs to local certs store failed, "" + ""skipping IdP: "" + entityId,e);<line30>        log.info(""No signing certificate found for SP, skipping it as ""+ ""the 'strict' trust model is used: ""+ entityId);"
<line6>        LOG.debug(qname.toString());
"<line30>          log.error(""Unable to install "" + file, invocationTargetException);"
"<line1>    log.debug("" Worlflow Detail ====  Inside WorkflowRetryExecutor retryWorkflowWithCompletedTask "");<line10>      log.debug("" Worlflow Detail  ==== Inside WorkflowRetryExecutor retryWorkflowWithCompletedTask Retry""+ "" flow workflowHistory {}  "",lastCompletedTaskExecution);<line13>        log.debug("" Worlflow Detail  ====  workflow failed to retry and will be picked up in next retry""+ "" schedule  {} "",lastCompletedTaskExecution);"
<line3>        logger.trace(marker, message, exception);<line5>        logger.debug(marker, message, exception);<line7>        logger.info(marker, message, exception);<line9>        logger.warn(marker, message, exception);<line11>        logger.error(marker, message, exception);
"<line5>    LOGGER.info(""Login statut: {} / user: {} - {} / surrogate: {} / IP: {} / errorMessage: {}"",errorMessage != null ? StatusCode.KO : StatusCode.OK,user.getIdentifier(),user.getEmail(),surrogateIdentifier,ip,errorMessage);"
"<line4>      log.debug(""Checking child metadata provider for entity descriptor with entity ID: {}"", entityID);<line10>        log.warn(""Error retrieving metadata from provider of type {}, proceeding to next provider"",provider.getClass().getName(),e);"
"<line20>            logger.info(""Filebeat server on port "" + port + "" shut down"");<line32>    logger.info(""listening on port: "" + port + "" for Filebeat logs"");"
"<line10>      LOGGER.debug(""Failed to parse mimetype: "" + mimeTypeString, e);<line16>      LOGGER.debug(""Failed to create BinaryContent"", e);"
"<line2>      log.info((sending ? ""Sending "" : ""Received "")+ (message.isRequest() ? ""Request: "" : ""Answer: "")+ message.getCommandCode()+ ""\nE2E:""+ message.getEndToEndIdentifier()+ ""\nHBH:""+ message.getHopByHopIdentifier()+ ""\nAppID:""+ message.getApplicationId());<line2>      log.info(""Request AVPS: \n"");"
"<line13>            log.error(""Could not find profile ""+ profileRef.getName()+ "" of user ""+ getAuthService().getUsername());"
<line19>      log.error(systemException, systemException);
"<line2>    log.debug(""Claz: "" + claz.getName() + "" method: "" + method, e);"
<line1>    LOG.warn(FIXED_REASON, what, reason);
"<line40>      LOGGER.info(""Starting CC in "" + ccRoot.getAbsolutePath());"
"<line6>        LOGGER.error(""handle global session failed: {}"", globalSession.getXid(), th);"
"<line3>    LOG.debug(""Begin processPatientDiscoveryAsyncReq"");<line27>    LOG.debug(""End processPatientDiscoveryAsyncReq"");"
"<line12>            log.debug(""got connection 1st tme"");<line18>                    log.debug(""got connection 2nd time"");<line26>                    log.info(result2.cause());<line32>            log.debug(""didn't get a connection 2nd time yet"");<line38>                  log.debug(""got a connection 2nd time"");<line42>            log.info(result.cause());"
"<line2>    log.info(""listZoneNames()"");<line16>      log.error(CollectionListingUtils.QUERY_EXCEPTION_FOR_QUERY, e);<line23>      log.info(""got zone:{}"", zone);"
"<line10>      log.warn(""Missing key CustomPluginSettings"");<line16>        log.warn(""Missing key MountFSClassName"");<line46>        log.warn(String.format(""Unable to determine protocol for %s"", identifier));"
"<line10>    LOG.trace(""path for key: {} is: {}"", key, path);"
"<line33>      logger.error(""Error during prepare composite key, Caused by {}."", e);<line35>      logger.error(e.getMessage());"
"<line3>    LOG.debug(""get repair_schedule called with: id = {}"", repairScheduleId);"
"<line15>    logger.warn(""Unknown airport code: "" + code);"
"<line4>      logger.error(""meet error when doing start checking"", e);"
"<line3>      log.info(""setting up..."");"
"<line24>        LOGGER.warn(""Unable to find data ID '""+ StringUtils.stringFromBinary(dataId)+ "" (hex:""+ ByteArrayUtils.getHexString(dataId)+ "")' with adapter ID ""+ adapterId+ "" in data table"");<line26>      LOGGER.warn(""Unable to close reader"", e);"
"<line5>    LOGGER.debug(""overlaps:\n{}"", output);"
"<line1>    LOG.debug(""TPnodeEquipments"");"
"<line3>      LOGGER.trace(""Completing ConfigObservable for termination."");"
<line9>      log.error(e.getMessage(), e);
"<line6>      LOG.info(""Container {} has {} replicas on {}"",containerID,replicas.size(),replicas.stream().map(ContainerReplica::getDatanodeDetails).map(DatanodeDetails::getUuidString).sorted().collect(toList()));<line8>      LOG.warn(""Container {} not found"", containerID);"
"<line45>      log.error(""Could not read input stream from request"", e);"
"<line11>              log.warn(""Parent edge from child {} to parent {} does not have a branch uuid. Skipping""+ "" this edge."",vertex.getProperty(""uuid""),branchUuid);"
"<line4>      logger.warn(""exception occurred during releasing thrift client"", e);"
"<line8>      LOG.debug(""Identifier [{}] deleted. It took [{}]ms."",new Object[] {identifier, (System.currentTimeMillis() - start)});"
"<line4>    LOGGER.info(""AMQP.BasicProperties: {}"", properties);"
"<line4>      LOG.error(""Error while waiting for extractor job to be finished"", e);"
<line19>          LOGGER.error(e, e);<line24>          LOGGER.debug(e, e);<line26>        LOGGER.error(e, e);<line28>      LOGGER.error(tracker.getException(), tracker.getException());
"<line7>      logger.debug(""tracked modems: {}"", modems.keySet());<line8>        logger.debug(""processing modem {}"", e.getKey());<line13>      logger.error(""Unexpected error during monitoring"", ex);"
"<line4>            log.warn(""Failed to close the pool"", h.cause());<line5>          log.debug(""SMTP connection pool closed."");"
"<line3>    LOGGER.info(""[DeleteFileRequest] Notify data delete "" + this.loc.getPath() + "" to DIP..."");<line9>          LOGGER.info(""[DeleteFileRequest] File "" + filePath + "" deleted."");<line10>          LOGGER.error(""[DeleteFileRequest] Error on deleting file "" + filePath);<line13>      LOGGER.info(""[DeleteFileRequest] Deleting Data in Task Analyser"");"
"<line2>    logger.info(""python task params {}"", taskExecutionContext.getTaskParams());"
<line13>      log.warn(message, o1, o2, o3, o4, o5, o6, o7, t);<line14>      log.warn(message, o1, o2, o3, o4, o5, o6, o7);
"<line2>    log.info(""Starting mysql server. Docker image: {}"", properties.dockerImage);"
"<line5>      logger.debug(""Stopped polling: {}"", stopped);"
"<line3>      logger.debug(""Detect meters from #{} objects"", telegram.getCosemObjects().size());"
"<line12>      LOG.error(""Error while initializing CoordinateOperations"", e);"
"<line6>      LOG.warn(""Could not create a SASL client with valid Kerberos credential"");"
"<line4>      LOG.warn(""{} couldn't be stopped gracefully"", getClass().getSimpleName());"
"<line1>    log.debug(""deleting StgG20Sys instance"");<line3>      log.debug(""delete successful"");<line4>      log.error(""delete failed"", re);"
"<line24>        logger.trace(""[BEGIN] "" + proxy.getClass() + "" : "" + method.getName());<line27>        logger.trace(""[END] "" + proxy.getClass() + "" : "" + method.getName());<line47>          logger.info(builder.toString());<line51>          logger.info(""[ERROR] "" + proxy.getClass() + "" : "" + method.getName() + "" : "" + e.getMessage());<line52>            logger.info(""  <Cause>"", e.getCause());"
<line10>          log.debug(_NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey);
"<line3>    LOG.debug(""Found config value {} for key {} "" + ""from YAML configuration."", result, option);"
"<line30>                LOG.info(""Updated ZK to point ledger fragments"" + "" from old bookies to new bookies: {}"",oldBookie2NewBookie);<line32>                LOG.error(""Error updating ledger config metadata for ledgerId {}"", lh.getId(), ex);"
"<line3>          logger.warn(""Git error on branch notification"");"
<line21>      log.error(systemException, systemException);
"<line11>        LOG.debug(""WAITING FOR SCENARIO"");<line16>      LOG.error(ex.getMessage(), ex);<line17>      LOG.debug(""shutting down"");<line19>        LOG.debug(""shutdown succesful"");<line20>        LOG.error(e.getMessage(), e);"
"<line8>                    logger.debug(""Characteristic {} from {} has been read - value {}"",characteristicUUID,address,HexUtils.bytesToHex(data));"
<line20>      log.error(exception, exception);
<line5>      LOG.error(e.getMessage(), e);
"<line3>    LOG.info(""Removing all values for table {}, keys = {}."", table, keysToRemove);"
"<line7>      log.trace(""[{}] Failed to find session"", session.getId());<line8>    log.trace(""[{}] Session transport error"", session.getId(), tError);"
"<line10>              logger.info(""find singleton tuning Object from IOC"");<line11>              logger.info(""can not find singleton  tuning Object from IOC"");<line23>          logger.warn(""can not load tuning class,tuning is close"", e);"
"<line2>    LOGGER.info(""Get Observation from ExperimentRun test start................................"");<line10>    LOGGER.info(""GetObservations Response : "" + response.getObservationsCount());<line23>    LOGGER.info(""Get Observation from ExperimentRun tags test stop................................"");"
"<line6>        log.debug(""The method "" + aopMethodInvocation + "" must have at least one parameter"");<line12>        log.debug(""The first parameter of "" + aopMethodInvocation + "" must implement ClassedModel"");<line18>        log.debug(""The first parameter of "" + aopMethodInvocation + "" must be a long"");<line33>        log.debug(sb.toString());"
"<line8>      logger.trace(""Beginning paged ldap search with ""+ resultsPerPage+ "" results per page.  Filter: ""+ inEncodedFilter+ "" using dc: ""+ ((LdapContextSource) inLdapTemplate.getContextSource()).getUrls()[0]+ "" using baseLdapPath: ""+ ((LdapContextSource) inLdapTemplate.getContextSource()).getBaseLdapPathAsString());<line16>      logger.trace(""Paged ldap search complete with "" + inHandler.getList().size() + "" results retrieved"");"
"<line12>      LOGGER.error(""Cannot create lock on "" + lockF, e);"
"<line10>      LOG.error(""Interrupted BSP thread."", e);<line12>    LOG.info(""Succesfully cleaned up after bsp."");"
"<line7>      logger.debug(""The buffer for AMQP Large Mesasge was probably not complete, so an exception eventually""+ "" would be expected"",expected);"
"<line4>      LOG.debug(""Failed to clone the AstElement. Returning same reference; the client may fail. {} - {}"",e.getClass().getName(),e);"
"<line29>        logger.error(""Error"", e);"
"<line7>      LOGGER.debug(""Unable to get connectionn string"", ex);"
"<line2>    LOGGER.debug(""CCSProtocollType: "" + msg.getCcsProtocolType().getValue());"
"<line29>    log.info(""publish message : {}, sendResult:{}"", sendMsg, sendResult);<line45>    log.info(""receive message : {}"", receiveMsg);"
"<line13>      LOG.error(""error during delete document: {}"", e.getLocalizedMessage(), e);"
"<line5>    logger.info(""Stopping Event Broker instance ..."");<line12>      logger.error(""Failed to stop Event Broker"", e);<line20>    logger.info(""Stopping Event Broker instance ... done!"");"
"<line7>    log.trace(""Scheduling proactive sas token renewal for device {} in {} milliseconds"",this.amqpsSessionHandler.getDeviceId(),sasTokenRenewalPeriod);"
"<line4>        LOGGER.error(""Host Name should not be null."");"
"<line6>        logger.error(""Failed to shut down registry cleanly: "" + getRegistryId(), e);<line11>      logger.error(""Failed to shut down registry cleanly: "" + getRegistryId(), e);<line15>      logger.error(""Failed to cleanly dispose: "" + e.getMessage(), e);"
"<line5>    QLog.l().logger.info(""\""QSystem "" + About.ver + ""\""!  date: "" + About.date);<line5>    QLog.l().logger.info(""START LOGER. Logger: "" + QLog.l().logger().getName());<line6>      QLog.l().logger.info(""Version DB="" + ServerProps.getInstance().getProps().getVersion());<line6>      QLog.l().logRep.info(""START LOGGER for reports. Logger: "" + QLog.l().logRep().getName());<line9>        .info(""Mode: ""+ (QConfig.cfg().isDebug()? ""KEY_DEBUG"": (QConfig.cfg().isDemo() ? ""KEY_DEMO"" : ""FULL"")));<line9>    QLog.l().logger.info(""Plugins: "" + (QConfig.cfg().isNoPlugins() ? ""NO"" : ""YES""));<line10>      QLog.l().logger.info(""Auto start: YES"");"
"<line3>      log.info(""Server started. Press return to stop."");<line25>      log.info(""Shutting down applications."");<line27>      log.info(""Launched Brooklyn; will now block until shutdown command received via GUI/API""+ "" (recommended) or process interrupt."");"
"<line3>    LOGGER.debug(""getting service for filter {} from tracker"", filter);"
"<line9>                  log.warn(""My Header"");<line9>                  log.warn(""In Two Lines"");<line14>    logger.warn(""Long line that will get next message to be written to next file"");<line14>    logger.warn(""test2"");"
"<line22>                  logger.trace(LogMarker.DLS_VERBOSE, ""getLockTokensForRecovery is skipping {}"", token);"
"<line4>    LOG.trace(""Adding parentProcedure {} to procedure {}"", parentProcedure, procedure);"
"<line7>      logger.info(""Trying to cancel job {} with savepoint, but no savepoint directory configured."",executionGraph.getJobID());<line14>    logger.info(""Triggering {}savepoint for job {}."",cancelJob ? ""cancel-with-"" : """",executionGraph.getJobID());<line28>                logger.info(""Savepoint stored in {}. Now cancelling {}."", path, executionGraph.getJobID());"
"<line7>      LOG.error(""Failed to process ValidationResult as JSON"", e);"
"<line5>    log.error(""BUG! %s for %s leaked with %s %s.  Cleaning up so server can continue to function."",getClass().getName(), taskId, leakedValues, counterName);"
"<line5>    logger.warn(""no extension found for class:"" + clazz.getName());"
"<line2>      LOG.debug(""Getting info for task(id={})"", taskAssignmentId);"
<line9>              LOGGER.error(msg, t);
"<line4>    LOGGER.debug(""Get {}"", id);"
"<line10>    logger.debug(""deleting {}"", target);"
"<line3>        logger.error(""Reconnect to resource store timeout, abandoning..."", ex);<line7>      logger.info(""Will try to re-connect after {} seconds."", seconds);"
"<line7>      LOGGER.warn(""You have provided additional parameters that are not necessary for the information""+ "" parameter {}."",infoRequest.getInfoParameter());"
"<line15>        log.error(""Can not handle POP3 connection"", e);"
"<line2>    logger.warn(""Nuvo binding incorrectly configured. Please configure for Serial or IP over serial""+ "" connection"");"
"<line7>        LOG.info(""addSdsets tenant={} name={} description={}"",getTenant(),sd.getName(),sd.getDescription());<line14>          LOG.warn(""addSdsets tenant={} name [{}] caught SecurityException={}"",getTenant(),sd.getName() + se);"
"<line3>      log.warn(""Asterisk is not configured or denied in room #{}"", r.getId());"
"<line12>      Log.info(""Project extracted to '"" + outputFolder.getAbsolutePath() + ""'"");"
<line7>      LOGGER.info(prettyString(doc));
"<line4>      LOGGER.error(""Failed to terminate workers"", e);"
"<line1>    log.info(""Entering DelegateExpressionBean.execute()"");<line11>    log.info(""Leaving DelegateExpressionBean.execute()"");"
"<line1>    LOG.info(StringUtils.format("" type = %s, density = %06.5f, count = %5d, average byte size = %5d"",type, density, count, totalBytes / count));"
"<line8>      LOG.debug(""execute command: "" + hardLink.toString());"
"<line4>        logger.trace(""Pump command (ack): {}: "", p);<line6>        logger.trace(""Turn pump control panel (ack) {}: {} - {}"",p.getSource(),p.getByte(PentairPacket.STARTOFDATA),p);<line8>        logger.trace(""Set pump mode (ack) {}: {} - {}"",p.getSource(),p.getByte(PentairPacket.STARTOFDATA),p);<line10>        logger.trace(""Set run mode (ack) {}: {} - {}"",p.getSource(),p.getByte(PentairPacket.STARTOFDATA),p);<line13>          logger.debug(""Expected length of 15: {}"", p);<line15>        logger.debug(""Pump status: {}"", p);<line27>        logger.debug(""Unhandled Intelliflo command: {}"", p.toString());"
"<line1>    logger.debug(""Registering OSGIObjectFactory"");<line2>      logger.debug(""De-Registering Previous OSGIObjectFactory"");<line7>    logger.debug(""OSGIObjectFactory installed"");"
"<line2>      log.debug(""Try relogin kerberos at first!"");"
"<line13>              log.debug(""Authentication failed"", e);"
<line3>        logger.warn(STACK_TRACE_LOGGER_PREFIX + format, buildNewArgs(args, cause));<line4>        logger.warn(format, args);
"<line30>    log.info(""Counts of documents in Each Label"");<line31>    log.info(""{}"", c);<line32>    log.info(params.print());"
"<line2>    LOG.info(""Executing Alluxio action: LoadAction, path:"" + uri.toString());<line5>        LOG.warn(message);<line7>      LOG.info(""Path "" + uri + "" was successfully loaded."");"
"<line4>      log.warn(""Transport error"", exception);"
"<line18>      LOG.debug(""Waiting for stats"");<line24>      LOG.debug(""Waiting for property set"");"
"<line17>      logger.warn("""", t);"
"<line51>      logger.error(""Error occurred while trying to calculate statistics for Index: {}, Type: {}"",index,type);<line51>      logger.error(e.getMessage(), e);"
<line27>      log.error(systemException, systemException);
"<line2>      logger.debug(""Annotator sends {} new variants for annotation. Waiting for the result"",variantList.size());<line21>          logger.warn(""Empty result for '{}'"", cellBaseDataResultList.get(i).getId());"
"<line7>      logger.debug(""Refresh for Trigger group not implemented"");<line9>        logger.debug(""Channel Value command for {}: {}"", channelUID, command);<line14>          logger.debug(""Failed to send trigger command {} to {}"", command, group, e);"
"<line21>        logger.warn(""eXist-db does not support the collation reorderCode: {}"", reorderCode);"
"<line6>        LOG.warn(""Ignoring attempt to create spout '{}' with override == false."", id);"
"<line34>        logger.error(""Could not update family {}: {}"", family.getId(), e.getMessage(), e);"
"<line4>      logger.error(""stateMachine with id not found while processing event {} "",stateMachineInstanceId,event.getName());<line12>    logger.debug(""These states {} depend on event {}"", dependantStates, event.getName());<line13>    logger.debug(""These states {} are now unblocked after event {}"", executableStates, event.getName());"
"<line14>        logger.debug(""Realm was specified in principal {}, default realm was not added to the identity being""+ "" authenticated"",rawPrincipal);<line16>        logger.debug(""Realm was not specified in principal {}, default realm {} was added to the identity""+ "" being authenticated"",rawPrincipal,defaultRealm);<line18>        logger.debug(""Realm was not specified in principal {}, default realm is blank and was not added to""+ "" the identity being authenticated"",rawPrincipal);<line22>        logger.debug(""Created authentication token "" + token.toString());<line25>        logger.debug(""Ran provider.authenticate(token) and returned authentication for ""+ ""principal={} with name={} and isAuthenticated={}"",authentication.getPrincipal(),authentication.getName(),authentication.isAuthenticated());"
"<line2>    LOG.info(""onCachePeriodChanged with value {} has been triggered!"", period);"
"<line6>    logger.info(""PREPARE QUERY HANDLE : "" + queryPrepareHandle1);<line9>    logger.info(""PREPARE QUERY HANDLE : "" + queryPrepareHandle2);"
"<line14>      Log.error(""Problem creating a temp directory to use for zipping workflow "" + e.getMessage());<line41>      Log.error(""Failed to copy file to S3!"");"
"<line15>      LOG.debug(""==> RangerAccessControlEnforcer.isAccessAllowed(""+ path+ "", ""+ access+ "", ""+ context.user+ "")"");<line18>      LOG.warn(""RangerAccessControlEnforcer.isAccessAllowed(""+ path+ "", ""+ access+ "", ""+ context.user+ ""): no Ranger accessType found for ""+ access);<line50>      LOG.debug(""<== RangerAccessControlEnforcer.isAccessAllowed(""+ path+ "", ""+ access+ "", ""+ context.user+ ""): ""+ ret);"
"<line7>        logger.debug(""Attempting to guess on driver name."");<line16>      logger.debug(""Unable to read JDBC metadata."", e);"
"<line1>    log.debug(""deleting FilterCol instance"");<line3>      log.debug(""delete successful"");<line4>      log.error(""delete failed"", re);"
"<line1>    log.debug(""finding NZobSb instance by example"");<line7>      log.debug(""find by example successful, result size: "" + results.size());<line9>      log.error(""find by example failed"", re);"
"<line2>      log.debug(""hostname verification disabled"");<line11>    log.debug(""configuring DefaultHostnameVerifier to expect hostname={}"", tHostname);"
"<line21>        logger.error(""Failed retrieving features: {}"", e.getMessage(), debugException(e));"
"<line2>      LOGGER.debug(""Loading users information from the external UserSystemService"");<line4>      LOGGER.debug(""Users information was loaded successful: {} users were returned from external system,""+ "" next synchronization will be in a period of {}"",userListSize,usersSyncInterval);<line13>      LOGGER.warn(msg);<line13>      LOGGER.debug(msg, e);"
"<line4>        logger.info(""Cuboid "" + lastCuboidId + "" has "" + cuboidRowCount + "" rows"");"
"<line2>    log.warn(""Not implemented: DockerIaas.releaseAddress()"");"
"<line14>        LOGGER.info(""Unable to create service from WSDL location. Set log level for""+ "" \""org.codice.ddf.security.claims.attributequery.common\"" to DEBUG for more""+ "" information."");<line14>        LOGGER.debug(""Unable to create service from WSDL location."", e);"
"<line28>    LOGGER.info(""Checking if MirrorMaker has log level set properly"");"
<line1>    LOGGER.error(message, e);<line1>    getProcessLogger().error(message, e);
"<line9>            log.error(""Failed to reload authN providers on endpoint {} response {}"",endpoint,response.toString());<line11>          log.error(""Caught exception trying to reload an authsvc on {} continuing"", endpoint, e);<line14>      log.error(""Caught coordinator exception trying to find an authsvc endpoint"", e);"
"<line11>          LOG.debug(""Set attributes for {}, options: {}"", path.getPath(), options);"
"<line2>    logger.debug(""Adding ring set: "", ringSet);"
"<line13>      log.debug(""Executing SQL query: {}"", getDiffsAffectedByUserSQL);<line34>      log.warn(""Exception while removing item diff"", sqle);"
"<line28>      LOGGER.debug(""Problem determining size of resource; defaulting to {}."", size, e);"
"<line11>      LOG.error(""Exception when doing join on delegate "" + this, e);"
"<line13>        log.info(this, ""@key: "" + key + ""@value:"" + value);"
"<line5>    LOG.debug(""LinkRemoved notification   ........"");<line5>    LOG.debug(""-------------------------------------------"");"
"<line14>          logger.debug(""Found evaluator {} for TestFunction {}"",clazz.getCanonicalName(),functionClass.getCanonicalName());<line16>          logger.debug(""Found evaluator {} for NodeTest {}"",clazz.getCanonicalName(),functionClass.getCanonicalName());<line18>          logger.debug(""Found evaluator {} for NodeFunction {}"",clazz.getCanonicalName(),functionClass.getCanonicalName());<line20>          logger.debug(""Found evaluator {} for NodeSelector {}"",clazz.getCanonicalName(),functionClass.getCanonicalName());"
"<line2>      logger.error(""failed to execute multiSearch: {}"", multiSearchResponse);"
"<line3>    LOGGER.debug(""***************  START: {}  *****************"", methodName);<line17>    LOGGER.debug(""***************  END: {}  *****************"", methodName);"
"<line10>        LOG.warn(""could not get table column view index for [{}]. Loaded value '{}'"",col.getClass().getName(),value,e);"
"<line9>            log.debug(""to be introspected: {}"", type);"
<line7>      log.debug(key);
"<line6>        LOG.debug(""Merging XML based CamelContext with Spring Boot configuration properties"");"
<line6>      logger.error(errMsg);
"<line8>      LOG.warn(""Progress from unknown child task: "" + taskId);"
"<line6>    logger.info(""exec script : {}"", transformCmd);<line12>        logger.error(""Here is the standard error of the command (if any):\n"");<line12>        logger.error(s);"
"<line14>        logger.info(""Creating new table '{}' with command(s): {}"", spec.getName(), sqlCmds.toString());"
"<line1>    logger.debug(""vcenter: "" + vcenter.name);<line11>      logger.error(""Failed to save controller configuration"", e);"
"<line2>    logger.debug(""Received API discovery response "" + response);"
"<line4>      logger.debug(String.format(""host[uuid:%s] load successfully"", cmsg.getHostUuid()));<line5>      logger.warn(String.format(""canceled connect kvm host[uuid:%s], because it connecting now"", cmsg.getHostUuid()));<line6>      logger.warn(String.format(""failed to load host[uuid:%s], %s"", cmsg.getHostUuid(), reply.getError()));"
"<line2>    logger.debug(""postProcessContext() for "" + deploymentUnit.getName());"
"<line2>    logger.info(getClass().toString() + "" job starting ..."");<line4>    logger.info(allShares.size() + "" anonymous share(s) have been found to be deleted"");"
"<line3>    LOG.trace(""Sleeping {} ms before retry #{}..."", sleepTime, attempts);"
"<line6>      logger.warn(""Failed to create reflection factory [""+ factoryClass.getName()+ ""], falling back to standard reflection instead"");"
"<line4>        log.debug(""Received frame {}"", frame);<line7>            log.debug(""WebSocket data: {}"", frame);<line22>        log.debug(""Returning nominal exception back to the client: {}"", e.getMessage());<line25>      log.error(""Internal Server Error while handling incoming web socket frame"", e);<line28>        log.warn(""Could not inform client of earlier Internal Server Error due to additional exception ""+ e2,e2);"
"<line7>        LOG.warn(""Cannot generate NodeStat, datanode {} not found."", dn.getUuid());"
"<line9>      log.info(""é®ä»¶åéæå"");<line10>      log.error(e.getMessage());"
"<line8>      log.debug(""Deleting node {"" + getUuid() + ""}"");<line19>      log.debug(""Deleting node {"" + getUuid() + ""} vertex."");"
"<line5>    logger.info(""Stopping EnOcean discovery scan"");"
"<line2>    LOG.info(""Running Verify with outputDir="" + outputDir + "", numReducers="" + numReducers);"
"<line18>      logger.info(""skipped unit or constant = "" + c);<line20>    logger.info(""added to co-prime mset = "" + c);"
"<line49>      log.error(""Exception while comparing values of property '"" + prop + ""': "" + ex.getMessage());"
"<line4>      LOG.warn(""Cannot send ping message over web-socket session {}."", session, e);"
"<line25>      logger.error(""Error"", e);<line64>      MiscUtils.getLogger().error(""Couldn't add requested OLIS lab to Inbox."", e);"
"<line2>    logger.info(""Received: {}"", message);<line13>      logger.error(""Failed to send a message back to remote."", e);"
"<line17>        logger.warn(e.getMessage());<line17>        logger.warn(trash.getLabelValue());<line20>      logger.debug(String.format(""transfer %s trash to recycles from %s: %s"", recycles.size(), type, recycles));"
"<line3>      log.debug(""Calling ExampleActionSetService"");"
"<line7>      LOG.warn(""Unable to find user role in SAML, will not be included in message to policy engine"");"
"<line3>      ActiveMQRALogger.LOGGER.trace(""getClientID()"");"
"<line4>    LOGGER.info(""instances: {}"", instances);"
"<line13>        log.warn(""exception closing inputstream"", e);"
<line19>    LOGGER.info(sb.toString());
"<line3>    log.info(""Initializing PageServlet"");<line8>    log.info(""Referrer spam check enabled = "" + this.processReferrers);<line13>        log.error(""Error parsing referrer.robotCheck.userAgentPattern value '""+ robotPatternStr+ ""'.  Robots will not be filtered. "",e);"
"<line28>        LOG.error(""Error getting container quota: {}"", e.getMessage());"
<line13>        log.debug(sb.toString());
"<line7>      log.error(""Error in thread "" + Thread.currentThread().getId(), ex);"
"<line12>        LOGGER.error(String.format(""Unable to compare Kubernetes version for cluster version : %s with %s"",version.getName(), KubernetesClusterService.MIN_KUBERNETES_VERSION_HA_SUPPORT),e);"
"<line1>    log.trace(""Stopping the Connect group member."");<line9>    else log.debug(""The Connect group member has stopped."");"
<line17>        LOG.warn(txt.toString());<line23>      LOG.trace(sb.toString());
"<line9>            LOGGER.warn(""On notify lost lock"", e);"
"<line11>        log.trace(""Sending additional invalidation for requestors if necessary."");"
"<line6>    LOG.info(""Successfully processed password reset request for user {}"", resetPassword.getUser());"
"<line6>    LOGGER.info(""Switch Configuration Async Request received from organisation: {} for device: {}."",organisationIdentification,request.getAsyncRequest().getDeviceId());<line14>        LOGGER.debug(""Get Configuration data is null"");"
"<line2>    log.info(""Entering: isVisible"");"
"<line34>        logger.error(""replicaSet:{} shutdown....."", replicaSetConfig, e);<line41>    logger.info(""replicaSet:{}, already shutdown, replicaTask end of life cycle"", replicaSetConfig);"
<line10>      log.error(portalException, portalException);
"<line7>      log.debug(this, ""Send SMS END: phone="" + phoneNum + "",code="" + code + "",desc="" + desc);"
"<line2>    log.info(""lastModified() for path:{}"", getAbsolutePath());<line7>      log.warn(""file not found exception, return 0L"", e);<line8>      log.error(""jargon exception, rethrow as unchecked"", e);"
"<line10>        logger.info(""found non-instrumented http url connection output stream, please""+ "" report to the Glowroot project: {}"",returnValue.getClass().getName());"
"<line2>      log.debug(""Entering finally block in test case"");"
"<line3>      LOG.info(""reading "" + dockerConf);"
"<line34>          LOGGER.error(""Exception while trying to get attachment version !"", e);"
"<line28>              LOG.error(""Caught exception while checking all ledgers"", e);"
"<line6>        logger.debug(""Set header {}={}"", name, value);"
"<line4>    log.debug(""objectClassTypes={}"", Arrays.toString(objectClassTypes));"
"<line5>    logger.info(""\n\nCreating Cat entity A with name of Dylan\n"");<line9>    logger.info(""\n\nEntity A created with id "" + catA.getUuid() + ""\n"");<line9>    logger.info(""\n\nLooking up cat with id "" + catA.getUuid() + ""\n"");<line11>    logger.info(""\n\nFound entity ""+ cat.getUuid()+ "" of type ""+ cat.getType()+ "" with name ""+ cat.getProperty(""name"")+ ""\n"");<line11>    logger.info(""\n\nCreating cat entity B with name of Nico\n"");<line15>    logger.info(""\n\nEntity B created with id "" + catB.getUuid() + ""\n"");<line15>    logger.info(""\n\nCreating award entity with name of 'best cat'\n"");<line19>    logger.info(""\n\nEntity created with id "" + awardA.getUuid() + ""\n"");<line19>    logger.info(""\n\nConnecting "" + catA.getUuid() + "" \""likes\"" "" + catB.getUuid() + ""\n"");<line20>    logger.info(""\n\nConnecting "" + awardA.getUuid() + "" \""awarded\"" "" + catB.getUuid() + ""\n"");<line22>    logger.info(""Find all connections for cat A: "" + catA.getUuid());<line23>    logger.info(""Find all connections for award A: "" + awardA.getUuid());<line24>    logger.info(""\n\nConnecting "" + awardA.getUuid() + "" \""awarded\"" "" + catA.getUuid() + ""\n"");<line28>    logger.info(""\n\nSearching Award A for recipients with the name Dylan\n"");"
"<line7>        LOG.info(""addUserAdminRoles tenant={} userid={} role name={}"",getTenant(),userRole.getUserId(),userRole.getName());<line10>          LOG.warn(""addUserAdminRoles tenant={} userId={} role name={} caught SecurityException={}"",getTenant(),userRole.getUserId(),userRole.getName(),se);"
"<line2>    log.info(""Executing: {}"", testInfo.getDisplayName());"
"<line4>        LOG.trace(""Expected active node {} but it wasn't there"", event.getPath());"
"<line8>        logger.error(""Error processing "" + tableName, ex);"
"<line3>      log.info(""Run started"");<line7>      log.info(""Run successful"");<line8>      log.error(""Cannot process run"", e);"
"<line6>      LOG.debug(""Session not of the expected type [session={}, expectedType={}]"", session, type);"
<line58>      logger.debug(advice);
"<line2>      logger.info(""Executing resource DeleteStoragePoolCommand: "" + _gson.toJson(cmd));<line17>        logger.warn(""Encounter remote exception to vCenter, invalidate VMware session context"");"
"<line6>        LOG.warn(""Ignoring child {} of location {}({}), as cannot be found"",new Object[] {childId, memento.getType(), memento.getId()});"
"<line9>          LOG.warn(""No preferred Writer {} for format: {} - use {}"",param.className,param.formatName,writer.getClass().getName());"
"<line4>    LOGGER.info(""Initializing domainDistributionAutomationInboundKafkaRequestsMessageListenerContainer""+ "" bean."");"
"<line9>      logger.error(""TTransportException inside handler"", e);<line12>      logger.error(""TApplicationException inside handler"", e);<line15>      logger.error(""Exception inside handler"", e);<line23>      logger.error(""Exception writing to internal frame buffer"", ex);"
"<line2>    LOG.trace(""TX:{} has failed a acknowledge."", getTransactionId());"
"<line40>              log.warn(""KaleoTaskFormInstancePersistenceImpl.fetchByKaleoTaskFormId(long, boolean) with""+ "" parameters (""+ StringUtil.merge(finderArgs)+ "") yields a result set with more than 1 result. This violates the logical""+ "" unique restriction. There is no order guarantee on which result is""+ "" returned by this finder."");"
"<line4>      LOGGER.warn(""Can not be canceled because the caller future isDone or isCompletedExceptionally"");<line8>        LOGGER.warn(""Waiting 1s for taskThread to be set..."");<line10>        LOGGER.warn(e.toString(), e);"
"<line2>      LOG.info(""Not authorized to access resource id <{}>. User <{}> is missing permissions {} on""+ "" instance <{}>"",instanceId,getSubject().getPrincipal(),Arrays.toString(permissions),instanceId);"
"<line2>    log.debug(""DexMatcherServiceImpl:findCounterOffer()"");<line8>    log.debug(""Dumping arguments: type: {}, currentTime: {}, offerAmount: {}, offerCurrency: {},""+ "" pairRate: {}, order: {}"",counterOrderType,currentTime,offerAmount,pairCurrency,pairRate,orderBy);<line19>    log.debug(""offers found: {}"", orders.size());<line26>          log.debug(""match found, id: {}, amount: {}, pairCurrency: {}, pairRate: {}  "",counterOffer.getId(),counterOffer.getOrderAmount(),counterOffer.getPairCurrency(),counterOffer.getPairRate());<line29>        log.debug(""Validation error: {}"", ex.toString());"
"<line3>      LOG.trace(threadName + "": starting"");<line8>      LOG.trace(threadName + "": stopped"");"
"<line16>            LOG.error(""Invalid execution status '"" + execution.getStatus() + '\'', e);"
"<line19>          logger.debug(""Failed to parse {}"", pager.requestedTimeRange, e);"
"<line2>    LOGGER.debug(""Ending tests"");<line5>      LOGGER.error(e);"
<line5>      Log.error(ex);
"<line19>    log.debug(""getHistory:  fsym: {}, resolution: {}, to: {}, from: {}"", symbol, resolution, to, from);<line20>      log.debug(""flushing: "");"
<line8>      log.error(exception, exception);
<line15>      LOGGER.error(e.getMessage(), e);
"<line5>    log.debug(""Operator event for {} - {}"", executionAttemptID, operatorId);"
"<line3>      LOGGER.info(""Found "" + type + "" file: "" + path.toAbsolutePath());"
"<line7>        logger.error(""Hibernate exception on deleting ModelFamily using ID=""+ modelFamilyID+ he.getStackTrace());"
"<line2>    logger.info(""[scheduledTask] start retrieving info"");<line8>    logger.info(""[scheduledTask] scheduled report, total email count: {}"", totalCount);"
"<line8>      log.info(destination + "" created"");<line9>      log.info(destination + "" didn't change, skip rewriting"");"
"<line3>        LOG.debug(""Removing ""+ localJobFile+ "" and ""+ localJarFile+ "" getJobFile = ""+ profile.getJobFile());<line15>      LOG.info(""Error cleaning up "" + profile.getJobID() + "": "" + e);"
<line2>      securityLogger.error(message, t);<line3>      logger.error(message, t);
"<line5>    log.info(m1, ""markers - INFO"");"
"<line9>    logger.info(""Created master and branch1 branches in "" + helper.getGithubRepository().getFullName());"
<line39>    LOGGER.debug(LOG_TIME_TO_QUERY, System.currentTimeMillis() - start);
<line9>      LOG.error(Freedomotic.getStackTraceInfo(e));
"<line13>        LOG.debug(""WAITING FOR SCENARIO"");<line18>      LOG.error(ex.getMessage(), ex);<line19>      LOG.debug(""shutting down"");<line21>        LOG.debug(""shutdown succesful"");<line22>        LOG.error(e.getMessage(), e);"
"<line7>    LOG.warn(""Invalid commit level provided, using the default commit level."");"
"<line12>      LOG.error(""Exception do pre-connect job"", e);"
"<line5>    log.info(Color.GREEN + ""Shape_7 : invalid column shape_dist_traveled"" + Color.NORMAL);"
<line31>    LOGGER.info(msg);<line31>    LOGGER.debug(msg, ce);
"<line14>      LOG.error(I18n.err(I18n.ERR_143,1 + TLV.getNbBytes(hostAddressLength) + hostAddressLength,buffer.capacity()));<line17>      LOG.debug(""Checksum encoding : {}"", Strings.dumpBytes(buffer.array()));<line17>      LOG.debug(""Checksum initial value : {}"", this);"
<line4>      log.error(exception, exception);
"<line6>      log.error(""setEndTime must be called after setStartTime"", new Throwable(INVALID_CALL_SEQUENCE_MSG));"
"<line5>      LOG.debug(""Parsing json array : {}"", value);"
"<line6>      log.warn(""Instantiation exception,caused by :"" + ie.getMessage());<line8>      log.warn(""Illegal access exception,caused by :"" + iae.getMessage());"
"<line4>    logger.debug(""Configuring channels for keypad {}"", integrationId);<line7>      logger.debug(""Clearing existing channels for keypad {}"", integrationId);<line47>    logger.debug(""Done configuring channels for keypad {}"", integrationId);"
"<line7>    logger.error(""{}: INVALID message type with tx: {} from {}"",new Object[] {serverConnection.getName(),Integer.valueOf(clientMessage.getTransactionId()),serverConnection.getSocketString()});"
"<line8>        LOG.debug(""Policy Check Succeeded"");<line11>        LOG.error(""Failed to retrieve payload document. "" + lpe.getLocalizedMessage(), lpe);<line14>      LOG.error(""Policy Check Failed"");"
"<line2>      logger.debug(""Closing handles for LedgerRange: {}"", range);"
"<line14>        logger.warn(""Invalid property value: {}={}"", propertyName, typedValue);"
"<line8>    log.info(""getProjectOverview took "" + watch.taken());"
"<line14>      LOGGER.info(""Retrieving next {} accounts."", NUMBER_OF_RESULTS);<line16>      LOGGER.info(""{} accounts retrieved."", clientCustomerIdsSet.size());"
"<line27>            log.error(e, ""Internal Server Error: Could not covert message body to a Buffer"");"
"<line24>    log.info(""Resolved {} to {}"", name, uris);"
"<line31>            LOG.warn(""Persistence tasks took too long to terminate, when stopping persistence, although""+ "" pending changes were persisted (ignoring): ""+ scheduledTask);"
"<line12>          logger.error(""TTransportException writing to internal frame buffer"", e);<line14>          logger.error(""Exception writing to internal frame buffer"", e);<line29>          logger.error(""TTransportException inside handler"", e);<line32>          logger.error(""TApplicationException inside handler"", e);<line35>          logger.error(""Exception inside handler"", e);<line43>          logger.error(""Exception writing to internal frame buffer"", ex);"
"<line6>      LOGGER.info(""Failed parsing charset"", e);"
"<line6>      logger.debug(""DistTXPrecommitMessage.operateOnTx: Tx {} with Secondaries List {}"",txId,this.secondaryTransactionalOperations);<line32>            logger.debug(""DistTXPrecommitMessage.operateOnTx: Tx {} Failed while creating response"", txId);<line36>          logger.debug(""DistTXPrecommitMessage.operateOnTx: Tx {} Failed while applying changes for""+ "" replicates"",txId);"
"<line29>                          log.info(""Setting AUTO_PAY_OFF on accountId='{}' because of default payment""+ "" method deletion"",account.getId());"
"<line13>    log.info(""getContact {}"", contact.toString());"
"<line3>      log.debug(""Found existing entry for "" + key);<line5>    log.debug(""Did not find existing entry for "" + key);"
"<line15>        log.error(EcompLoggerErrorCode.DATA_ERROR,null,""GetToscaElement"",result.right().value().name() + ""for component with id {}"",componentId);"
"<line4>      log.error(""Local alert buffer full!  Clearing!  Dropping "" + alert.getId() + "" record"");"
"<line13>    LOG.error(""Unknown message category: "" + category);"
"<line2>    log.info(""Server: {} - onPing ping={}"", server, ping);"
"<line3>      logger.debug(""{}: Send heartbeat to {} followers"", memberName, nodes.size() - 1);<line14>          logger.warn(""The leadership of node {} is ended."", localMember.getThisNode());"
"<line1>    logger.info(""Stoping mongo server.."");<line14>        logger.info(""stopped.............."");"
"<line2>      LOGGER.info(""Loading libs..."");"
"<line8>      LOGGER.debug(""Failed to load the file [""+ configurationLocation+ ""] using direct ""+ ""file access. The error was [""+ e.getMessage()+ ""]. Trying to load it ""+ ""as a resource using the Servlet Context..."");<line11>      LOGGER.debug(""Failed to load the file [""+ configurationLocation+ ""] as a resource ""+ ""using the Servlet Context. Trying to load it as classpath resource..."");<line15>      LOGGER.debug(""No Servlet Context available. Trying to load it as classpath resource..."");"
"<line28>        LOG.warn(String.format(""Error while closing the input stream of process %s: %s"", process, e.getMessage()));"
"<line28>      LOG.info(""> {}"", new String(buffer));"
"<line9>    log.info(""delete avu metadata from resource: {}"", resourceName);<line9>    log.info(""avu: {}"", avuData);<line11>    log.debug(""sending avu request"");<line17>      log.error(""jargon exception removing AVU metadata"", je);<line19>    log.debug(""metadata removed"");"
"<line3>    ActiveMQRestLogger.LOGGER.debug(""Handling POST request for \"""" + uriInfo.getPath() + ""\"""");"
<line2>    logger.warn(string, o);
"<line2>    LOG.warn(""Failed Context Inferrence"");"
"<line11>    logger.trace(""Using configuration {}"", deviceConfig);"
"<line24>        logger.error(""Recursive problem with weighting model named: "" + name, e);<line25>        logger.error(""Problem with weighting model named: "" + name, e);"
"<line15>      log.info(""Task {} already exists with spec {}"", id, originalSpec);<line25>      log.info(""Failed to create a new task {} with spec {}: {}"", id, spec, failure);<line35>    log.info(""Created a new task {} with spec {}, scheduled to start {} ms from now."",id,spec,delayMs);"
"<line4>      logger.debug(""Retention policy applied successfully."");<line5>      logger.error(""Application of retention policy failed."", e);"
"<line7>      logger.error(""TTransportException writing to internal frame buffer"", e);<line9>      logger.error(""Exception writing to internal frame buffer"", e);"
"<line8>      logger.error(""Error while converting tuple {} {}"", tuple, e);"
"<line7>        LOG.info(""delUserRoles tenant={} userid={} role name={}"",getTenant(),userRole.getUserId(),userRole.getName());<line10>          LOG.warn(""delUserRoles tenant={} userId={} roleName={} caught SecurityException={}"",getTenant(),userRole.getUserId(),userRole.getName(),se);"
"<line2>      LOG.error(""currentElement == null"");"
"<line6>      LOG.warn(""Failed to marshal the events: "" + e, e);"
"<line13>          logger.warn(""Unknown extension mapping type specified in fedora.fcfg"");"
"<line5>        logger.debug(""{}: Conflating {} at queue index={} queue size={} head={} tail={}"",this,object,tailKey,size(),this.headKey,tailKey);<line18>        logger.debug(""{}: Adding index key={}->index={} for {} head={} tail={}"",this,key,tailKey,object,this.headKey,tailKey);<line21>          logger.debug(""{}: Indexes contains index={} for key={} head={} tail={} and it can be used."",this,previousIndex,key,this.headKey,tailKey);<line25>          logger.debug(""{}: No old entry for key={} head={} tail={} not removing old entry."",this,key,this.headKey,tailKey);<line33>          logger.debug(""{}: Previous conflatable at key={} head={} tail={}: {}"",this,previousIndex,this.headKey,tailKey,previous);<line33>          logger.debug(""{}: Current conflatable at key={} head={} tail={}: {}"",this,tailKey,this.headKey,tailKey,object);<line34>            logger.debug(""{}: Removed {} and added {} for key={} head={} tail={} in queue for region={} old""+ "" event={}"",this,previous.getValueToConflate(),object.getValueToConflate(),key,this.headKey,tailKey,rName,previous);<line39>        logger.debug(""{}: Not conflating {} queue size: {} head={} tail={}"",this,object,size(),this.headKey,tailKey);"
"<line5>    LOGGER.debug(""Preparing to ingest {} records"", metacards.size());<line12>      LOGGER.debug(""Error during ingest. Attempting to ingest batch individually."");<line15>      LOGGER.debug(""Error during ingest:"", e);<line19>      LOGGER.debug(""Unexpected Exception during ingest:"", e);"
"<line3>      log.info(""Trying to resend the add command [""+ adaptorName+ ""][""+ offset+ ""][""+ params+ ""] [""+ numRetries+ ""]"");<line5>      log.warn(""Exception in AddAdaptorTask.run"", e);"
"<line2>      logger.info(""Executing resource PingTestCommand: "" + _gson.toJson(cmd));<line17>        logger.error(""Unable to execute ping command on DomR (""+ controlIp+ ""), domR may not be ready yet. failure due to ""+ VmwareHelper.getExceptionMessage(e),e);<line36>        logger.error(""Unable to execute ping command on host (""+ cmd.getComputingHostIp()+ ""). failure due to ""+ VmwareHelper.getExceptionMessage(e),e);"
"<line6>    log.info(""starting reduce, key: "" + key.toString());<line10>    log.info(""time [msec]: "" + (new Date().getTime() - startTime));"
"<line39>          LOGGER.error(() -> ""Unsupported LogType: "" + reusableLog.getLogType());"
"<line2>    LOG.debug(""Grok parser validating message: {}"", message);<line6>        LOG.debug(""Grok parser validated message: {}"", message);<line9>    LOG.debug(""Grok parser did not validate message: {}"", message);"
"<line18>      LOGGER.error(""Error writing HTTP result"", ex);"
"<line11>        LOG.error(""Timer thread caught, ignored an exception"", e);"
"<line3>    LOGGER.debug(""Exchange In Body: {}"", body);<line8>      LOGGER.debug(""New Exchange In Body: {}"", newBody);"
<line57>      LOGGER.error(e.getMessage(), e);
"<line8>          log.trace(""Invoking synchronization.onAfterRoute: {} with {}"", synchronization, exchange);<line10>          log.warn(""Exception occurred during onAfterRoute. This exception will be ignored."", e);"
"<line11>      log.error(""Error writing Metadata Value"", e);"
"<line4>      LOG.error("""", t);<line6>      LOG.error(""èæ°æ®: {}%n"", this.formatDirty(dirtyRecord, t, errorMessage));"
"<line23>      log.error(""Problem with GET request"", e);"
"<line5>    LOG.info(""---------------1 addSchemaVersion {} {}"", schemaMetadata, schemaVersion);"
"<line3>      LOG.info(""Attempting to declare TX:[{}]"", txId);"
"<line4>        logger.debug(""{}: {}"", message, desc.getEndpointUrl());<line7>        logger.trace(""{}: {}"", message, desc);"
"<line3>    this.logger.info(""Trying to find light measurement devices..."");<line5>    this.logger.info(""Found {} light measurement devices."", lmds == null ? ""null"" : lmds.size());<line14>    this.logger.info(""Returning {} results."", result.size());"
"<line5>      logger.debug(""CSRF token from login html: {}"", csrfTokenString);"
"<line1>    log.warn(""WebSocket server is unreachable. Possible VPN/Network issues, will retry in: ""+ reconnectDelay+ "" milliseconds."");"
"<line13>      logger.debug(""Could not resolve a reasonable super-query alias for SelectItem: {}"", toSql());"
"<line4>      log.warn(""CryptoTransfer has no throttle buckets, fee multiplier will remain at one!"");"
"<line2>      LOGGER.debug(""No LdapContextFactory specified - creating a default instance."");"
"<line18>    LOGGER.info(""Found {} testcases."", testcases.size());"
"<line7>      logger.debug(""Naming Exception caught in lookup: "" + e);<line10>      logger.debug(""Exception caught during naming lookup: "" + e);<line18>      logger.debug(""Success SQLException caught in runTest1: "" + e);<line20>      logger.debug(""Exception caught in runTest1: "" + e);"
"<line2>      log.debug(""unsubscribed"");<line3>      log.debug(""onCompleted"");"
"<line4>    logger.debug(""onAuthenticationSuccess() started"");"
<line3>      LOG.info(chat.getParticipant());
<line14>      logger.info(ex.getMessage());
"<line15>          LOGGER.info(""No primary email for orcid: "" + orcid);<line18>          LOGGER.info(""Found {} messages to send for orcid: {}"", notifications.size(), orcid);<line40>        LOGGER.warn(""Problem sending email message to user: "" + orcid, e);"
<line15>      LOG.warn(PARTIAL_IMPL_WARNING);<line38>              LOG.error(I18n.err(I18n.ERR_75));
<line21>      log.error(systemException, systemException);
<line35>      LOG.error(msg, e);
"<line2>    logger.debug(""[isKeeper] check {} keeper or not"", hostPort);<line6>      logger.error(""[isKeeper]{}"", e);"
"<line2>    logger.info(""registering servlet"");"
"<line4>      log.warn(""Capacity hit adding client back to queue. Closing extra"");"
<line49>            LOGGER.debug(e.getMessage());
"<line5>    log.debug(""TAC-participate-commit ::: {}"", hmilyParticipantList);"
"<line48>      logger.debug(""Unable to distribute element: {}"", e.getMessage());<line50>      logger.warn(""Unable to distribute media package {}, element {} to aws s3 channel: {}"",mediaPackageXml,elementId,e);"
"<line5>      log.debug(""Task not found matching "" + matcher + ""; contender names were "" + taskNames);"
"<line3>      log.warn(""The global thread cannot be replaced. Reverting to shutting down the client."");<line3>      log.error(""Encountered the following exception during processing ""+ "" The streams client is going to shut down now. "",throwable);"
"<line10>        log.error(""Unable to get IDP alias for:"" + delegate, e);"
"<line1>    logger.warn(""Unexpected exception in the selector loop."", t);"
"<line33>    logger.debug(""document:{}"", document);<line34>    logger.debug(""val:{}"", stringStringValue);"
"<line12>    LOG.debug(""[deleteResource] (record = [{}]). deletedId: {}"", new Object[] {record, fileId});"
"<line3>        LOGGER.info(format(""%s '%s' never started, nothing to dispose of"",capitalize(artifactType), getArtifactName()));<line9>      LOGGER.error(format(""Error stopping %s '%s'"", artifactType, getArtifactName()), e);"
"<line7>    logger.debug(""Query for alert having id {} resulted in : {}"", id, result);"
"<line2>    logger.info(""The URI "" + uri + "" is recognized as "" + uriType);"
"<line29>      LOG.trace(""Index was not found after recomputing the reducer map"");"
"<line11>        LOGGER.trace("""", x);<line14>      LOGGER.error(L.m(""Kann RDF-Metadatum zur DataID '%1' nicht setzen."", dataId), e);"
"<line9>      log.warn(""Duplicate property values for ["" + propertyKey + ""]."");"
"<line6>      logger.error(""Error on searching pages"", t);"
"<line6>      logger.error(new QueryMobileDetailWithStatusException(0));<line12>          logger.error(new AnalyseMobileDetailByIdException(id));<line15>    logger.info(""Timertask MobileRecordAnalyseTask completed and excute success."");"
"<line9>          logger.error(""ConsistencyWriter: lsn {} or GlobalCommittedLsn {} is not set for global strong""+ "" request"",lsn,globalCommittedLsn);<line14>        logger.debug(""ConsistencyWriter: globalCommittedLsn {}, lsn {}"", globalCommittedLsn, lsn);<line30>                        logger.error(""ConsistencyWriter: Write barrier has not been met for global strong""+ "" request. SelectedGlobalCommittedLsn: {}"",request.requestContext.globalCommittedSelectedLSN);<line30>                        return Mono.error(new GoneException(RMResources.GlobalStrongWriteBarrierNotMet));<line41>      return Mono.error(e);"
"<line3>      logger.error(""Null content"");"
"<line3>      LOGGER.debug(""ApplicationName of filterFactory had been updated to {}"", applicationName);"
<line23>      log.error(systemException, systemException);
"<line4>    log.debug(""Analysing "" + url);<line12>      log.error(i + "": "" + i.getMessage() + "";tika; "" + url + ""@"" + header.getOffset());"
"<line2>    LOG.debug(""Committing record: {} with metadata: {}"", record, metadata);<line3>    LOG.debug(""Committing record with claim check number: {}"", claimCheck);<line9>      LOG.debug(""Record with claim check number: {} committed."", claimCheck);<line10>      LOG.error(""Exception during Unit Of Work completion: {} caused by: {}"",t.getMessage(),t.getCause());<line14>      LOG.debug(""Claim check number: {} freed."", claimCheck);"
"<line13>        log.info(MessageFormat.format(""Found multiple implementations for {0}. Returning first implementation"",executionRequestLabelKey.toString()));"
"<line18>            log.error(""matching returned error. (Should never happen): {}"", match.getMessage());"
"<line7>      logger.info(""## stop the canal client adapters"");<line15>          logger.error(e.getMessage(), e);<line19>      logger.warn(""## something goes wrong when stopping canal client adapters:"", e);<line20>      logger.info(""## canal client adapters are down."");"
"<line1>    logger.debug(""createNetModeTransaction"");"
"<line9>    logger.debug(""Import file: {}"", dropFile.getName());"
"<line2>    LOG.info(""Aborting repair on segment with id {} on coordinator {}"",segment.getId(),segment.getCoordinatorHost());"
"<line1>    log.debug(""PARTICIPANT {}: Releasing listeners"", uid);<line3>      log.trace(""PARTICIPANT {}: Released incoming EP for {}"", uid, inUid);<line9>              log.trace(""PARTICIPANT {}: Disconnected successfully incoming EP for {}"",KStream.this.uid,inUid);<line13>              log.warn(""PARTICIPANT {}: Could not disconnect incoming EP for {}"",KStream.this.uid,inUid);<line19>              log.trace(""PARTICIPANT {}: Released successfully incoming EP for {}"",KStream.this.uid,inUid);<line23>              log.warn(""PARTICIPANT {}: Could not release incoming EP for {}"", KStream.this.uid, inUid);"
<line11>      LOGGER.error(t.getMessage(), t);
"<line32>            logger.debug(""Can not send CEA"", e);<line43>        logger.debug(""Unknown event type {} in state {}"", type(event), state);"
"<line1>    log.debug(""getting FilterSuchenFelderTxt instance with id: "" + id);<line6>        log.debug(""get successful, no instance found"");<line7>        log.debug(""get successful, instance found"");<line10>      log.error(""get failed"", re);"
<line8>      log.error(exception, exception);
"<line6>    LOG.debug(""value: {} timestamp: {}"", value, context.timestamp().getMillis());"
"<line11>      log.error(""Test failed"", e);"
"<line8>        log.debug(""Found {} entitlements for consumer: {}"", foundEntitlements.size(), consumer.getUuid());"
"<line25>        log.info(""Property type unknown: "" + actionMap.get(""type""));"
"<line4>      logger.info(""JspPlugin config={}"", config);<line7>        logger.info(""JspPlugin disabled"");<line11>      logger.info(""Adding Jasper 2 JSP Engine(Tomcat, Jetty, JBoss)."");"
"<line6>      Logger.logger.info(Logger.MISC_LOG + id + "": "" + message);"
"<line21>          log.warn(""Found non-definitional spec parameter: "" + p + "" adding to "" + spec);"
"<line1>    logger.info(""rsfNetManager, shutdownGracefully."");"
"<line2>    log.trace(""draggedImage_movedTo"");"
"<line4>      logger.trace(""scheduled {} for refresh"", this.thing.getUID());"
"<line1>    logger.info(""Scheduling report aggregator job..."");"
"<line4>      log.trace(""Not found portalUser with username ""+ principal.getUserName()+ "". Redirecting to registration form"");"
"<line3>          logger.debug(""startPollStatus '{}' @ rate of '{}' seconds"",getThing().getUID(),tivoConfigData.getPollInterval());<line13>      logger.debug(""Status collection '{}' will start in '{}' seconds."",getThing().getUID(),INIT_POLLING_DELAY_S);<line17>      logger.debug(""Status polling '{}' will start in '{}' seconds."",getThing().getUID(),INIT_POLLING_DELAY_S);"
"<line10>      log.error(""Error while Flushing Lucene Indexes, Caused by: "", e);"
"<line6>      LOGGER.debug(""Setting feature {} = {} for artifact [{}] because of System Property '{}'"",feature,enabled,artifactName,systemPropertyName);<line8>      LOGGER.debug(""Setting feature {} = {} for artifact [{}]"", feature, enabled, artifactName);"
"<line2>    logger.info(""{}"", this);"
"<line5>        LOGGER.warn(""Unable to create XACML PDP."", e);"
<line2>    log.warn(msg);
"<line16>        logger.warn(String.format(""Something seriously wrong happened when roll back""), t);<line18>      logger.warn(String.format(""workflow[%s] in chain[%s] failed because of an unhandle exception"",flow.getName(), getName()),t);<line22>        logger.warn(String.format(""Something seriously wrong happened when roll back""), t1);"
"<line2>    logger.warn(""Error on '{}' delivery: {}"", variant.getType().getTypeName(), reason);"
"<line4>      logger.error(""Path must be not null"");<line10>      logger.debug(""{} uploaded"", mpf.getOriginalFilename());<line21>        logger.error(""Error during file upload"", e);"
"<line9>      log.info(""Trying {}"", method);"
"<line6>    log.debug(""getAggregated() = {}"", result);"
"<line28>          logger.trace(""updateRefNamesInRelations: no documents could be found that referenced the old""+ "" refName"");<line31>        logger.trace(""updateRefNamesInRelations: current page=""+ currentPage+ "" documents included in page=""+ docsInCurrentPage);<line32>          logger.trace(""updateRefNamesInRelations: no more documents requiring refName updates could be""+ "" found"");<line35>          logger.trace(""updateRefNamesInRelations: assuming no more documents requiring refName updates will""+ "" be found, as docsInCurrentPage < pageSize"");<line43>            logger.error(String.format(""Could not update field '%s' with updated refName '%s' for relations record""+ "" CSID=%s"",targetField, newRefName, docModel.getName()));<line51>      logger.error(""Internal error updating the ref names in relations: "" + e.getLocalizedMessage());<line51>      logger.debug(Tools.errorToString(e, true));<line56>      logger.error(""Could not flush results of relation-refName payload updates to Nuxeo repository"");<line57>    logger.debug(""updateRefNamesInRelations updated ""+ docsUpdated+ "" relations document(s)""+ "" with new refName ""+ newRefName+ "" where ""+ targetField+ "" contained old refName ""+ oldRefName);"
"<line12>      logger.info(""Connecting Rayo client XMPP Connection"");<line67>        logger.error(""Trying to connect while the old XMPP connection is active. Please, disconnect first"");<line68>      logger.info(""Rayo client is now connected"");<line73>      logger.error(""Error while trying to opean an XMPP connection"");"
"<line3>    logger.debug(""cycle:"" + cmdprops + "" command:"" + cmdprops);<line8>      logger.info(""skipping cycle "" + cycle + "" because dryrun is set to true"");<line10>      logger.info(""running cycle "" + cycle + "" because dryrun is set to false"");"
"<line1>    logger.info(""Downloading gene Ensembl data (gtf, pep, cdna, motifs) ..."");"
"<line16>                logger.debug(""Failure handling send event request"", e);<line43>                logger.debug(""Failure handling received answer event"", e);<line64>              logger.warn(""Wrong event type ({}) on state {}"", eventType, state);<line68>          logger.warn(""Wrong event type ({}) on state {}"", eventType, state);"
"<line2>    LOG.debug(""Shutting down journal!"");"
<line3>    logger.info(LOG_MESSAGE);
"<line8>      log.error(""Could not get image in setupJob() at zoom level ""+ context.getZoomLevel()+ "" for ""+ input);<line16>        log.debug(""In setupJob(), loading pyramid for ""+ input+ "" pyramid instance is ""+ pyramid+ "" metadata instance is ""+ metadata);"
"<line3>      log.warn(""EOF_ACK_LIMIT reached"");<line5>      log.info(""CFDP sending EOF"");"
"<line7>              log.info(""zookeeper client register success: {}"", v);"
<line20>    logger.info(strb.toString());
"<line4>      log.debug(String.format(""Run with feature %s"", feature));"
"<line4>      LOG.info(""Domain group not found. Nothing to do."");"
<line5>          log.debug(measurement.toString());<line8>        log.error(measurement.toString());<line10>        log.fatal(measurement.toString());<line13>          log.info(measurement.toString());<line17>          log.trace(measurement.toString());<line20>        log.warn(measurement.toString());
"<line4>          logger.warn(""@EnableAutoConfiguration was declared on a class ""+ ""in the default package. Automatic @Repository and ""+ ""@Entity scanning is not enabled."");<line8>          logger.debug(""@EnableAutoConfiguration was declared on a class ""+ ""in the package '""+ packageNames+ ""'. Automatic @Repository and @Entity scanning is ""+ ""enabled."");"
<line6>      log.error(portalException, portalException);<line18>      log.error(exception, exception);
"<line4>        logger.info(""Scheduling job {}"", job);<line11>        logger.info(""Successfully scheduled job."");<line13>      logger.warn(""Could not enqueue scheduled job. "" + ex.getMessage());"
"<line9>      log.error(""Error getting department"", e);"
"<line13>          this.logger.error(""Failed to parse extension [{}]"", localExtension.getId(), e);"
"<line6>      LOGGER.debug(""Proxy handler "" + this + "": connect.strategy="" + connectStrategy + ""."");"
<line12>        log.debug(sb.toString());
"<line4>      logger.debug(String.format(""Attempting to realign read %s at %d more than %d bases to %d."",read.getReadName(), read.getAlignmentStart(), MAX_POS_MOVE_ALLOWED, newStart));"
"<line5>    logger.info(""Writing to COS '"" + bucketName + "":"" + objectName + ""', bytes: "" + raw.length);"
"<line3>    logger.debug(""Putting {} on cache. size: {}"", key, size());"
<line7>      log.error(msg);
"<line5>    LOGGER.debug(""Update partially provider id={} with partialDto={}"", id, provider);"
"<line6>      LOGGER.debug(""UUID matched previously registered subscription."", e);"
"<line23>              log.info(""Preparing to push (stats): processed rows: [%d], sinks: [%d], fireHydrants""+ "" (across sinks): [%d]"",rowIngestionMeters.getProcessed(), theSinks.size(), pushedHydrantsCount.get());<line23>              log.debug(""Building and pushing segments: %s"",theSinks.keySet().stream().map(SegmentIdWithShardSpec::toString).collect(Collectors.joining("", "")));<line25>                  log.warn(""Skipping push of currently-dropping sink[%s]"", entry.getKey());<line32>                  log.warn(""mergeAndPush[%s] returned null, skipping."", entry.getKey());<line34>              log.info(""Push complete..."");"
"<line5>    logger.debug(""logIntraMeasurementDetailsToDB started..."");<line38>      logger.debug(ex.getMessage(), ex);"
"<line2>    LOGGER.info(""Running TestManagementNotifications.testIngest..."");<line9>    LOGGER.info(""Running TestManagementNotifications.testModifyObject..."");<line22>    LOGGER.info(""Running TestManagementNotifications.testAddRelationship..."");<line35>    LOGGER.info(""Running TestManagementNotifications.testPurgeRelationship..."");<line39>    LOGGER.info(""Running TestManagementNotifications.testPurgeObject..."");"
<line6>      logger.error(String.format(NEGATIV_WEIGHT, w));
"<line3>      log.debug(""Calling authorization advice before "" + method.getName());<line6>      log.debug(""User "" + user);<line7>        log.debug(""has roles "" + user.getAllRoles());<line18>          log.debug(""User has privilege "" + privilege + ""? "" + Context.hasPrivilege(privilege));"
"<line6>    logger.info(""Writing {} entities."", size);<line18>    logger.info(""Writes took {} ms"", stop - start);<line33>    logger.info(""Query took {} ms to return {} entities"", stop - start, count);"
"<line2>    log.debug(""Retrieving items for user ID '{}'"", userID);<line13>      log.debug(""Executing SQL query: {}"", getUserSQL);<line23>      log.warn(""Exception while retrieving item s"", sqle);"
<line21>      log.error(e.getMessage(), e);
<line10>        logger.error(e.getMessage(), e);<line22>      logger.error(t.getMessage(), t);
"<line12>        LOG.trace(""Login failed."");<line17>      LOG.trace(""User is already authenticated."");"
"<line3>    logger.debug(""StartOf getProviders - REQUEST for /providers"");<line8>      logger.info(""getTemplates exception:"" + e.getMessage());<line10>    logger.debug(""EndOf getTemplates"");"
"<line1>    log.info(""Query API Key"");"
<line22>      LOGGER.warn(error.toString());
"<line2>    LOGGER.trace(MessageFormat.format(""Deleting Connection {0}."", metadataKey.toString()));"
"<line2>    logger.info(""Starting the Source Connector"");"
"<line6>      LOGGER.debug(""Experiment Count : {}"", count);"
"<line7>      logger.error(""Error loading actionlogger records"", t);"
"<line2>    LOG.debug(""Starting..."");"
"<line16>        LOG.info(""saveExchange--updated-exchangeFound"");<line25>        LOG.info(""saveExchange--adding-exchangeUpdate"");<line30>      LOG.error(""unable to save-exchangeType: {}"", e.getLocalizedMessage(), e);"
"<line9>    logger.debug(""deleting {}"", pageCode);"
"<line6>    LOG.info(""Finished writing all ledger entries so shutdown {} bookies."", numBookiesToKill);<line19>    LOG.info(""Now recover the data on the killed bookie (""+ bookiesSrc+ "") and replicate it to a random available one"");"
"<line6>      log.error(""Thrift error"", e);"
"<line6>      log.warn(""Failed to obtain network statistics. Model is not loaded yet."");<line8>        log.warn(""Failed to obtain network statistics. Topology is not loaded yet."");<line15>            log.warn(""Failed to obtain port statistics for network element""+ ne.getId()+ "". Skipping it."",e);<line18>          log.error(""No matching port statistics"");"
"<line2>    logger.debug(""Initializing MQTT skeleton (ownGbid={}) ..."", ownGbid);"
"<line10>      LOG.info(""Waiting for the driver to be ready"");<line15>    LOG.info(""Launching DAG..."");<line31>      LOG.info(""Waiting for the DAG to finish execution"");<line33>      LOG.warn(INTERRUPTED, e);<line36>      LOG.info(""DAG execution done"");"
"<line12>      LOGGER.debug(""Creating JAXB context with context path: {}."", contextPath);<line15>      LOGGER.info(""Unable to create JAXB context using contextPath: {}."", contextPath, e);"
"<line16>      log.error(""Error while agent {} "", action, e);"
"<line14>                logger.warn(""Failed to write {} to response."", id, e);"
"<line3>      LOG.debug(""Retrieving business entities from UDDI using find_business web service call."");<line14>      LOG.error(sErrorMessage, e);"
"<line2>    log.warn(""[{}] Affected data:\n{}"", this.sqlTableName, this.dataToString(data));"
"<line5>      logger.debug(""validating widget {} for page {}"", widget.getCode(), page.getCode());<line12>      logger.error(""error in validate wiget {} in page {}"", widget.getCode(), page.getCode());"
<line25>      LOG.error(msg);<line30>      LOG.error(message);<line36>    LOG.info(msg);
"<line17>          LOGGER.warn(""Could not find a streaming server provider."");"
"<line20>      LOGGER.warn(""Unable to parse aquisition date"", e);"
"<line6>        logger.debug(""User already has an expired token (or at least a ""+ ""token that expires within the next ""+ EXPIRY_THRESHOLD_MINUTES+ "" minutes). This token ""+ ""will be deleted."");<line8>        logger.debug(""Returning existing token for user '"" + user.getAccountName() + ""'"");<line14>    logger.debug(""Successfully created a user token of type '""+ tokenType+ ""' for user '""+ user.getAccountName()+ ""'"");"
<line1>    log.debug(printStatus());
"<line7>      log.error(""main threw"", e);"
<line9>          logger.debug(exception, ex);
"<line23>      logger.debug(""Entity {} created"", entity.getProperty(""name""));"
"<line17>      log.debug(""no results from iRODS, return as an empty result set"");<line34>    log.debug(""doing a close for this page..."");"
"<line2>      LOG.info(""Test a="" + a);<line12>            LOG.info(""Case 1 found solution ""+ a+ ""^""+ m+ "" + ""+ b+ ""^""+ n+ "" = ""+ rhs.base+ ""^""+ rhs.exponent);<line22>            LOG.info(""Case 2 found solution ""+ a+ ""^""+ m+ "" = ""+ b+ ""^""+ n+ "" + ""+ rhs.base+ ""^""+ rhs.exponent);<line32>            LOG.info(""Case 3 found solution ""+ b+ ""^""+ n+ "" = ""+ a+ ""^""+ m+ "" + ""+ rhs.base+ ""^""+ rhs.exponent);<line32>            LOG.debug(n + "".th root(a^m) = "" + nthRoot + "", b - nthRoot = "" + (b - nthRoot));"
"<line8>      LOG.error(""Error while substituting variables"", e);"
<line6>      log.error(exception, exception);
"<line5>        log.debug(""Dis-enrolling sampledevice device : "" + deviceId);<line16>        log.warn(msg, iotDAOEx);<line18>      log.error(msg, e);"
"<line22>          log.debug(""Outbound Websocket frame is throttled. "" + ctx.channel().toString());"
"<line11>        LOG.debug(""Triggering checkpoint {} for {} ({}) was rejected by the mailbox"",checkpointID,taskNameWithSubtask,executionId);<line21>          LOG.debug(""Encountered error while triggering checkpoint {} for ""+ ""{} ({}) while being not in state running."",checkpointID,taskNameWithSubtask,executionId,t);<line24>      LOG.debug(""Declining checkpoint request for non-running task {} ({})."",taskNameWithSubtask,executionId);"
"<line17>            log.trace(""sendHeartBeat({}) received reply size={} for request={}"",session,reply.available(),heartbeatRequest);"
<line2>    logger.debug(Messages.GETTING_PARAMETERS_OF_SERVICE_BINDING_0, guid);
"<line7>      LOG.debug(""Removing file [{0}] with length [{1}]."", name, length);"
<line29>              log.debug(illegalArgumentException, illegalArgumentException);
<line7>      log.error(exception, exception);
"<line3>    LOGGER.debug(""Solr Config directories made?  {}"", directoriesMade);<line13>          LOGGER.debug(""Wrote out {} bytes for [{}]."", byteCount, filename);<line14>          LOGGER.warn(""Unable to copy Solr configuration file: "" + filename, e);"
"<line3>      logger.warn(""{} denies to handle request for {} due to missing search service"", this, query);<line5>      logger.warn(""{} denies to handle request for {} since no uri is defined"", this, query);<line7>      logger.debug(""{} denies to handle unknown request"", this);<line11>      logger.debug(""{} denies to handle request for {}"", this, query);<line16>    logger.debug(""{} accepts to handle request for {}"", this, query);"
"<line12>      logger.info(""GET JDBC User FAIL. "" + e.getMessage());<line17>      logger.debug(""Install JDBC Proxy END: "" + targetServer + ""@"" + user + "","" + rc, null);"
"<line3>      LOG.debug(""The configured batch timeout '{}' for sensor type '{}' is <=0 or > the maximum allowable""+ "" batch timeout '{}'. Setting the batch timeout to the maximum allowable."",batchTimeoutSecs,sensorType,maxBatchTimeout);"
"<line6>    LOG.info(""ZKClient session closed"");<line10>      LOG.info(""Updating timestamp. Previous/New {}/{}"", previousMaxTimestamp, newMaxTimestamp);"
"<line6>      LOGGER.trace(""DesEncrypter unsupported encoding exception"", e);<line8>      LOGGER.trace(""DesEncrypter encryption failed"", e);"
"<line10>    logger.info(""dataèæ¶ï¼"" + DateUtils.getWasteTime(startTime));"
"<line10>          log.debug(""Received status change event ["" + (i++) + ""]: "" + statusUpdate);<line22>    log.info(""Waiting to receive all {} status updates..."", nStatusUpdates);<line26>    log.info(""Waiting to receive all {} build set status updates..."", BUILD_SET_STATUS_UPDATES);<line28>    log.debug(""All status updates should be received. Semaphore has {} free entries."",semaphore.availablePermits());"
"<line2>      log.info(""Writing source file for document {}"", doc.getName());<line4>      log.info(""Writing source file for document {} (skipped due to dry run)"", doc.getName());"
"<line12>      LOGGER.warn(""Could not generate Signature! Using empty one instead!"", E);"
"<line25>      log.error(""Exception was thrown during entry processing."", e);"
<line12>      log.error(ex.getMessage(), ex);
"<line1>    LOG.trace(""buildRepairRunFromRow {} / {}"", id, repairRunResult);"
<line10>        log.debug(exception, exception);
"<line16>    LOGGER.info(""Modify the business object definition in the search index associated with the business""+ "" object definition tag being deleted. tagTypeCode=\""{}\"", tagCode=\""{}\"",""+ "" businessObjectDefinitionId=\""{}\"", searchIndexUpdateType=\""{}\"""",businessObjectDefinitionTagEntity.getTag().getTagType().getCode(),businessObjectDefinitionTagEntity.getTag().getTagCode(),businessObjectDefinitionEntity.getId(),SEARCH_INDEX_UPDATE_TYPE_UPDATE);"
"<line19>          LOG.debug(""Registered JDBC driver via shim {}. Actual Driver {}."", driverShim, driver);<line34>      LOG.debug(""Transaction isolation level: {}"", level);"
"<line46>      logger.info(""orphan = "" + _orphaned);"
"<line4>      LOG.trace("""" + AbstractEntity.this + "" modifyAttribute "" + attribute + "" "" + modifier);<line17>        LOG.warn(message + "" (future messages for this sensor logged at trace)"");<line18>        LOG.trace(message);"
"<line5>      LOG.warn(""Can't commit a completed message."");"
"<line15>        logger.error(""[refresh]"", e);"
"<line5>      LOG.error(""Error while creating process log"", e);"
"<line25>        logger.debug(""{}: get path count of {} from {}, result {}"",metaGroupMember.getName(),partitionGroup,node,count);<line35>    logger.warn(""Cannot get paths of {} from {}"", pathsToQuery, partitionGroup);"
"<line2>    LOGGER.debug(""GET called on : {}"", arg0);"
"<line1>    log.debug(""findAllDataFileColumns() - dataFileId: {}"", dataFileId);"
"<line10>        log.warn(""update: remove failed for key: {0}"", toSpool.getKey());"
"<line2>    logger.debug(""Update received for CCO: {} {}"", type, Arrays.asList(parameters));<line14>          logger.warn(""Unable to parse update {} {} from CCO {}"",type,Arrays.asList(parameters),integrationId);"
<line8>          log.debug(_NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey);
<line23>    logger.info(statusString);
"<line12>      logger.info(""[uploadInternal] - received request to upload file "" + name);<line24>      logger.info(""[uploadInternal] - successfuly uploaded file ""+ name+ "" [upload key = ""+ uploadKey+ ""]"");"
"<line10>        log.info(""Failed to de-registering driver {}"", driver);"
"<line12>    logger.warn(""Unable to parse the long integer system property '{}':{} - using the default value: {}"",key,value,def);"
"<line3>    LOG.debug(""Calling OpenstackSwiftContainerResource.getAllShouldSucceed()"");"
"<line8>          LOG.warn(""[converLeadRecord] Couldn't find mapping for column {}."", f.name());"
"<line10>      logger.error(""Action=CreateQueue status=error exception="", e);"
"<line10>      logger.debug(""insert HostApplicationMap, host:{}, app:{},SType:{},parentApp:{},parentAppSType{}"",host,bindApplicationName,bindServiceType,parentApplicationName,parentServiceType);"
"<line4>        logger.trace(""Iterator not complete and there are results object is null, advancing"");"
"<line4>    LOGGER.info(""Server Profile Template object returned to client: {}"", template.toJsonString());"
"<line2>      logger.debug(""Flow Configuration file was null"");<line7>        logger.warn(""Flow Configuration does not exist or was empty"");<line10>      logger.error(""An error occurred determining the size of the Flow Configuration file"");<line16>        logger.warn(""Could not extract root group id because Flow Configuration File was empty"");<line25>        logger.warn(""rootGroup element not found in Flow Configuration file"");<line30>        logger.warn(""id element not found under rootGroup in Flow Configuration file"");<line38>      logger.error(""Unable to parse flow {} due to {}"", new Object[] {flowPath.toAbsolutePath(), ex});"
"<line2>    log.info(""Generating POJO"", out.file().getName());"
<line28>        log.trace(r.toString());<line28>        log.trace(Hexdump.toHexString(this.sbuf, 4, size));
"<line22>      log.error(""{}"", e);"
"<line3>    LOG.debug(""Inside InsightsAuthenticationProviderImpl === "");<line10>      LOG.debug(""Authentication token is missing - authentication.getPrincipal() {} "",authentication.getPrincipal());"
"<line2>    logger.debug(""Executing job command line"");<line9>        logger.error(""Subcommand not valid"");"
<line3>    OneResponse response = image.info();<line5>      LOGGER.error(message);
"<line24>          LOGGER.error(""Failed to encrypt preferences: "" + e, e);<line39>        LOGGER.warn(""Failed to rename preferences temp file [""+ temp+ ""] to [""+ aFile+ ""]: preferences were not saved correctly."");<line40>        LOGGER.debug(""Failed to delete temp file: "" + temp);"
"<line12>    LOGGER.info(""got values {}"", values.toString());"
"<line8>      logger.info(""more accurate (""+ dfPercent.format(currentHighestAccuracy)+ "") class expression found after ""+ durationStr+ "": ""+ descriptionToString(bestEvaluatedDescriptions.getBest().getDescription()));"
"<line3>    LOG.debug(""[{}] SSE exception"", this);"
"<line11>      LOG.error(""While handling {}"", artifactDownloadRequest.getTargetDirectory(), t);<line24>        LOG.error(""While sending error for {}"", artifactDownloadRequest.getTargetDirectory(), t2);"
<line7>      LOG.error(e.getLocalizedMessage(), e);
"<line5>      log.error(""onStatus threw"", e);"
"<line8>    LOG.error(""schedule is not supported on Server.Please run your operation on Prism "");"
"<line3>      log.error(""offset < 0 in transfer get() operation, return from get method"");<line19>        log.error(IO_EXEPTION_IN_PARALLEL_TRANSFER, parallelGetFileTransferStrategy.toString());"
"<line1>    LOGGER.info(""About to index disambiguated org, id={}"", entity.getId());<line3>      LOGGER.error(""Unable to send orgs disambiguated message for org: ""+ document.getOrgDisambiguatedName()+ ""(""+ document.getOrgDisambiguatedId()+ "")"");"
<line15>                  LOGGER.info(CALLED);
"<line47>      logger.info(""Network rule conflict: "" + ex.getMessage());<line47>      logger.trace(""Network Rule Conflict: "", ex);"
<line12>          log.debug(e, e);<line15>      log.error(t, t);
"<line2>    LOGGER.debug(""Browser window size was maximized!"");"
"<line4>      logger.debug(""Received an exception sending pdx type to pool {}, {}"",pool,serverConnectivityException.getMessage(),serverConnectivityException);"
"<line16>            LOG.warn(""Timeline server could not bind on a random free port."");<line17>            LOG.warn(String.format(""Timeline server could not bind on port %d. "" + ""Attempting port %d + 1."",tryPort, tryPort));<line19>          LOG.warn(String.format(""Timeline server start failed on port %d. Attempting port %d + 1."",tryPort, tryPort),e);"
"<line6>    logger.trace(""{}"", arg);"
"<line20>      log.error(""Unable to get asset entries"", exception);"
"<line7>    log.info(""TestOpenL: Elapsed time = {}."", System.currentTimeMillis() - t);"
"<line13>            logger.error(""Unable to open GPIO resource {}"", pin.getName(), e);"
"<line35>      log.debug(""Logged in as '"" + user + ""'"");"
"<line9>      logger.error(""Error occured while working with JSON!"", e);<line10>      logger.error(""Error reading from history file!"", e);"
"<line16>      LOG.error(""Error updating domain: {}"", domainException.getLocalizedMessage(), domainException);"
"<line2>    logger.warn(""Putting host in rotation"");<line4>      logger.info(""Registering ServiceRegistry {}"", serviceRegistry);<line7>        logger.warn(""Failed to register ServiceRegistry {}. Exception: {}"", serviceRegistry, e);"
"<line1>    log.debug(""getting StgG20Anwb instance with id: "" + id);<line5>        log.debug(""get successful, no instance found"");<line6>        log.debug(""get successful, instance found"");<line9>      log.error(""get failed"", re);"
"<line3>    log.info(""addUserGroupAsGroupAdmin()"");<line12>    log.info(""user group:{}"", userGroup);<line13>      log.error(""cannot create a group with a different zone"");<line17>      log.debug(""executing user admin PI"");<line19>      log.warn(""no more rules exception will be treated as duplicate user to normalize behavior for""+ "" pre-2.5 iRODS servers"");"
"<line15>        logger.debug(""Exception creating connection: "" + e);"
"<line2>      log.debug(""Processing tests results."");<line7>      log.error(""error while processing tests results."", e.getMessage());"
"<line3>      logger.debug(BaseServiceTest.getTestBanner(""headSupported"", CLASS_NAME));<line10>        logger.debug(""headSupported url="" + url + "" status="" + statusCode);<line11>          logger.debug(""headSupported header name="" + h.getName() + "" value="" + h.getValue());<line15>      logger.error(""Fatal protocol violation: "", e);<line16>      logger.error(""Fatal transport error"", e);<line17>      logger.error(""unknown exception "", e);"
"<line21>          LOGGER.error(""Failed to create ReaderListener"", e);<line37>        LOGGER.error(""Unexpected error invoking beforeScan listener ["" + listener.getClass().getName() + ""]"",e);<line59>        LOGGER.error(""Unexpected error invoking afterScan listener ["" + listener.getClass().getName() + ""]"",e);"
"<line24>          logger.error(METADATA_INDEX_NODE_DESERIALIZE_ERROR, file);<line39>          logger.error(""Something error happened while deserializing TimeseriesMetadata of file {}"", file);"
"<line12>      LOGGER.warn(""Cannot save output of time measuring: "" + ex.getMessage());"
"<line2>    log.info(""Finding document by id."");"
"<line15>        LOG.debug(""request finalize {}"", prevPartNotFinalized);"
"<line2>    log.info(""starting testTargetMappingUpdatesAfterRebind"");<line14>    log.info(""resizing "" + cluster + "" - "" + cluster.getChildren());<line17>    log.info(""resized "" + cluster + "" ("" + result + "") - "" + cluster.getChildren());<line22>    log.info(""expecting "" + ImmutableSet.of(target1, target2, target3));<line23>    log.info(""pretending one node down"");<line25>    log.info(""unmanaging another node"");<line27>    log.info(""success - testTargetMappingUpdatesAfterRebind"");"
"<line2>    LOGGER.trace(event.getPhaseId().toString() + "" - Before Phase"");"
"<line1>    logger.debug(""Adding map: {}"", mapName);<line6>        logger.debug(""Found map {}"", tempFieldName);"
"<line10>        log.debug(""Impossible to undelete document ""+ docRef+ "" as it does not support transition ""+ LifeCycleConstants.UNDELETE_TRANSITION);"
"<line13>      logger.error(""Failed to retrieve job process status for process "" + processId, ex);"
"<line8>        LOG.debug(""Request received error: {}"", result.getMessage());"
"<line19>              log.info(""Waiting for messages delivery"");<line23>    log.info(""Sending dummy custom messages"");<line31>              log.info(""Waiting for messages delivery [sentSize=""+ sentEnsuredMsgs.size()+ "", rcvdSize=""+ receivedEnsuredMsgs.size()+ ']');"
"<line16>    LOG.info(""Full Outer Join ("" + plan.getPID() + "") chooses [Hash Join]"");"
"<line10>      logger.debug(""to be created, pottag common"");<line10>      logger.debug(objectAsXmlString(pottagCommon, PottagsCommon.class));"
<line7>      LookupResource.LOG.error(message);
"<line1>    LOGGER.debug(""==== BUILDING SPRING APPLICATION CONTEXT AND APP ENGINE""+ "" ========================================="");<line12>      LOGGER.debug(""==== SPRING APP ENGINE CREATED""+ "" =================================================================="");"
"<line10>        log.debug(""No applicable policy at "" + absPath);"
<line14>      log.error(exception, exception);
"<line18>    LOG.info(""Found at ""+ instantTime+ "" from CleanerPlan. #partitions_updated=""+ records.size()+ "", #files_deleted=""+ fileDeleteCount[0]);"
"<line12>        LOG.warn(""Cannot find managed bundle for added bundle "" + id + ""; ignoring"");"
"<line2>    logger.info(""Connection to Minecraft server stopped"");"
"<line8>      logger.debug(""Found prefix {}:{} for suffixPath: {}"", component.getHome(), prefix, suffixPath);<line14>    logger.debug(""Fixed path for {} is {}"", suffixPath, path.toString());"
"<line2>      log.debug(""JBoss 6 VFS API is not available in this environment."");"
"<line6>        LOGGER.debug(""Routes: \n{}"", ModelHelper.dumpModelAsXml(camelContext, routes));"
<line33>      log.error(e);
"<line3>      log.warn(""No entityId for "" + key + "" ignoring IdP"");<line6>      log.warn(""No address for "" + entityId + "" ignoring IdP"");<line9>      log.warn(""No certificate for "" + entityId + "" ignoring IdP"");<line16>      log.warn(""No translation profile for "" + entityId + "" ignoring IdP"");"
"<line10>          logger.info(""No. of shards provided is : "" + no_of_shards);<line31>            logger.info(""Created collection ""+ solr_collection_name+ "" with config name ""+ solr_config_name+ "" replicas =  ""+ no_of_replicas+ "" Shards = ""+ no_of_shards+ "" max node per shards  = ""+ max_node_per_shards);<line34>          logger.info(""Collection already exists with name "" + solr_collection_name);"
"<line1>    log.debug(""Initializing Metadata Validation Timer"");"
"<line2>      LOGGER.error(""Raft incomplete initialization!"");<line8>      LOGGER.debug(""putClusterMeta {} {}"", metaType, metaKey);"
"<line5>      logger.debug(""GetStorageStatsCommand on pool "" + cmd.getStorageId() + "" failed"", e);"
"<line18>        logger.warn(""Couldn't contact AIDRTaggerAPI for sending error message"");<line20>      logger.error(""Error in contacting AIDRTaggerAPI: "" + clientResponse);"
"<line11>      logger.warn(""Usage stats job aggregation range is to small, using the minimum value of ""+ UsageUtils.USAGE_AGGREGATION_RANGE_MIN);<line14>      logger.debug(""Usage timezone = "" + _usageTimezone + "" AggregationDuration="" + _aggregationDuration);"
"<line3>      LOG.info(""Deleted file [{}]"", f);<line9>      LOG.debug(""Deleted directory [{}], [{}]"", f, f.delete());"
"<line3>      logger.trace(LogMarker.SERIALIZER_VERBOSE, ""Writing Short {}"", value);"
"<line8>      logger.info(""adding activemqComponent for brokerUri {} with brokerComponentName {}"",brokerUri,brokerComponentName);<line14>        logger.warn(""could not start activemq"", e);"
"<line2>      LOGGER.debug(""Encrypting byte[] of size [{}] on thread [{}]"",input == null ? null : input.length,Thread.currentThread().getName());<line14>                      return Mono.error(new IllegalArgumentException(""Invalid encryption path: "" + includedPath.getPath()));<line32>                                      return Mono.error(ex);"
"<line9>                LOG.debug(""Streams cleared successfully"");<line13>                LOG.warn(""Unable to clear streams"", throwable);"
"<line13>          logger.debug(""Unexpected: Holder B <-> holdable connection accepted"");<line21>          logger.debug(""As expected, accepting second holder conneciton failed"");"
"<line2>    LOG.info(""Trying to connect to TSO [{}]"", tsoAddress);<line8>              LOG.info(""Connection to TSO [{}] established. Channel {}"",tsoAddress,channelFuture.channel());<line9>              LOG.error(""Failed connection attempt to TSO [{}] failed. Channel {}"",tsoAddress,channelFuture.channel());"
"<line3>    logger.info(""Stopping processor [{}]"", name);"
"<line2>    LOGGER.debug(""KeyShareLength: "" + entry.getPublicKeyLength().getValue());"
"<line40>    logger.info(""Started redis cluster with mapped ports: {}"", translationMappings);"
"<line12>    log.debug(""Processing [{}] tables"", result.size());<line16>          log.debug(""Table = '{}'"", item.toString());<line35>            log.debug(""Table = {}, Min/Max = {} at height = {}"",item.toString(),minMaxValue,targetHeight);<line46>              log.debug(""Table = {}, exported rows = {}"", item.toString(), totalCount);<line50>              log.debug(""Table = {}, imported rows = {}"", item.toString(), imported);<line56>            log.error(""Exception"", e);<line60>    log.debug(""Processed Tables = {}"", result);"
"<line10>    logger.trace(String.format(""getting nodes: %s"", Joiner.on(',').join(toGet)));"
<line11>      log.error(exception, exception);
"<line2>      log.info(""Attempt to re-start paused TaskManager is ignored. Please use resume instead"");<line15>        log.debug(""No JMS resources will be cached/shared between poller ""+ ""worker tasks of ""+ jmsConsumerName);<line17>        log.debug(""Only the JMS Connection will be cached and shared between *all* ""+ ""poller task invocations"");<line19>        log.debug(""The JMS Connection and Session will be cached and shared between ""+ ""successive poller task invocations"");<line21>        log.debug(""The JMS Connection, Session and MessageConsumer will be cached and ""+ ""shared between successive poller task invocations"");<line31>    log.info(""Task manager for "" + jmsConsumerName + "" [re-]initialized"");"
"<line12>      log.error(""Unable to add folder menu item"", exception);"
"<line3>    LOG.info(""Obtaining authentication info"");<line5>        LOG.info(""Authentication is using OAuth"");<line7>        LOG.info(""Authentication is using local login authority"");<line9>        LOG.info(""Authenticating user '{}'"", principal);<line11>          LOG.info(""No exception but user object is null"");<line13>        LOG.info(""User {} is authenticated"", principal);<line16>          LOG.error(""Unexpected exception creating account object"", t);<line23>      LOG.warn(""Blocked user {}"", ex);<line25>      LOG.warn(""Authentication failed. Message: {}"", ex.getMessage());<line27>      LOG.info(""Insufficient bucket tokens: {}"", ex.getMessage());<line29>      LOG.warn(""NullPointer"", npe);<line31>      LOG.warn(""Throwable"", t);"
"<line2>    logger.error(""OAuth exception: "" + ex.getMessage());"
<line18>              log.error(e.getMessage(), e);
"<line2>    log.trace(""[{}] Processing edge {} event update "", tenantId, edgeId);"
"<line3>    logger.debug(""sendScreenshotMissingMessage to: {}"", username);"
"<line7>      LOGGER.trace(format(""Generate iterator for select : %s"",statementWrapper.getBoundStatement().preparedStatement().getQueryString()));"
"<line11>            log.debug(""Tick"");"
"<line7>          LOG.debug(""Using cache directory: {}"", cacheDirectory);<line20>          LOG.info(""Downloading {}:{}:{}"", groupId, artifactId, version);<line28>        LOG.warn(""Error during add components from artifact {}:{}:{} due {}"",groupId,artifactId,version,e.getMessage(),e);"
<line12>      LOGGER.error(ex);<line16>        LOGGER.error(e);
"<line6>    LOG.debug(""startKey: {}"", pr.getStartKey());<line6>    LOG.debug(""startDocId: {}"", pr.getStartKeyDocId());"
"<line10>      log.error(""Cannot load stream definitions from ""+ directory.getAbsolutePath()+ "" directory not exist"");<line13>      log.error(""Cannot load stream definitions from ""+ directory.getAbsolutePath()+ "" not a directory"");<line33>            log.error(""Error in reading file "" + fileEntry.getName(), e);<line34>            log.error(""Error in converting Stream definition "" + e.getMessage(), e);<line40>              log.error(""Error occurred when reading the file : "" + e.getMessage(), e);"
"<line4>        LOG.info(""Creating configuration store directory: {}"", storePath);"
"<line3>    log.trace(""Produced XML configuration:\n"" + generatedXMLConfiguration);<line3>    log.trace(""Expected XML configuration:\n"" + expectedXMLConfiguration);"
<line7>      log.error(new IllegalStateException(_getDependentStringErrorMessage(WindowId.class)));
"<line12>        logger.error(""Preview sync request for site ""+ site+ "" returned status ""+ httpStatus+ "" (""+ httpStatus.getReasonPhrase()+ "")"");<line14>      logger.error(""Error while sending preview sync request for site "" + site, e);"
"<line2>    logger.info(""Scheduler msg timeout ""+ _originalMessage.getMsgId()+ "" timout with ""+ _timeout+ "" Ms"");"
"<line1>    LOGGER.info(""Create Provider {}"", sourceEvent.toString());"
"<line48>      logger.error(""Either check for network connection or table isn't in enabled state, Caused by:"", ioe);"
"<line12>        LOGGER.debug(""overall bytes transfered: {} progress {}%"", bytes, percentage);<line17>      LOGGER.debug(""Transfered: {} bytes in: {} s -> {} kbytes/s"", bytes, time / 1000.0, kBps);"
"<line6>      log.debug(""Job for HD #{} has now reached the CONFIG_COUNT_SNAPSHOT limit {}"",job.getOrigHarvestDefinitionID(),CONFIG_COUNT_SNAPSHOT);<line15>        log.debug(""Job for HD #{} OBJECT_LIMIT of config (domain,config={},{}) incompatible with current""+ "" job"",job.getOrigHarvestDefinitionID(),cfg.getDomainName(),cfg.getName());<line21>        log.debug(""Job for HD #{} BYTE_LIMIT of config (domain,config={},{}) incompatible with current""+ "" job"",job.getOrigHarvestDefinitionID(),cfg.getDomainName(),cfg.getName());<line31>      log.debug(""Job for HD #{} will exceed LIM_MAX_TOTAL_SIZE({}), if config(domain,config={},{}) with""+ "" expected object count {} is added"",job.getOrigHarvestDefinitionID(),LIM_MAX_TOTAL_SIZE,cfg.getDomainName(),cfg.getName(),expectation);<line54>      log.debug(""Job for HD #{} will be incompatible with LIM_MAX_REL_SIZE({}), if""+ "" config(domain,config={},{}) with relDiff {} is added"",job.getOrigHarvestDefinitionID(),LIM_MAX_REL_SIZE,cfg.getDomainName(),cfg.getName(),relDiff);"
"<line11>      LOG.error(""Fail to set resource status, resource: "" + idealState.getResourceName(), e);"
"<line28>        log.warn(""Unexpected receipt response {} "", response);"
"<line4>        logger.debug(""key matches usagesKeyPattern, add the value as usage"");"
"<line24>        LOG.warn(""Task {} is missing port but is being passed to LB  ({})"", task.getTaskId(), task);"
"<line2>    logger.info(""### Create Cache Server. ###"");"
"<line34>        log.info(""Starting: "" + msg);"
"<line4>      LOG.warn(""æ²¡æå¯¹åºçconsumerLog, subject:{}"", subject);<line10>        LOG.info(""UNKNOWN consumeFromWhere code, {}"", consumeFromWhereCode);"
"<line13>          logger.info(""Overwriting existing file {} because import file is newer."", entry.getName());"
"<line2>      log.info(""creating initial "" + type + "" cache with timeout "" + timeout + ""..."");<line3>      log.info(""done"");"
"<line12>      LOG.trace(""Backpressure happens: in flight {} limit {}"",appendEntryLimiter.getInflight(),appendEntryLimiter.getLimit());"
"<line2>    LOG.debug(""invoke!!!"");<line2>    LOG.debug(object.getClass().getName() + ""#"" + method.getName() + ""()"");"
"<line3>        logger.trace(""Responding with error: {}, v={} ON {}"",error.getMessage(),request.connection().getVersion(),Thread.currentThread().getName());<line14>      logger.error(""Failed to reply with error {}, got error whilst writing error reply: {}"",error.getMessage(),t.getMessage(),t);"
"<line8>        LOG.debug(""No configuration data found for class loader default search contexts."");<line19>            LOG.debug(""Class {} found under default search contexts."", name);<line22>            LOG.debug(""Class {} could not be found under default search contexts."", name);<line36>      LOG.debug(msg);<line39>      LOG.error(msg, e);"
"<line5>        logger.debug(""restore properties file '{}' with properties '{}'"", propFile, entry.getValue());<line16>      logger.error(""Error in restore sahi config properties"", e);"
"<line17>          LOG.warn(String.format(""Failed to ingest message Error: %s Stacktrace: %s"",e.getMessage(), Throwables.getStackTraceAsString(e)));<line18>          LOG.warn(String.format(""%s %s"", err.getErrorMessage(), err.getStackTrace()));"
"<line7>        LOGGER.debug(""New mail - sender: {}, recipients: {}, name: {}, remoteHost: {}, remoteAddr: {},""+ "" state: {}, lastUpdated: {}, errorMessage: {}"",newMail.getMaybeSender(),newMail.getRecipients(),newMail.getName(),newMail.getRemoteHost(),newMail.getRemoteAddr(),newMail.getState(),newMail.getLastUpdated(),newMail.getErrorMessage());"
"<line6>          logger.trace(""Updateing Zone {} {} "", group, r.name);<line32>            logger.debug(""{} Time Left {}"", r.name, running.get().timeLeft);"
"<line29>    logger.info(""Runtime took {} milliseconds to search"", endTime - startTime);"
"<line5>      log.info(""Score at iteration {} is {}"", iteration, score);"
"<line11>      logger.info(""Changed last login column to allow NULL values. Please verify the registration feature ""+ ""if you are hooking into a forum."");"
<line4>      log.error(formattingTuple.getMessage(), formattingTuple.getThrowable());
"<line4>      logger.trace(LogMarker.DM_VERBOSE,""DistTXCommitPhaseTwoReplyMessage process invoking reply processor with processorId:{}"",this.processorId);<line7>        logger.trace(LogMarker.DM_VERBOSE, ""DistTXCommitPhaseTwoReplyMessage processor not found"");"
"<line4>      log.warn(""Task '"" + taskId + ""' not found"");"
"<line7>        LOG.warn(e.getMessage(), e);<line10>        LOG.warn(""Cannot find setter in class '{}' for [{}] column"",step.getClass().getName(),column.getColumnName());"
"<line9>      log.error(""token is null."");<line18>      log.error(""token is warning. token : {}."", tokenValue, e);"
<line16>      log.error(ex.toString());
<line9>        log.debug(exception, exception);
"<line2>    LOG.info(""Starting Recon Task Controller."");"
<line12>      log.error(e.getMessage(), e);
"<line3>    log.info(""Nuking ZooKeeper metadata of existing cluster, ZKServers: {} ledger root path: {}"",zkServers,ledgersRootPath);<line5>      log.info(""There is no existing cluster with ledgersRootPath: {} in ZKServers: {}, ""+ ""so exiting nuke operation"",ledgersRootPath,zkServers);<line14>          log.error(""Bookies are still up and connected to this cluster, ""+ ""stop all bookies before nuking the cluster"");<line21>            log.error(""Readonly Bookies are still up and connected to this cluster, ""+ ""stop all bookies before nuking the cluster"");"
<line33>      log.error(exception, exception);
"<line2>    logger.trace(""Getting sample metadata for "" + sampleId);"
"<line4>        logger.debug(""processing QueueSynchronizationMessage region {} does not exist."", regionName);"
"<line12>          LOGGER.warn(""{} Mapping property {} was found, but was empty"",new Object[] {getSubject.get(), propertyName});<line17>          LOGGER.warn(""{} Mapping property {} was found, but corresponding value {} was not found"",new Object[] {getSubject.get(), propertyName, identityValueProperty});<line22>          LOGGER.debug(""{} Mapping property {} was found, but no transform was present. Using NONE."",new Object[] {getSubject.get(), propertyName});<line28>          LOGGER.warn(""{} Mapping property {} was found, but corresponding transform {} was not valid.""+ "" Allowed values {}"",new Object[] {getSubject.get(),propertyName,rawIdentityTransform,StringUtils.join(Transform.values(), "", "")});<line34>        LOGGER.debug(""Found {} Mapping with key = {}, pattern = {}, value = {}, transform = {}"",new Object[] {getSubject.get(), key, identityPattern, identityValue, rawIdentityTransform});"
"<line4>      LOG.warn(""Graph transaction commit failed: {}; attempting to rollback graph transaction."", ex);"
"<line7>      logger.info(""CommitFunction should be invoked with a TransactionId as an argument i.e.""+ "" setArguments(txId).execute(function)"");<line14>        logger.debug(""CommitFunction: for transaction: {} committing locally"", txId);<line18>          logger.debug(""CommitFunction: resumed transaction: {}"", txId);<line28>        logger.debug(""CommitFunction: for transaction: {} executing NestedTransactionFunction on member: {}"",txId,member);<line41>      logger.debug(""CommitFunction: for transaction: {} returning result: {}"", txId, result);"
"<line32>              log.error(""Error occurred"", exc);"
"<line9>      LOG.warn(""Failed to verify local attachment file"", e);"
"<line3>    logger.debug(""Updating config group: "" + confGroup);"
"<line9>              logger.info(""Successfully scheduled {} to run every {}"",taskNode,taskNode.getSchedulingPeriod());<line10>              logger.error(""Could not schedule {} to run. Will try again in 30 seconds."", taskNode, e);"
"<line6>      this.logger.warn(""land workAt None mode, so land cannot be started."");"
"<line5>      logger.trace(""USER - lost connection: "" + userName + "" id: "" + userId);<line7>      logger.trace(""USER - created: "" + userName + "" id: "" + userId);<line10>      logger.trace(""USER - reconnected: "" + userName + "" id: "" + userId);"
"<line3>      this.logger.debug(""onSendAuthenticationInfoResp_V3"");<line5>      this.logger.debug(""onSendAuthenticationInfoResp_V2"");"
"<line3>    this.logger.info(""Rx: onServiceStartedEvent: event="" + event + "", serviceID="" + serviceID);"
<line21>      log.error(systemException, systemException);
"<line4>      LOGGER.error(""Model tracking failed for core: {}"", coreName, t);"
"<line1>    LOGGER.info(""testEndNoSuccessors"");<line15>      LOGGER.error(e);"
"<line14>      logger.error(""Error extracting the allowed types of attribute elements"", t);"
"<line3>        log.info(""SMS configuration file is not set, SMS notification channel won't be loaded."");<line14>      log.info(""Created a notification channel: ""+ smsCh.getName()+ "" [""+ smsCh.getFacilityId()+ ""]"");"
"<line3>    log.info(""INFO"", new Throwable());<line3>    log.trace(""TRACE"");"
"<line13>    logger.debug(""Deleting a named, parametrized Query with ID ({})."", queryId);"
"<line24>          LOG.debug(""Recoverable error during operation : `{}`. Retrying..."",mktoResult.getErrorsString());<line27>          LOG.error(""Unrecoverable error : `{}`."", mktoResult.getErrorsString());"
"<line3>    log.info(""findUserGroups()"");<line6>    log.info(""caseInsensitive:{}"", caseInsensitive);<line6>    log.info(""for user group name:{}"", userGroupName);<line11>      log.error(""error building query"", e);<line31>      log.error(""query exception for query"", e);"
"<line2>    log.info("">>> Starting new grid node [currGridSize="" + gridSize + "", arg="" + type + ""]"");<line7>    log.info("">>> Grid started [nodeId="" + g.localNode().id() + "", name='"" + g.name() + ""']"");"
"<line4>    logger.debug(""SEP Extension: Processing MatchDescriptor in state {}: {}"", seState, response);<line12>          logger.debug(""SEP Extension: Trust Centre not found in network nodes list"");<line18>        logger.debug(""SEP Extension: SEP discovery is using endpoint {} for KeyEstablishment"",trustCenterKeyEstablishmentEndpoint);<line30>          logger.debug(""{}: SEP Extension: SEP discovery node is unknown - getting IEEE address."",response.getSourceAddress().getAddress());<line35>          logger.debug(""{}: SEP Extension: SEP discovery node is not authorised"", node.getIeeeAddress());<line41>          logger.debug(""{}: SEP Extension: Endpoint {} Metering endpoint being added/updated"",node.getIeeeAddress(),endpoint.getEndpointAddress());<line44>            logger.debug(""{}: SEP Extension: Endpoint {} Exception getting simple descriptor"",node.getIeeeAddress(),endpoint.getEndpointAddress());<line56>        logger.debug(""SEP Extension: SEP discovery is using endpoint {} for KeepAlive"",trustCenterKeepAliveEndpoint);"
"<line22>    logger.debug(""Select filtered application map elapsed. {}ms"", watch.getTotalTimeMillis());"
"<line6>      LOGGER.warn(""Unable to register interpreter process, because no such interpreterGroup: {}"",registerInfo.getInterpreterGroupId());<line11>      LOGGER.warn(""Unable to register interpreter process, because no interpreter process associated with ""+ ""interpreterGroup: {}"",registerInfo.getInterpreterGroupId());<line13>    LOGGER.info(""Register interpreter process: {}:{}, interpreterGroup: {}"",registerInfo.getHost(),registerInfo.getPort(),registerInfo.getInterpreterGroupId());"
<line17>        log.debug(exception, exception);
"<line8>      LOG.error(""Found type {} in the query json, but expected type {}."", type, DataQuerySnapshot.TYPE);<line20>        LOG.error(""Error validating {} field"", DataQuerySnapshot.FIELD_DATA);<line31>            LOG.error(""The field {} was listed more than once, this is an invalid query."", field);"
"<line7>    logger.info(""Executing CQL migration from version {} to {}"", fromVersion.get(), toVersion.get());<line8>    logger.debug(""CQL: {}"", schema);"
<line13>        log.debug(exception, exception);
"<line15>            logger.debug(""Replaced {} with {}"", location, updatedLocation);<line26>        logger.warn(""Failed to delete index directory {}; this directory should be cleaned up manually"",oldIndexDir,e);<line29>    logger.info(""Successfully replaced old index directory {} with new index directory {}"",oldIndexDir,newIndexDir);"
"<line3>      LOG.error(""Already registered: "" + listener);"
"<line53>      logger.debug(""Unable to initialize local storage pool: "" + e);"
<line18>      log.error(systemException, systemException);
"<line25>        log.error(""Rollback Exception:{}"", e2);<line26>      log.error(""KafkaSink Exception:{}"", e);"
"<line6>        LOG.debug(""State get for {} {} {} {}"",pTransformId,userStateId,Arrays.toString(keyedStateBackend.getCurrentKey().array()),window);"
"<line5>        logger.warn(""Unable to get classpath URL "" + url);<line5>        logger.trace(""Unable to get classpath URL "" + url, ex);"
"<line11>        Log.info(""Launching with metadataWriteback = ""+ metadataWriteBack+ "" since property was ""+ model.getProperty(ReservedIniKeys.METADATA.getKey()));<line24>      Log.error(""You need to specify the output prefix for your workflow using either an override""+ "" parameter at schedule-time or in your workflow INI file as ""+ ReservedIniKeys.OUTPUT_PREFIX.getKey());<line32>      Log.error(""You need to specify the output dir for your workflow using either an override parameter""+ "" at schedule-time or in your workflow INI file as output_dir!"");"
<line3>      logger.debug(LogUtil.getMsg(msg), obj);
"<line6>        LOG.info(""PingChecker interrupted. - masterName:"" + masterName);<line12>              LOG.debug(""currentActiveMaster:"" + currentActiveMaster + "", thisMasterName:"" + masterName);<line22>          LOG.error(e.getMessage(), e);"
"<line17>          LOG.debug(""zkclient {} Prefetch data for path: {}"", _uid, path);<line21>          LOG.warn(""zkclient {} Prefetch data for path: {} failed."", _uid, path, e);"
"<line6>        LOG.debug(""NativeJanusGraphQuery.vertices({}, {}): resultSize={}, {}"",offset,limit,getCountForDebugLog(it),((GraphCentricQueryBuilder) query).constructQuery(ElementCategory.VERTEX));<line7>        LOG.debug(""NativeJanusGraphQuery.vertices({}, {}): resultSize={}, {}"",offset,limit,getCountForDebugLog(it),query);"
"<line2>    logger.debug(""Rollback Conn Transaction"");"
"<line39>        log.warn(""alias fail, missing "" + LessStrings.join(p, "" / ""));"
"<line1>    log.info(""stageBundleForServing("" + s3Path + "")"");<line6>      log.error(""no bundles found at path="" + s3Path);<line9>      log.info(""getting bundle="" + bundle);<line13>        log.info(""unGzip("" + bundleFileLocation + "", "" + _localBundleStagingPath + "")"");<line15>        log.info(""unTar("" + tarFilename + "", "" + _localBundleStagingPath + "")"");<line16>        log.info(""deleting bundle tar.gz="" + bundleFileLocation);<line21>        log.error(""exception exploding bundle="" + bundle, e);"
"<line6>      log.debug(""Set start loop point of Buffer {} to {}"", this.getSystemName(), startLoopPoint);"
"<line10>    log.debug(""Find documents: collection: %s, filter: %s, projection: %s"",tableHandle.getSchemaTableName(), query.toJson(), output.toJson());"
"<line15>        logger.warn(""Could not handle {} for {}"", ((TellstickDevice) device).getStatus(), device);"
"<line6>          LOGGER.warn(""Can't load driver file \""""+ d+ ""\""""+ (ioe.getMessage() != null ? "", reason: "" + ioe.getMessage() : "".""));"
"<line36>          logger.debug(""Parsed LDAP role \""{}\"" to role \""{}\"""", value, authority);<line37>            logger.debug(""Found group for the group with group role \""{}\"""", authority);<line39>              logger.debug(""\tAdded role from role \""{}\""'s group: {}"", authority, role);<line43>          logger.debug(""Found empty authority. Ignoring..."");"
"<line2>    LOG.info(""Load output dir: "" + outputDir);"
"<line11>          log.debug(""Orphan block {} is not connectable right now"", orphanBlock.block.getHash());<line13>        log.info(""Connected orphan {}"", orphanBlock.block.getHash());<line18>        log.info(""Connected {} orphan blocks."", blocksConnectedThisRound);"
"<line7>      logger.warn(""The given event "" + event.getClass().getName() + "" is not supported "");<line16>        CoherentEventFactory.logger.error(e, e);"
"<line16>          logger.info(String.format(""Auto-detection selected discovery strategy: %s"",autoDetectedFactory.getClass()));<line20>          logger.info(""No discovery strategy is applicable for auto-detection"");"
"<line2>    logger.debug(""serviceToID for {}/{}"", systemName, serviceName);<line15>      logger.debug(""serviceToID: "" + e.toString());"
<line20>      log.error(systemException, systemException);
<line18>          LOG.error(e.getMessage(), e);
<line13>        log.debug(sb.toString());
"<line4>    logger.info(""Disabling scheduling via the management service."");"
"<line1>    LOGGER.debug(""Get {}"", id);"
"<line8>      log.trace(""message [ID: {}, address: {}] accepted by peer"", messageId, messageAddress);"
"<line2>      LOG.debug(""The file doesn't own a write lock"");<line26>      LOG.error(ioe);"
"<line8>        OdeConnector.LOG.debug(""Returned list of processes from ODE is null, assuming no process is deployed on ODE"");<line14>      OdeConnector.LOG.error(""Unable to resolve a list of all processes available at ODE"", e);"
"<line13>          log.warn(""PurposeID should be an Integer. Found: "" + member.getText() + "" instead."");<line21>            log.warn(""DisplayOrder should be an Integer. Found: ""+ member.getText()+ "" instead. Setting ""+ ""default display order: ""+ DEFAULT_DISPLAY_ORDER);"
"<line4>      LOG.debug(""Sleeping, i = {}"", i);"
"<line6>    LOG.debug(""Registered {} with Mesos agent {} for framework {}"",executorInfo.getExecutorId().getValue(),agentInfo.getId().getValue(),frameworkInfo.getId().getValue());<line6>    LOG.trace(""Registered {} with Mesos agent {} for framework {}"",MesosUtils.formatForLogging(executorInfo),MesosUtils.formatForLogging(agentInfo),MesosUtils.formatForLogging(frameworkInfo));"
"<line1>    LOGGER.info(""Indexing Spaces..."");<line5>    LOGGER.info(""+++ "" + items.size() + "" items to index +++"");<line7>    LOGGER.info(""Items reindexed!"");"
"<line3>    logger.debug(""apiURL: "" + apiURL);<line55>        logger.info(""Loading "" + blogURL);"
"<line12>        LOGGER.info(""Log4j : {} added"", location);<line19>        LOGGER.info(""Log4j : {} not found"", location);<line28>          LOGGER.debug(""Log4j : property resolved {} -> {}"",propertyName,resolver.getProperty(propertyName));<line30>          LOGGER.warn(""Log4j : property {} cannot be resolved"", propertyName);<line35>        LOGGER.warn(""Log4j : no additional files configured in {}"", Joiner.on("","").join(log4jLocations));"
"<line5>        LOGGER.error(""Override directory must be absolute ["" + ovrTDir + ""]"");<line8>        LOGGER.error(""Bad override dir '"" + dir + ""'"");"
"<line3>      LOGGER.debug(""Code {} was logged but returned null to getExpectedContextEntries(). Should be empty""+ "" list if no expected entries."",code);<line11>        LOGGER.warn(""TDPException context for {}, is missing the given entry(ies) \n""+ ""{}. \n""+ ""Stacktrace for info"",code.getCode(),missingEntries,this);"
"<line6>        LOG.debug(""Acquired lock on file {}. LockFile= {}, Spout = {}"", fileToLock, lockFile, spoutId);<line8>        LOG.debug(""Cannot lock file {} as its already locked. Spout = {}"", fileToLock, spoutId);<line11>      LOG.error(""Error when acquiring lock on file "" + fileToLock + "" Spout = "" + spoutId, e);"
"<line2>    logger.info(""Found existing servers matching the name: "" + managementMachinePrefix);"
"<line2>    logger.debug(""XATerminator set."");"
"<line7>        logger.error(""An error occurred whilst redoing the last set of undone changes."", e);"
"<line5>      log.debug(""val="" + val + "" is not a valid number.  Fudge factor will be 0 seconds"");"
<line6>      log.error(exception, exception);
"<line7>      LOG.error(""Unable to set task as running in recovery db: SQLState: {} Error: {}"",e.getSQLState(),e.getMessage());"
"<line12>                logger.debug(""{}"", this);"
<line10>        logger.error(msg);<line17>      logger.error(msg, e);
"<line3>      logger.error(""Null content"");"
"<line10>    LOG.debug(""model: {}"", changedModel);"
"<line5>    LOGGER.debug(""========================== Test Total ""+ testColumns.length * 2+ "" Queries =========================="");<line18>        LOGGER.debug(""========================== End =========================="");"
"<line3>      LOG.debug(""Setting Robot Timeout back to default values : Selenium ""+ session.getCerberus_selenium_wait_element_default()+ "" Appium ""+ session.getCerberus_appium_wait_element_default()+ "" Sikuli ""+ session.getCerberus_sikuli_wait_element_default());<line7>      LOG.debug(""Setting Robot highlightElement back to default values : Selenium ""+ session.getCerberus_selenium_highlightElement_default()+ "" Sikuli ""+ session.getCerberus_sikuli_highlightElement_default());<line11>      LOG.debug(""Setting Robot minSimilarity back to default values : ""+ session.getCerberus_sikuli_minSimilarity_default());"
"<line2>    LOG.error(""Failed to stop Container "" + containerId);"
"<line4>      logger.debug(""Look in cache for answer to: "" + promptText + "", got "" + response);<line12>        logger.debug(""Asked the user, answer: "" + response);"
"<line5>      LOG.error(""Error in getChildren()"", e);"
"<line9>        log.info(""dropping latest partition from fact storage table: {}. Spec: {}"",storageTableName,partition.getSpec());"
"<line11>      LOG.error(""jar file with reports not found"", ex);"
"<line19>        log.debug(""Found ""+ count+ "" defined locations from properties (*.named.* syntax): ""+ definedLocations.values());<line22>        log.debug(""Adding a defined location for localhost"");"
"<line2>      logger.debug(""Canceling old scheduled job"");"
<line12>      LOG.error(I18n.err(I18n.ERR_132, key, value, name), ioe);
"<line2>    LOGGER.info(""Opening session"");"
"<line28>          logger.error(""Failed to close JMS connection: "", e);"
"<line38>          LOGGER.debug(ERROR_PARSING_MESSAGE, e);<line48>          LOGGER.debug(""Error encoding the binary value into the metacard."", e);"
"<line5>      logger.debug(""Set header {}={}"", name, value);"
"<line16>    LOG.info(""DATA: "" + dataAsJson);"
"<line9>      logger.debug(String.format(""Accept: %s"", accept));<line11>      logger.debug(user + "" "" + req.getURL());"
"<line2>      LOG.debug(""Initializing autoflush handler on channel {}"", ctx.channel());"
"<line4>      LOG.info(""Testcase: "" + this.getClass().getName() + ""#testGetRecord, testDir="" + dataStoreDir);<line5>      LOG.info(""Testcase: ""+ this.getClass().getName()+ ""#testGetRecord finished, time taken = [""+ (System.currentTimeMillis() - start)+ ""]ms"");<line6>      LOG.error(""error:"", e);"
"<line2>      LOG.debug(""==> RangerBasePlugin.createAdminClient(""+ pluginConfig.getServiceName()+ "", ""+ pluginConfig.getAppId()+ "", ""+ pluginConfig.getPropertyPrefix()+ "")"");<line8>        LOG.debug(String.format(""Value for property[%s] was null or empty. Unexpected! Will use policy source of""+ "" type[%s]"",propertyName, RangerAdminRESTClient.class.getName()));<line11>        LOG.debug(String.format(""Value for property[%s] was [%s]."", propertyName, policySourceImpl));<line18>        LOG.error(""failed to instantiate policy source of type '""+ policySourceImpl+ ""'. Will use policy source of type '""+ RangerAdminRESTClient.class.getName()+ ""'"",excp);<line29>      LOG.debug(""<== RangerBasePlugin.createAdminClient(""+ pluginConfig.getServiceName()+ "", ""+ pluginConfig.getAppId()+ "", ""+ pluginConfig.getPropertyPrefix()+ ""): policySourceImpl=""+ policySourceImpl+ "", client=""+ ret);"
"<line33>      logger.error(""getMessageById error"", e);"
"<line19>          LOGGER.debug(""optionalMap: {}"", optionalMap);"
"<line4>      log.debug(String.format(""Run with feature %s"", feature));"
"<line5>      log.debug(""File for writing: "" + file.getAbsolutePath());<line11>      log.debug(""File for writing which does not exist: "" + file.getAbsolutePath());<line12>      log.debug(""External directory not set"");"
"<line16>    LOG.info(""Successfully fetched consent status entity for bic={}, consentId={}"",getXS2AStandard().getAspsp().getBic(),consentId);"
"<line5>    logger.debug(""CatchAllFilter [""+ displayPath+ ""] received provided allowed context paths from NiFi properties: ""+ getAllowedContextPaths());"
"<line33>    log.info(""trips="" + totalTrips + "" withoutStopTimes="" + tripsWithoutStopTimes);"
"<line4>      logger.info(""Initializing Storage Resource Adaptor for storage resource : ""+ storageResourceId+ "", gateway : ""+ gatewayId+ "", user ""+ loginUser+ "", token : ""+ token);<line6>      logger.info(""Fetching credentials for cred store token "" + token);<line11>      logger.info(""Description for token : "" + token + "" : "" + sshCredential.getDescription());<line20>      logger.error(""Error while initializing ssh agent for storage resource ""+ storageResourceId+ "" to token ""+ token,e);"
"<line11>    log.info(""modifying ticket id/string:{}"", ticketId);<line14>    log.info(EXECUTING_TICKET_PI);<line24>    log.info(""received response from ticket operation:{}"", ticketOperationResponse);"
"<line3>      log.info(""Iteration: "" + i);"
"<line7>    LOG.info(""Activating Netconf channel for {} with {}"", getRemoteAddress(), listener);"
"<line7>      LOG.info(ScriptEngineManager.class.getName() + "" initialized in "" + (end - start) + "" ms"");"
"<line4>        logger.info(""SGBProxy done e1 "" + e1.getClass().getName());<line9>      logger.info(""SGBProxy e1 "" + e);<line9>      logger.info(""Exception SGBProxy F = "" + F);"
<line4>    LOGGER.info(messageBuilder.append(cleanAndEncode(message)).toString(), paramSuppliers);
"<line6>        LOGGER.error(""Unable to parse custom height value: "" + e.getMessage());"
"<line4>      LOGGER.warn(""Failed to close Kie Server Controller Client due to: "" + ex.getMessage(), ex);"
"<line2>    logger.info(""Database schema does not exists."");<line2>    logger.info(""Creating database schema from script "" + createSchemaScriptPath);<line15>      logger.error(""Error while running create DB script"", e);"
"<line1>    log.info(""\n-----------------------------------\ntransfer: request = ""+ com.hedera.services.legacy.proto.utils.CommonUtils.toReadableString(transaction));<line5>    log.info(""Transfer record :: "" + record);"
<line26>      log.error(systemException, systemException);
"<line7>    logger.info(""Find candidates by table ""+ factTableName+ "" and project=""+ projectName+ "" : ""+ StringUtils.join(realizations, "",""));<line13>    logger.info(""The realizations remaining: "");<line13>    logger.info(RoutingRule.getPrintableText(realizations));<line13>    logger.info(""The realization being chosen: "" + realizations.get(0).getName());"
"<line2>    logger.info(""Pattern: "" + pattern);<line4>    logger.info(""Modal depth: "" + modalDepth);"
"<line6>      LOG.warn(""parameter rowIds is empty, you should check it, just return a empty vector array""+ "" now!!!"");<line11>      LOG.warn(""parameter indices is empty, you should check it, just return empty vectors now!!!"");"
"<line17>          LOGGER.debug(""Key {} has status {}"", pk, s);<line25>        LOGGER.info(""Some key(s) on index {} do not currently have status {}: {}"",graphIndexName,status,waitingOn);<line26>        LOGGER.info(""All {} key(s) on index {} have status {}"", converged.size(), graphIndexName, status);<line31>        LOGGER.info(""Timed out ({}) while waiting for index {} to converge on status {}"",timeout,graphIndexName,status);"
"<line6>        log.info(""BrokerName {}, topicName {}, queueId {} is pause, Discard the message {}"",messageQueue.getBrokerName(),messageQueue.getTopic(),message.getQueueId(),msgId);"
"<line2>    LOGGER.info(""RiakHttpClient.start {}"", config);"
"<line2>    LOG.info(""SubmarineServerClusterTest:start()"");<line17>        LOG.info(""SubmarineServerClusterTest::start {}(ms) found cluster leader"", wait * 3000);"
"<line23>        logger.error(""{} - Error thrown while acquiring connection from data source"",poolName,e.getCause());<line27>        logger.debug(""{} - Cannot acquire connection from data source"", poolName, e);"
"<line9>    log.debug(""cacheKey:{} LessThanFilter:{} percentiles[{}] = {} multiplier: {}"",cacheKey,lessThanFilter,minBound,percentiles[minBound],result);"
"<line5>        log.error(""Failed to stop driver "" + driver, e);<line9>    log.info(""Stopped ML service"");"
"<line6>      logger.debug(""path="" + path);<line6>      logger.debug(""extension="" + extension);<line6>      logger.debug(""n="" + n);<line6>      logger.debug(""to="" + to);<line19>        logger.debug(""convert:"" + DateUtils.getWasteTime(startTime));"
"<line6>      log.error(e, ""Error reloading configuration"");"
"<line3>      LOG.debug(MessageFormat.format(""Missed explicit Hadoop conf directory: {0}"", ENV_HADOOP_CONF));<line6>      LOG.debug(MessageFormat.format(""Found explicit Hadoop confdir: {0}={1}"", ENV_HADOOP_CONF, conf));"
"<line9>          log.warn(""Cannot close: "" + name + "". Reason: "" + e.getMessage(), e);<line10>          log.warn(""Cannot close. Reason: "" + e.getMessage(), e);"
"<line5>      logger.info(""======={}======="", a);<line5>      logger.info(""1<=>1 = {}"", attribute2Transition2Double.get(a).get(TRANSITION.BETWEEN_11));<line5>      logger.info(""2<=>2 = {}"", attribute2Transition2Double.get(a).get(TRANSITION.BETWEEN_22));<line5>      logger.info(""3<=>3 = {}"", attribute2Transition2Double.get(a).get(TRANSITION.BETWEEN_33));<line5>      logger.info(""1<=>2 = {}"", attribute2Transition2Double.get(a).get(TRANSITION.BETWEEN_12));<line5>      logger.info(""1<=>3 = {}"", attribute2Transition2Double.get(a).get(TRANSITION.BETWEEN_13));<line5>      logger.info(""2<=>3 = {}"", attribute2Transition2Double.get(a).get(TRANSITION.BETWEEN_23));"
<line7>        LOGGER.error(e, e);
<line37>      log.error(exception, exception);
"<line1>    logger.info(""stopping bus ..."");"
"<line7>      log.debug(""Use default invitation type = "" + InvitationType.REGISTRATION.toString());"
"<line2>    JpqlQuery.LOG.debug(""Parsed query successfully {0}"",JpqlQuery.LOG.lazyBoxed(this.qlString, new Object[] {tree.toStringTree()}));"
"<line5>          log.debug(""Closing file handle "" + this);"
"<line6>      logger.debug(""Span faster than span_min_duration. Request discarding {}"", span);<line9>      logger.debug(""Discarding span {}"", span);"
"<line4>      logger.error(""Get file list exception"", e);"
"<line14>          log.warn(""A JSON web service action is already registered at "" + path);<line35>      log.warn(sb.toString());"
"<line2>    log.debug(""starting"");<line14>    log.debug(""finished"");"
"<line24>      log.error(String.format(""Unable to get the output parameters for data provider "" + ""instance with id '%d'"",dataProviderInstanceId),exception);"
"<line26>      logger.error(""Error when trying to encrypt using asymmetric key. Please check the following:\n""+ ""\t- Does the application use an IAM role?\n""+ ""\t- Does the application's role have the permission to use the CMK the value was""+ "" encrypted with?\n""+ ""More information on that topic:""+ "" https://confluence.in.here.com/display/CMECMCPDOWS/Encryption+of+secrets"",e);"
"<line26>          logger.trace(""{}: updateEntryVersionLocally in bucket: {}, key: {}"",getClass().getName(),bucket,key);<line29>          logger.debug(""{}: operateOnRegion caught EntryNotFoundException"", getClass().getName());"
"<line10>    LOG.info(""Running with ""+ numberOfClients+ "" clients - sending ""+ numberOfBatches+ "" batches of ""+ batchSize+ "" messages"");"
<line4>      log.debug(ex.getMessage());
"<line3>      LOG.debug(""==> AtlasStructDefStoreV1.preCreate({})"", structDef);<line22>      LOG.debug(""<== AtlasStructDefStoreV1.preCreate({}): {}"", structDef, ret);"
"<line16>      logger.debug(""done..."");"
"<line21>      LOG.error(""Failed to read report with id {}"", reportId, e);"
"<line16>        logger.debug(""Could not reevaluate key for hash index"");"
"<line2>      log.debug(""Starting Citrus before suite lifecycle"");<line4>      log.error(CitrusExtensionConstants.CITRUS_EXTENSION_ERROR, e);"
"<line8>      log.debug(""Ignoring expected exception during 1-phase prepare"", e);"
"<line14>      log.info(""json dump was created: "" + filename);<line15>      log.error(""cannot create dump"", e);"
"<line4>      log.info(""No lock implementation defined, going to pretend like we got the lock"");<line7>    log.debug(""Trying to acquire lock '{}' with token '{}' and expiry {}ms"", lock, token, lockExpiryMs);<line13>                  log.debug(""Acquired lock '{}' with token '{}'"", lock, token);"
"<line9>    log.info(LogManager.getHeader(context, ""load_bitstream_formats"", ""number_loaded="" + typeNodes.getLength()));"
"<line12>          LOG.trace(""Event matches comment '{}'"", body);<line20>      LOG.error(""Couldn't check comment #{}, skipping it."", issueComment.getId(), ex);"
"<line2>      log.trace(""menuKeyEquivalentTarget_forEvent:"" + menu);"
"<line28>            logger.info(""Call reached AGI"");<line29>            logger.error(""Call never reached agi"");<line31>          logger.error(""Call never reached agi"");<line33>        logger.warn(""Originate failed: "" + trcResult.getAbortReason());<line34>      logger.info(""Hangup status is "" + hangupDetected);"
"<line3>    LOGGER.debug(""Got request on /execute with experimentData={}"", experimentData);<line10>      LOGGER.warn(""Got a request containing a wrong ExperimentType (\""{}\""). Ignoring it."", typeString);<line34>        LOGGER.debug(""Created config: {}"", configs[count]);"
"<line3>      logger.info(""parsing db-schema from "" + options.model_database);<line12>      logger.debug(""read: "" + model);<line13>      logger.info(""parsing ui-schema"");<line15>      logger.debug(""validated: "" + model);<line16>      logger.error(""Parsing failed: "" + e.getMessage());"
<line10>        log.info(records);
<line25>    log.info(json);
"<line30>        logger.error(""getKVPessimisticRollbackMethod failed without a cause"");<line41>        logger.error(String.format(""unexpected resolveLock err: %s, lock: %s"", resp.getErrorsList().get(0), lock));"
"<line4>      log.error(""Configuration is missing values! Url: <{}>, Token: <{}>"", baseUrl, token);<line7>      log.error(""Invalid arguments, reportId must not be less thann 0!"");<line20>      log.error(""Error while trying to download report with id {}."", e, reportId);"
"<line6>        logger.debug(""bound {}->{}"", boundSize, i);"
"<line10>              LOGGER.error(""Failed to delete SSO auth lines from auth index table"", e);"
"<line12>                    logger.warn(""Exception at getUserLists"", e);"
"<line2>    logger.debug(""Request URL: "" + request.getRequestURI());<line2>    logger.debug(""Request Path Info: "" + request.getPathInfo());<line2>    logger.debug(""Request Param: "" + request.getQueryString());<line20>      logger.error(""Cannot write JSON!"", e);"
"<line7>    LOG.info(""ZeppelinHub REST API saving note {} "", note.getId());"
"<line5>      LOG.debug(""{} connecting {} to {} ({})"",remoteEndpoint,localAddress,remoteAddress,connectTimeout);<line18>                    LOG.debug(""{} connected {} {}->{}"",remoteEndpoint,session.getId(),session.getLocalAddress(),session.getRemoteAddress());<line27>                    LOG.debug(""{} connection to {} failed ({}); terminating operation"",remoteEndpoint,remoteAddress,cause.getClass());<line37>                    LOG.debug(""{} connection to {} failed ({}); retrying connection to the next address"",remoteEndpoint,remoteAddress,cause.getClass());"
"<line6>      logger.info(""Failed to initialize LOG4J with xml file."");"
<line7>      log.error(exception, exception);
"<line55>        logger.error(""Error while customizing CML output with customizer: "",customizer.getClass().getName());<line55>        logger.debug(exception);"
"<line32>          LOG.warn(""Unable to convert response headers to MantaHttpHeaders"", e);"
"<line4>    LOG.trace(""Removing procedure {} from offering {}"", procedure, offering);"
"<line5>    log.info(Color.GREEN + ""Transfer_4_1 : missing column min_transfer_time"" + Color.NORMAL);"
"<line26>    logger.debug(""New getIntraPingMedianMeasurement get request recieved with attribute: {}"", attribute);<line29>    logger.debug(""PingMeasurement entry successfully retrieved"");"
"<line36>        logger.info(""Waiting for partitioning tasks to complete: "" + taskCount.get());"
"<line20>      log.warn(""The matrix contains singular elements. Check S matrix at row "" + INFO.getInt(0));"
"<line2>    logger.debug(""Initializing RME handler."");"
<line6>        log.warn(exception, exception);<line19>            log.warn(throwable.getMessage());<line21>          log.error(exception, exception);
<line8>    LOG.info(deserializeRpc.toString());
"<line1>    logger.debug(""Closing connection to the heat pump"");"
"<line39>      LOG.warn(""Can't get the current value for {}, from {}-{}"", me.getId(), startTime, endTime, e);"
"<line23>      log.error(""unexpected key/value types: ""+ k.getClass().getCanonicalName()+ "" and ""+ v.getClass().getCanonicalName());"
"<line11>      LOG.warn(""there isn't a logged user for:"" + loginName, e);"
"<line26>      LOG.error(""Exception while processing tuple"", e);"
"<line5>        logger.warn(LogMessage.format(""Failed to bind Hikari metrics: %s"", ex.getMessage()));"
"<line22>        this.logger.debug(""Found function for POST: "" + path);"
"<line4>      log.info(""Bayes Parameter {}"", params.print());<line4>      log.info(""{}"", params.print());<line8>          log.info(""Testing Bayes Classifier"");<line11>          log.info(""Testing Complementary Bayes Classifier"");<line26>      log.warn(ex.toString(), ex);<line27>      log.error(e.toString(), e);"
"<line4>      LOG.warn(""Connection close fails when migrates state from {} to CLOSED"", getZkState());"
"<line1>    log.debug(""Creating persisted atom mannager."");<line2>    log.debug(""Atom manager initialization complete."");"
"<line8>        log.warn(""Could not fetch partitions for topic/stream [%s]"", getIoConfig().getStream());"
"<line13>        LOGGER.warn(""Exporting the csar failed with an exception"", e);"
"<line1>    log.info(""Stopping forcefully (status: [%s])"", status);"
"<line2>      LOG.debug(""==> RangerHdfsAuditHandler.logHadoopEvent(""+ path+ "", ""+ action+ "", ""+ accessGranted+ "")"");<line18>      LOG.debug(""<== RangerHdfsAuditHandler.logHadoopEvent(""+ path+ "", ""+ action+ "", ""+ accessGranted+ ""): ""+ auditEvent);"
"<line2>    log.info(""------  testBlackListMultiValueIncluded  ------"");"
<line2>      LOG.warn(INVALID_REASON, what.get(), reason.get());
"<line25>        logger.debug(""Copying ""+ script.getPath()+ "" to ""+ s_ovsAgentPath+ "" on ""+ _ip+ "" with permission 0644"");"
"<line4>    logger.info(""Preparing to create tracker file for {} at {}"", fileToRoll, dest);"
"<line7>    log.info(""config: {}"", arg);<line21>      log.error(""Handle createConnector error ."", e);"
"<line7>      log.warn(""Failed to terminate monitor executor service."");"
"<line3>    logger.info(""postProcessEnvironment"");<line13>      logger.info(""Environment order "" + propertySource.getName());"
"<line1>    log.debug(""attaching dirty ModZobjBstMassMitarb instance"");<line3>      log.debug(""attach successful"");<line4>      log.error(""attach failed"", re);"
"<line34>      LOG.info(""Result: "" + counter + "" = "" + record);"
<line15>      LOGGER.info(result);<line16>      LOGGER.info(shards[3]);<line20>        LOGGER.info(keystorePassword);
"<line17>                logger.warn(""No schema agreement from live replicas after {} s. The schema may not be up to""+ "" date on some nodes."",configuration.getProtocolOptions().getMaxSchemaAgreementWaitSeconds());<line22>                logger.error(""Error during schema refresh ({}). The schema from Cluster.getMetadata() might""+ "" appear stale. Asynchronously submitting job to fix."",e.getMessage());<line24>                logger.warn(""Error while waiting for schema agreement"", e);"
"<line3>      logger.info(""{}: Requesting a vote from {}"", memberName, node);<line6>        logger.error(""{}: Cannot request a vote from {}"", memberName, node, e);"
"<line8>          log.warn(""{} skip delete file {} located in parent directory "", LOG_PREFIX, line);<line11>          log.info(""{} deleting file  {}"", LOG_PREFIX, fullPath);<line13>              log.info(""{} successfully deleted file {}"", LOG_PREFIX, fullPath);<line14>              log.error(""{} could not delete file {}"", LOG_PREFIX, fullPath);<line19>      log.error(""Delete file exception "", e);"
"<line3>      LOG.debug(""["" + id_ + ""] moveTo("" + x + "", "" + y + "")"");"
"<line16>      log.debug(""Total bytes read {}"", totalCount);"
"<line10>      logger.warn(""Could not find selection '{}' in mapping {} of device {}"",value,device.getMapping(),device);"
"<line5>    logger.info(""Initialized ripme v"" + UpdateUtils.getThisJarVersion());"
"<line2>    logger.error(""Message arrived to a PublishMQTT processor { topic:'""+ topic+ ""; payload:""+ Arrays.toString(message.getPayload())+ ""}"");"
"<line8>        LOG.debug(""KeyAuthenticationProvider handleAuthentication ({}, {}) -> FAIL.\n"", keyStr, authStr);<line11>    LOG.debug(""KeyAuthenticationProvider handleAuthentication -> OK.\n"");"
"<line1>    LOGGER.debug(""Execute MCRAccessBaseImpl checkPermission for permission {}"", permission);"
"<line3>    LOGGER.debug(Messager.ELEMENT_ATTRIBUTE_FOUND.getMessage(""Location"", point.toString(), getName()));"
"<line8>    logger.info(""called API method: GET /dime/rest/"" + said + ""/resource/"" + personID + ""/"" + resourceID);"
"<line10>      log.info(""Prop ""+ KafkaAvroMessageEncoder.KAFKA_MESSAGE_CODER_SCHEMA_REGISTRY_CLASS+ "" is: ""+ props.getProperty(KafkaAvroMessageEncoder.KAFKA_MESSAGE_CODER_SCHEMA_REGISTRY_CLASS));<line10>      log.info(""Underlying schema registry for topic: "" + topicName + "" is: "" + registry);"
"<line6>    logger.debug(""{} discovered, serialnumber: {}"", deviceTypeName, serialNumber);"
"<line6>      logger.error(""TTransportException writing to internal frame buffer"", e);<line8>      logger.error(""Exception writing to internal frame buffer"", e);"
"<line3>      logger.warn(""{} ConnectorId is missing as param in the Connector-Config! {} / {}@{}"",traceItem,config.getDatabaseSettings().getDb(),config.getDatabaseSettings().getUser(),config.getDatabaseSettings().getHost());<line10>        logger.info(""{} dbMaintainer start"", traceItem);<line11>        logger.info(""{} dbMaintainer finished"", traceItem);"
"<line2>      LOGGER.info(String.format(""Loading '%s' from the classpath."", defaultConfigFile));"
"<line16>        LOG.error(""Error accessing document"", e);"
"<line44>      LOG.error(""Not enough bookies are available to replaceBookie : {} in ensemble : {} with""+ "" excludeBookies {}."",bookieToReplace,currentEnsemble,excludeBookies);"
<line14>        log.debug(exception, exception);
"<line6>      log.warn(""{} was interrupted running task: {}"", name, description);<line7>      log.error(""{} timed out running task: {}"", name, description);<line8>      log.error(""{} caught exception in task: {}"", name, description, e);"
"<line7>    logger.debug(""Creating thing for type '{}'."", thingTypeUID);<line12>          logger.warn(""Cannot create thing of type '{}'. Binding '{}' says it supports it, but it could not""+ "" be created."",thingTypeUID,thingHandlerFactory.getClass().getName());<line18>    logger.warn(""Cannot create thing. No binding found that supports creating a thing of type '{}'."",thingTypeUID);"
"<line10>      logger.info(""Command line argument --""+ KEY_STORE_TYPE_ARG+ ""=""+ keyStoreType+ "" only applies to keystore, recommended truststore type of ""+ KeystoreType.JKS.toString()+ "" unaffected."");"
<line2>    logger.debug(Messages.BINDING_SERVICE_INSTANCE_0_TO_APPLICATION_1, serviceInstanceName, applicationName);
"<line4>      LOGGER.error(""Exception while waiting for registry mutex. Returning null."", e);"
<line20>      LOGGER.info(sb.toString());
"<line2>    log.info(""stopping orbcad ftp download client"");"
"<line16>    log.error(msg + "" "" + path);"
"<line16>      LOGGER.debug(""Invocation has non-null target"");<line31>        LOGGER.debug(""Parameter keeps rename: "" + renamedFilePath);<line39>      TIMERlogger.debug(""[TIMER] Bind original files for job ""+ invocation.getJobId()+ "": ""+ timeBindOriginalFilesElapsed+ "" ms"");"
"<line15>    LOGGER.info(""{}MB {} message sent"",sizeInMB,mode == DeliveryMode.PERSISTENT ? ""durable"" : ""non-durable"");<line17>    LOGGER.info(""{}MB {} message received"",sizeInMB,mode == DeliveryMode.PERSISTENT ? ""durable"" : ""non-durable"");"
"<line6>      LOGGER.debug(""Got ExperimentRun Tags"");"
<line9>        logger.error(e.toString(), e);<line18>      logger.error(e.getMessage(), e);
"<line21>        logger.debug(""findDocs() NXQL: "" + query);<line28>        logger.debug(""Caught exception "", e);"
"<line2>    LOG.info(""Closing the SlotManager."");"
"<line20>      LOGGER.error(""Unable to retrieve OAuth provider's metadata."", e);"
"<line17>    log.debug(""Will allow CORS for the following origins: "" + originsJoiner.toString());"
"<line5>      log.debug(""doClose({})[id={}] SSH_FXP_CLOSE (handle={}[{}])"", session, id, handle, h);"
"<line2>    LOGGER.info(""Add Project Tags Negative test start................................"");<line11>      LOGGER.warn(""Error Code : "" + status.getCode() + "" Description : "" + status.getDescription());<line21>    LOGGER.info(""Add Project tags Negative test stop................................"");"
"<line12>        log.debug(""Error code: 0x"" + Hexdump.toHexString(resp.getErrorCode(), 8));<line21>      log.debug(""Got referral "" + dr);"
"<line5>        LOG.debug(""Checking whether {} is listening for RPCs"", mNodeAddress);<line6>        LOG.debug(""Successfully connected to {}"", mNodeAddress);<line8>        LOG.debug(""Failed to connect to {}"", mNodeAddress);"
"<line12>          logger.error(""TTransportException writing to internal frame buffer"", e);<line14>          logger.error(""Exception writing to internal frame buffer"", e);<line27>          logger.error(""TTransportException inside handler"", e);<line30>          logger.error(""TApplicationException inside handler"", e);<line33>          logger.error(""Exception inside handler"", e);<line41>          logger.error(""Exception writing to internal frame buffer"", ex);"
"<line5>      logger.debug(""Received message on topic {} that could not be decoded. Wrapping it into an""+ "" KuraPayload."",topic);"
"<line5>    logger.debug(""user18 can read (has ACL:READ): {}"", id);<line8>    logger.debug(""user18 can't append (no ACL): {}"", id);<line15>    logger.debug(""user18 can't delete (no ACL): {}"", id);<line18>    logger.debug(""user18 can read (ACL read, append): {}"", id);<line19>    logger.debug(""user18 can append (ACL read, append): {}"", id);<line20>    logger.debug(""user18 still can't delete (ACL read, append): {}"", id);"
"<line7>        log.debug(""    Copy "" + fileName + "" to backup folder"");<line8>        log.info(""    Copy "" + fileName + ""failed"", e);"
"<line2>    LOG.info(""Catalog URI {}"", catalog.getUri());<line10>    LOG.debug(""Target database {}, table {}"", db, table);<line41>      LOG.debug(""partitionKeys {} and partitionValue {}"",partitionKeys.toString(),partitionValues.toString());"
"<line3>      logger.debug(""DecrementAndGet key: {}, count: {}"", key, tally);<line6>      logger.debug(""DecrementAndGet key: {}, count: {}"", key, -1);"
"<line5>          LOGGER.debug(""Attempted to remove "" + name + "", which was not tracked"");<line6>          LOGGER.debug(""Removing "" + name + "" ("" + m_trackedObjectHash.size() + "" remaining)"");"
"<line2>    logger.info(name.getMethodName());<line5>    logger.info(name.getMethodName() + "" - OK"");"
"<line2>    LOGGER.info(""[TaskScheduler] Exception on action "" + action);"
"<line11>      LOGGER.info(""Working with "" + entityType + "" : "" + entityName);<line13>      LOGGER.info(""Instances from -getStatus API: "" + Arrays.toString(instancesFromStatus));<line24>      LOGGER.info(""Instances from SummaryResult: "" + Arrays.toString(instancesFromSummary));"
"<line1>    LOG.info(""Starting to notify all observers that config changed."");<line8>          LOG.error(""Encountered a throwable while notifying observers: of type : {}({})"",observer.getClass().getCanonicalName(),observer,t);"
"<line32>      log.info(""leaving main"");<line33>      log.error(""main threw"", e);"
"<line12>          logger.debug(""Folder closed, reopening"");<line16>          logger.warn(ex);<line21>      logger.error(ex);"
"<line2>    logger.info(""DummyStateModel.onBecomeOfflineFromSlave()"");"
"<line2>    LOG.debug(""mark at {}"", position());"
"<line37>    LOGGER.debug("" # of intervals for M"" + i + "": "" + ms.size());"
<line11>      logger.error(msg, e);
"<line13>    logger.debug(""addMedicalData success : resultId="" + resultId);"
"<line2>    LOG.debug(""Current Backup Count = "" + failoverTransport.getCurrentBackups());"
"<line2>    logger.debug(""test-auth filter {}: '{}'"", XOkapiHeaders.FILTER, phase);<line9>    logger.debug(""filter: 'X-Filter-{}': {}"", phase, phaseHeader);"
<line12>          LOG.warn(ex);
"<line14>    LOGGER.info(""conversation {} runner: {} done"", conversationState.getConversationId(), runner);"
"<line17>        LOGGER.error(""Exception when starting embedded mongo"", e);"
"<line5>      LOGGER.debug(""Launching command: '"" + getCmdString() + ""'"");<line32>        LOGGER.debug(message);<line33>        LOGGER.info(message);<line37>      LOGGER.error(""Execution failed (with exception): "" + e.getMessage(), e);<line39>      LOGGER.fatal(""Execution failed (with error): "" + t.getMessage(), t);"
"<line13>    CompositeFuture.all(tenantId, deviceId, device)<line15>              logger.debug(""updating device [tenant: {}, device-id: {}]"",tenantId.result(),deviceId.result());"
"<line4>    LOG.info(""Http service started on {}"", httpService.getBindAddress());<line22>            LOG.error(""Connect failed {} {}"", uri, sa, ioe);"
"<line2>      log.debug(""JBoss 6 VFS API is not available in this environment."");"
"<line3>      logger.trace(""AIO on error issued. Error(code: "" + errno + "" msg: "" + message + "")"");"
"<line8>        logger.warn(""Exception consumer {} in handle {} threw an exception"", onException, this, ex);"
"<line1>    log.warn(String.format(""Validation result query failed, code: '%s', message: '%s'"",error.getErrorCode(), error.getMessage()));"
"<line5>    logger.warn(""page is not an instance of "" + IManageablePage.class);"
"<line11>            LOG.debug(""Authentication token is present."");<line16>                LOG.debug(""Authenticated principal {} has authority to access resource."",((JwtAuthenticationToken) a).getName());<line26>              LOG.debug(""Authenticated principal {} doesn't have authority to access resource."",((JwtAuthenticationToken) a).getName());<line28>            LOG.warn(""Authentication token is not present. Access is FORBIDDEN."");"
"<line12>        LOG.error(""Failed to delete token crc file."", e);"
"<line6>                log.trace(""periodic commit already triggered, skipping invocation"");<line13>                        log.trace(""do periodic commit; offsets: [{}]"",HonoKafkaConsumerHelper.getOffsetsDebugString(offsets));<line20>                              log.info(""periodic commit failed: {}"", error.toString());<line21>                              log.trace(""periodic commit succeeded"");<line25>                      log.trace(""skip periodic commit - no offsets to commit"");"
"<line3>      LOG.info(""Running in Debug mode"");"
"<line12>            LOG.info(""Component#onConfigure()"");<line27>            LOG.info(""Component#onConfigure(JQueryBehavior)"");<line33>            LOG.info(""Component#onBeforeRender()"");<line38>            LOG.info(""Component#onBeforeRender(JQueryBehavior)"");"
"<line7>                log.info(""Refreshing system access control from %s"", config.getConfigFile());"
"<line6>        logger.debug(""regexMap is not empty : "" + balancerContext.regexMap);<line13>              logger.debug(""Found node for pattern : "" + entry.getKey() + "" and key :"" + entry.getValue());<line16>              logger.debug(""Node not found in the map of nodes. It is null. For pattern: ""+ entry.getKey()+ "" and key :""+ entry.getValue());<line35>        logger.debug(""No node found in the affinity map. It is null. We select new node: "" + node);<line41>          logger.debug(""The assigned node in the affinity map is still alive: "" + node);"
"<line3>      LOG.debug(""This service is maintenance mode now."");<line5>    LOG.trace(""called. [notifyfuture]"");<line16>        LOG.debug(""finish NotifyMailBat [result]"" + result.getResultCode());<line18>        LOG.trace(result.getStdout());<line20>      LOG.error(""Failed to Notify"", e);"
"<line23>      log.warn(""Unexpected exception while map clean-up"", e);"
"<line3>      LOG.debug(""Sign-In-Response received"");<line5>        LOG.debug(""Validating RSTR..."");<line11>          LOG.debug(""RSTR validated successfully"");<line13>          LOG.error(""Federation processing failed: "" + e.getMessage());"
"<line4>    LOGGER.info(""Initializing wsDistributionAutomationInboundDomainResponsesMessageListenerContainer""+ "" bean."");"
"<line2>    LOGGER.debug(""ENTERING: PropertyIsEqualTo filter"");"
"<line9>      logger.error(""Error deleting work gui - typeCode {} - stepCode {}"", typeCode, stepCode, t);"
"<line2>    LOGGER.debug(""get Ingest Entities for id={} "", id);"
"<line4>      LOG.warn(""A CLIENT_ID must be configured for OAuth 2.0"");<line7>    LOG.debug(""Using scope: {}"", scope);<line24>      LOG.error(""Invalid Redirect URL for Trusted Idp"", ex);"
<line26>      LOG.error(e);
"<line2>    LOGGER.info(""feedName: "" + feedName);<line8>    LOGGER.info(""bundleIds: "" + bundleIds);<line9>      LOGGER.info(""bundleId: "" + bundleId);<line11>      LOGGER.info(""coords: "" + coords);"
"<line17>          LOG.error(""Cleanup cycle failed: "" + e.getMessage());<line21>        LOG.info(""Next execution: "" + new Date(new Date().getTime() + sleepMillis));<line24>        LOG.error(""Cleanup failed: "" + e.getMessage(), e);"
"<line3>    log.trace(""Calling onSubscribe of subscriber"");"
<line3>      logger.info(msg);
"<line2>    LOG.info(""finished node: "" + node + "" depth: "" + depth);"
"<line2>    logger.debug(String.format(""Skipped '%s' because idempotent and exists in the machine."", id));"
"<line3>    LOG.trace(""Using NoOp Implementation for Adapter CORE X12 Doc Submission Service"");"
"<line8>      LOG.info(""Unable to get read operation."", exn);<line20>      LOG.debug(""Does not have exactly one grpcWRite and bundleProcess operation."", exn);"
"<line33>        logger.info(""Exception while registering CQ on server. CqName : {}"", cQuery.getName());<line40>      logger.debug(""Successfully created CQ on the server. CqName : {}"", cQuery.getName());"
"<line7>      LOGGER.debug("""", ex);"
"<line37>            log.info(StringBundler.concat(""Ignoring group "", newValue, "" because it "", ""cannot be converted to scope""));<line54>          log.info(StringBundler.concat(""Ignoring scope "", newValue, "" because the "", ""referenced group was not found""),noSuchGroupException);<line57>          log.info(StringBundler.concat(""Ignoring scope "", newValue, "" because the "", ""referenced layout was not found""),noSuchLayoutException);<line60>          log.info(StringBundler.concat(""Ignoring scope "",newValue,"" because the "",""referenced parent group no longer allows sharing "",""content with child sites""),principalException);"
"<line27>        log.info(""Protofile "" + protofile + "" registered."");"
"<line13>            log.error(""Failed to get service: "" + serviceClass + "", "" + e.getMessage());<line16>            log.debug(""The component exposing the service "" + serviceClass + "" is not resolved"");"
<line6>      log.info(col);
"<line6>        logger.warn(""{} sending command while socket not connected: {}"",this.getClass().getSimpleName(),cmd);<line8>        logger.trace(""{}: sendCommand: {}"", this.getClass().getSimpleName(), cmd);<line10>      logger.warn(""{}: cannot send command"", this.getClass().getSimpleName(), e);"
"<line22>            LOG.info(""Error in pingAsyncAsync() {}"", t.getMessage());"
"<line4>    log.info(""DP is:\n"" + plan.toString());"
"<line14>      logger.debug(""exception while formatting :: {}"", e.getMessage(), e);"
"<line4>      log.info(""no transferControlBlock provided, building a default version"");<line7>      log.info(""creating a default transferOptions, as none specified"");"
"<line1>    logger.debug(""Station {}: Update name={} and location={} for MAC {}"",station,name,location,macAddress);<line4>      logger.debug(""Unable to process info event: {}"", e.getMessage());"
"<line28>        log.debug(""Loading experimentOptParamDef to the command object for editing."");"
"<line20>        log.info(""Deleting background task "" + backgroundTask.toString());"
"<line10>      logger.error(""InternalServerErrorException in rollbackVfModule"", e);"
"<line8>      logger.debug(""Can't find snapshot; deleting it in DB"");<line23>          logger.debug(""Failed to change snapshot state: "" + e1.toString());<line28>      logger.debug(""Failed to set the state to destroying: "", e);<line45>      logger.debug(""Failed to delete snapshot: "", e);<line48>        logger.debug(""Failed to change snapshot state: "" + e.toString());"
"<line1>    log.debug(""getConcept() == {}"", getConcept());"
"<line5>      logger.error(""Failed to initialize data grid: {}"", e);"
"<line8>    LOG.info(""Spilling to file location ""+ writeOnlyFile.getAbsolutePath()+ "" in host (""+ InetAddress.getLocalHost().getHostAddress()+ "") with hostname (""+ InetAddress.getLocalHost().getHostName()+ "")"");"
"<line2>    LOGGER.debug(""removedPendingHpcJob: connection: "" + hpcAccount.getConnectionName());<line2>    LOGGER.debug(""removedPendingHpcJob: algorithm: "" + hpcJobInfo.getAlgoId());<line2>    LOGGER.debug(""removedPendingHpcJob: status: "" + hpcJobInfo.getStatus());<line2>    LOGGER.debug(""removedPendingHpcJob: pid: "" + hpcJobInfo.getPid());"
"<line24>        LOGGER.debug(""monitoring from ""+ req.getRemoteAddr()+ "", request=""+ req.getRequestURI()+ (req.getQueryString() != null ? '?' + req.getQueryString() : """")+ "", application=""+ application+ "" in ""+ (System.currentTimeMillis() - start)+ ""ms"");"
"<line40>    log.debug(""createGroup start"");"
"<line2>      logger.error(""Cannot invoke signing without signature token. Add 'withSignatureToken()' method call or""+ "" call 'buildDataToSign() instead.'"");"
"<line8>            log.trace(StringUtils.replacePrms(""Waiting for unlocked ({0}) key={1}, contextInfo={2}..."",Thread.currentThread().getName(), getterId.get(bean), contextInfo));<line11>            log.trace(StringUtils.replacePrms(""Waiting ended ({0}) key={1}, contextInfo={2}..."",Thread.currentThread().getName(), getterId.get(bean), contextInfo));"
"<line5>      LOG.error(""Could not run completion of exchange {}"", exchange, e);"
"<line23>                  logger.trace(""HeliosVentilation: Read from serial port: {}"",String.format(""%02x %02x %02x %02x"", frame[1], frame[2], frame[3], frame[4]));<line27>                  logger.trace(""HeliosVentilation: Read frame with not matching checksum from serial port:""+ "" {}"",String.format(""%02x %02x %02x %02x %02x %02x (expected %02x)"",frame[0], frame[1], frame[2], frame[3], frame[4], frame[5], sum));<line31>            logger.debug(""Error reading from serial port: {}"", e1.getMessage(), e1);"
"<line6>        log.debug(""Unable to retrieve generic type of field: "" + f, e);"
"<line13>        log.debug(field.getProperty() + "": "" + msg);"
"<line16>            logger.debug(""Add table cache for database {}"", database);"
"<line12>          LOGGER.error(""User "" + username + "" was not found"");<line13>          LOGGER.error(""Error finding user "" + username, e);"
<line41>      LOG.warn(ex);
"<line1>    log.debug(""Elect a new leader now."");<line2>    log.debug(""Leader election completed."");"
"<line28>        LOG.debug(""Found ExpiredObject: "" + expiredObject.getKey());<line30>        LOG.error(""ExpiredObject not found: "" + key);<line33>      LOG.error(""Failed to find ExpiredObject: "" + key + "". Error: "" + e.getMessage(), e);"
"<line3>      log.debug(""[DSU] skip "" + xpp.getName());"
"<line4>        LOGGER.debug(""Entry has no 'updated' element, won't process"");<line23>        LOGGER.error(""Attempt to delete an item which didn't exist... its ok"");<line26>      LOGGER.error(e);"
"<line4>      LOG.trace(""Can't calculate percentage value for entity {} as current from producer {} is null"",entity,producer);<line8>      LOG.trace(""Can't calculate percentage value for entity {} as total from producer {} is null"",entity,producer);<line13>      LOG.trace(""Can't calculate percentage value for entity {} as total from producer {} is zero"",entity,producer);<line16>      LOG.trace(""Can't calculate percentage value for entity {} as current ({}) or total ({}) from""+ "" producer {} is negative"",new Object[] {entity, currentDouble, totalDouble, producer});"
"<line9>      logger.trace(""Unable to parse sensor definition: {}"", json, e);"
"<line4>      logger.error(""An error was produced during ""+ VFSPipelineExecutorRegistry.class.getName()+ "" directories initialization."",e);"
"<line4>        LOG.debug(""Received layout update for table {}: {}."",mTableURI,Bytes.toStringBinary(layoutUpdate));<line8>      LOG.info(""Tracked table layout node for table {} has been removed. Tracking will cease."",mTableURI);<line9>      LOG.error(""Unrecoverable ZooKeeper error: {}"", ke.getMessage());"
"<line5>      LOG.debug(""Starting SNMP Trap producer on {}"", this.endpoint.getAddress());<line13>      LOG.debug(""SnmpTrap: getting pdu from body"");<line22>      LOG.debug(""SnmpTrap: sending"");<line23>      LOG.debug(""SnmpTrap: sent"");"
"<line4>      log.error(""Error while saving filter."", e);"
<line8>      log.error(exception, exception);
"<line2>    logger.info(name.getMethodName() + """");<line48>    logger.info(name.getMethodName() + "" - OK"");"
"<line2>    LOGGER.debug(""Response has been sent !!"");<line12>      LOGGER.info(""Trace Info: request handler processing time : {}, send response latency: {}, total time""+ "" to handle request: {}"",_lastProcessingLatency.getLatencyMs(),_lastSendResponseLatency.getLatencyMs(),totalQueryTime);"
"<line2>    log.info(""Testing log resource"");"
"<line9>        LOGGER.debug(""File {} was removed from mining"", fileName);"
"<line7>          LOG.debug(""Client cancels: "" + id);<line12>          else LOG.debug(""Unmatched cancel notification for request id "" + id);<line14>          LOG.warn(""Cancellation support is disabled, since the '""+ MessageJsonHandler.CANCEL_METHOD.getMethodName()+ ""' method has been registered explicitly."");<line16>        LOG.warn(""Missing 'params' attribute of cancel notification."");"
"<line26>      logger.error(""Error detected during user authentication"", e);"
"<line2>    log.info(""waiting "" + synchronizingDelay + "" millis for Proxy undeployment in worker"");<line9>        log.info(""Proxy UnDeployed in "" + time + "" millis in worker"");"
"<line15>      log.error(""Error generating CND of "" + def, e);"
"<line6>      LOGGER.error(""Error doing {} init"", SiegfriedPlugin.class.getName(), e);"
"<line17>      LOG.debug(""Error occurred: "", e);"
<line2>      log.fatal(MessageFormat.format(message.toString(), args), t);
"<line27>          log.error(""Invalid state request: {}"", mAppearance);"
"<line3>    LOG.info(""call for oneway ping"");"
"<line3>    LOG.debug(""Assigning entity {} to owner {}..."", entityIdentity, ownerName);<line9>    LOG.info(""Assigned entity {} to owner {}."", entityIdentity, ownerName);"
<line56>      LOG.warn(msg, e);<line60>      LOG.info(msg, e);
"<line7>        LOGGER.error(""File not readable %s"", filename);<line8>    } else LOGGER.error(""File not found %s"", filename);"
"<line14>    LOG.debug(""{} publishing value {} (original sensor value {}) for mapped sensor {}, of entity {}"",new Object[] {this, newVal.get(), sensorVal, mappedSensor, entity});"
"<line2>    logger.debug(""initializing handler for thing {}"", getThing().getUID());<line18>                    logger.trace(""Powermax job..."");<line26>                    logger.warn(""Exception in scheduled job: {}"", e.getMessage(), e);"
"<line48>      log.debug(""Another Health query {} "", healthQuery);"
"<line6>      LOG.debug(""get additional info auth-id: {}, device: {}@{}"",authId,device.getDeviceId(),device.getTenantId());<line8>    LOG.debug(""get no additional info"");"
"<line2>    log.error(""Recording import failure"", error);"
"<line3>      log.debug(""Executing applications synchronization task"");"
"<line9>      logger.warn(""Cannot correctly name transaction for method {} because JobExecutionContext is null"",signature);<line13>      logger.debug(""Not creating transaction for method {} because there is already a transaction running""+ "" ({})"",signature,active);"
"<line25>      LOGGER.warn(""File {} yet exists, overwrite."", filename);<line27>    LOGGER.info(""Writing to file {} ..."", filename);"
"<line2>    Log.debug(""Test"");"
"<line5>    log.debug(""exists (then evict): "" + exists);"
<line7>      log.error(exception, exception);
"<line3>      log.info(""Connecting to JMX directly on the JVM"");<line13>      log.info(""Connecting using Attach API"");<line15>    log.info(""Connecting using JMX Remote"");"
"<line9>        log.error(""Failed to marshal deployment error, err="" + th, e);"
"<line4>      logger.warn(""No MD4 digest provider found !"");"
"<line4>      LOG.trace(knowledgeId);<line16>            LOG.trace(key + "" = "" + value + ""  ("" + value.getClass().getName() + "")"");<line23>          LOG.trace(PropertyUtil.reflectionToString(stock));"
"<line3>      log.debug(""Member attribute changed: [""+ memberAttributeEvent.getKey()+ ""] ""+ memberAttributeEvent.getValue());"
"<line6>      logger.debug(""could not get '{}' value from scim resource as an array"", ResponseCodeConstants.SCHEMAS);"
"<line10>    logger.info(""Trying to set inconsistent user '"" + u + ""' domain to '"" + d + ""'."");"
"<line1>    LOG.info(""Testing self kill on lost contact."");<line11>      LOG.error(""Interrupted the timer for 1 sec."", e);<line19>      LOG.error(""Interrupted Exception."", e1);<line20>      LOG.error(""ExecutionException Exception."", e1);<line21>      LOG.error(""TimeoutException Exception."", e);"
"<line7>    LOG.info(""Finished deleting all ledgers so waiting for the GC thread to clean up the entryLogs"");"
"<line14>    LOG.debug(""STDOUT: "" + stdout.getAbsolutePath());<line15>    LOG.debug(""STDERR: "" + stderr.getAbsolutePath());<line15>    LOG.debug(""DEBUG: cmd: "" + cmd);<line20>        LOG.debug(""DEBUG: waiting for Client at "" + serverSocket.getLocalSocketAddress());<line22>        LOG.debug(""DEBUG: Client connected! - start BinaryProtocol!"");<line31>        LOG.error(""PipesApp Error: "" + inputLine);"
"<line2>    LOG.info(""Instantiating {}"", this.getClass().getSimpleName());"
"<line14>          LOG.debug(""Rejected on attempt to queue message dispatch"", rje);"
"<line7>        LOGGER.debug(""TaskService.updateTask() for TaskId={} INSERTED an Attachment={}."",newTaskImpl.getId(),attachmentImpl);"
"<line3>          logger.warn(""Generic error warning."");"
"<line26>      LOG.debug(""Using SPN: {}"", spn);"
"<line27>          log.info(""Scenario '""+ tc.getScenario()+ ""' skipped due to constraints ""+ Arrays.toString(constraint.value()));"
<line19>      log.error(systemException, systemException);
"<line39>                logger.error(""[buildDasColumn executeQuery Exception] --> ""+ ""the exception message is:""+ e1.getMessage());<line44>      logger.error(""[buildDasColumn Exception] --> "" + ""the exception message is:"" + e.getMessage());"
"<line5>      log.warn(""admin has already joined the language [{}]"", localeId);"
"<line9>        log.debug(""Unregistered killer hook: {}"", killerHook);<line10>        log.debug(""Could not unregister killer hook: {}"", e);"
"<line17>    logger.debug(""Update VF Module command attempted but not supported"");"
"<line13>    log.debug(query);<line23>      log.trace(""Replacing record"");<line23>      log.debug(""oldValue: "" + oldStr);<line25>      log.debug(""newValue: "" + newStr);<line28>    log.debug(""Modifying "" + Integer.toString(modifyCounter) + "" Records."");<line30>    log.debug(""Removing old data:\n"" + deleteQ);<line31>    log.debug(""Inserting updated data:\n"" + insertQ);"
"<line18>        LOGGER.debug(""ignoring axis, can not be built: {} {}{}"",nameStep.getAxis(),prefix.isEmpty() ? """" : prefix + "":"",name);"
"<line9>    logger.trace(""Current inFlight = {}, {} connections needed, {} connections available, trashing {}"",currentLoad,needed,actual,toTrash);"
"<line15>          logger.error(""{} can not be deleted."", file, e);"
"<line15>      logger.error(""StudyDAOImpl - getComprehensionTestQuestionList() - Error"", e);"
"<line2>    LOG.info(""Connector config keys: {}"", String.join("", "", configProps.keySet()));"
"<line1>    logger.warn(""org.apache.kylin.storage.hbase.util.StorageCleanupJob is deprecated, use""+ "" org.apache.kylin.tool.StorageCleanupJob instead"");"
"<line2>    LOG.debug(""Get file length. COS key: {}"", key);<line13>      LOG.error(errMsg);"
"<line1>    LOGGER.debug(""Storing message {} into outgoing after {} retries"", mail.getName(), retries);"
"<line1>    log.debug("""");"
"<line6>        LOGGER.error(""Destroying datasource"", e);"
"<line32>      logger.error(""error saving account to cloud_usage db"", ex);"
"<line23>        LOG.debug(""Dropping {} because no topic is specified."", jsonMessage);"
"<line6>    LOGGER.info(""Incoming SendNotificationRequest for organisation: {} device: {}."",organisationIdentification,request.getNotification().getDeviceIdentification());"
"<line1>    LOGGER.trace(""Mapping date '{}' to the apropriate month."", strDate);<line12>      LOGGER.error(""Could not parse the given date: {}"", ex);"
"<line9>        log.error(""Error compiling pattern: "" + regExpKey);<line17>        log.error(""Error compiling pattern: "" + regExpValue);<line47>          log.error(""Value extracted is not a number: "" + sValue);<line50>        log.warn(""No data found for regExpKey: "" + regExpKey + "" and regExpValue: "" + regExpValue);"
"<line2>      log.info(""Confirmation servlet factory is not available, skipping its deploymnet"");<line4>    log.info(""Deploing confirmation servlet"");"
"<line5>      logger.debug(""Handling command [{}]"", command.getCommandName());"
"<line4>      LOG.debug(""Ignoring a duplicate slot request with allocation id {}."",slotRequest.getAllocationId());"
"<line5>    logger.debug(""-----deleteGroup--- , Group Name: {}"", groupName);"
"<line5>      logger.info(""Invalid payload {}"", value, e);"
<line8>      LOGGER.error(e.getMessage(), e);
"<line9>        logger.warn(""KeyValue source {} of {} returned invalid value {}"",_keySource.getUuid(),getUuid(),item);"
"<line21>        LOGGER.debug(String.format(""Kubernetes cluster : %s is already stopped"", kubernetesCluster.getName()));<line26>        LOGGER.debug(String.format(""Kubernetes cluster : %s is getting stopped"", kubernetesCluster.getName()));"
<line27>            log.warn(errorMessage);
"<line9>        log.debug(""handled image error with type "" + onErrorType, e);"
"<line10>          logger.error(""TTransportException writing to internal frame buffer"", e);<line12>          logger.error(""Exception writing to internal frame buffer"", e);<line29>          logger.error(""TTransportException inside handler"", e);<line32>          logger.error(""TApplicationException inside handler"", e);<line35>          logger.error(""Exception inside handler"", e);<line43>          logger.error(""Exception writing to internal frame buffer"", ex);"
"<line22>    logger.debug(""wrote state snapshot"");"
"<line11>    LOG.debug(""Added mutation to the batch; size={}"", batchHelper.getBatchSize());"
"<line5>      log.warn(""Nested (i)frame depth exceeded limit: "" + MAX_FRAME_DEPTH);"
<line4>    log.debug(message);
"<line1>    LOG.trace(""Check if created by Stocator: {}"", objectName);<line13>          LOG.trace(""Object {} was created by Stocator"", objectName);"
"<line15>      LOG.error(""Failed to write token to cache File. "" + e.toString());"
"<line16>          log.trace(""{} spent {} outside home"", person, timeSpent);"
"<line46>      LOGGER.warn(""preparedInsertVarbinaryApi() only applies to TeradataJdbcDriver"");"
<line27>      log.error(systemException, systemException);
<line12>        log.debug(noSuchNodeException, noSuchNodeException);
"<line5>      log.debug(""[{}] Known colour {} (LL {},{}; pixel {},{})"", kvStoreType, hex, lat, lng, x, y);<line10>        log.error(""[{}] For colour {} (LL {},{}; pixel {},{}) the webservice gave zero locations."",kvStoreType,hex,lat,lng,x,y);<line11>        log.warn(""[{}] For colour {} (LL {},{}; pixel {},{}) the webservice gave zero locations."",kvStoreType,hex,lat,lng,x,y);<line14>      log.debug(""[{}] New colour {} (LL {},{}; pixel {},{}); remembering as {}"",kvStoreType,hex,lat,lng,x,y,joinLocations(locations));"
"<line3>      logger.debug(""Aborted {}"", this);"
"<line21>      logger.info(String.format(""Banning %s (%s) by request of %s"",kickUser.getNickname(), banIp, user.getNickname()));<line23>      logger.info(String.format(""Banning %s by request of %s"", banIp, user.getNickname()));"
"<line3>      log.warn(""PARTICIPANT {}: Trying to connect to a user without receiving endpoint ""+ ""(it seems is not yet fully connected)"",this.name);<line6>      log.debug(""PARTICIPANT {}: configuring loopback"", this.name);<line9>      log.warn(""PARTICIPANT {}: There is a sending endpoint to user {} ""+ ""when trying to create another one"",this.name,sender.getName());<line11>    log.debug(""PARTICIPANT {}: Creating a sending endpoint to user {}"", this.name, sender.getName());<line15>      log.warn(""PARTICIPANT {}: 2 threads have simultaneously created a sending endpoint for user {}"",this.name,sender.getName());<line17>    log.debug(""PARTICIPANT {}: Created sending endpoint for user {}"", this.name, sender.getName());<line25>        log.warn(""Receiving endpoint is released when trying to connect a sending endpoint to it"", e);<line26>        log.error(""Exception connecting receiving endpoint to sending endpoint"", e);<line33>                log.error(""Exception releasing WebRtcEndpoint"", cause);"
"<line14>          logger.debug(""Waiting thread is now locking version generation for {} region {} RVV {}"",locker,regionPath,System.identityHashCode(this));<line23>          logger.fatal(""Request from {} to block operations found that operations are already blocked by""+ "" member {}."",new Object[] {locker, lockOwner});"
<line6>      logger.debug(HANDLER_IS_NULL);
<line17>                log.debug(e.getMessage(), e);
<line18>      log.error(systemException, systemException);
"<line3>      LOGGER.debug(""Attempting to delete temporary files from `{}`"", tempFileLocation.toString());<line7>          LOGGER.warn(""Failed to delete the Archive Analyzer's temporary files from `{}`, ""+ ""see the log for more details"",tempFileLocation.toString());"
"<line5>    log.info(Color.GREEN + ""Calendar_3_8 : empty column sunday"" + Color.NORMAL);"
"<line2>      logger.error(""Failed to execute query."");"
"<line14>            logger.info(""### Create Client. ###"");<line27>                  .info(""### Successfully Created Region on Client :"" + regions[i]);"
"<line8>    log.info(""Incoming clear alarm register request for meter: {}"", request.getDeviceIdentification());<line22>      log.error(""Exception: {} while sending clear alarm register request of device: {} for organisation""+ "" {}."",e.getMessage(),request.getDeviceIdentification(),organisationIdentification);"
"<line1>    logger.info(""called"");"
"<line7>      LOGGER.debug(""POST"");<line35>      LOGGER.debug(""Create Response id [{}]"", id);<line35>      LOGGER.debug(""Entry successfully saved, id: {}"", id);<line36>        INGESTlogger.info(""Entry successfully saved, id: {}"", id);<line44>      LOGGER.info(exceptionMessage, e);<line47>      LOGGER.info(errorMessage, e);"
"<line3>        logger.info(""Deregister processor {}, do nothing because this router doesn't have registered""+ "" processor"",new Object[] {processor});<line4>        logger.info(""Deregister processor {}, do nothing because this router is assigned to different""+ "" processor {}"",new Object[] {processor, this.processor});<line15>                logger.warn(""Failed to disconnect session {} due to {}"", sessionId, e, e);"
"<line3>    LOGGER.info(""save the results of the test suite to the table 'sakuli_suites'"");<line4>    LOGGER.debug(""write the following values to 'sakuli_suites': ""+ tcParameters.getValues()+ "" ==>  now execute ...."");<line8>    LOGGER.debug(""SQL-Statement for update 'sakuli_suites': "" + sqlStatement);<line9>    LOGGER.info(""update 'sakuli_suites' affected "" + affectedRows + "" rows"");"
<line18>      logger.debug(sql.toString());
"<line4>        log.warn(""Event type by name '""+ eventTypeName+ ""' was not found for property '""+ propertyName+ ""'"");<line7>        log.warn(""Event type by name '""+ eventTypeName+ ""' is not an XML event type for property '""+ propertyName+ ""'"");"
"<line5>        log.debug(""put i="" + i);<line9>        log.debug(""get i="" + i + "":"" + c.get(String.valueOf(i)));<line13>      log.debug(""get i="" + i + "":"" + c.get(String.valueOf(i)));<line14>    log.debug(""size: "" + c.size());<line14>    log.debug(c);<line14>    log.debug(CacheManager.inst);"
"<line12>        logger.warn(""Created node object for {} that was not available on the network"", node);<line15>      logger.debug(""Inspecting new node {}"", node);<line18>        logger.warn(""ZDO_NODE_DESC_REQ FAILED on {}"", node);<line27>        logger.warn(""ZDO_POWER_DESC_REQ FAILED on {}"", node);<line35>        logger.warn(""Node {} removed from network because no endpoints have been discovered"", node);<line38>      logger.trace(""Inspecting existing node {}"", node);<line39>        logger.warn(""The device {} has been found again with a new network address #{} "",node,nwkAddress.get16BitValue());"
"<line11>      logger.info(""Expected error"", e);"
"<line23>    LOG.info(""Adding shutdown hook with kill in {} secs"", workerShutdownSleepSecs);"
"<line3>    logger.debug(""Phase {}"", this);"
"<line3>    logger.info(""Waiting for channel to reach agi "" + this);<line4>    logger.info(""Result of waiting for channel to reach agi "" + this + "" "" + tmp);"
"<line9>          logger.error(""error in hasReferencingObjects"", t);<line22>      logger.error(""error loading references for page {}"", page.getCode(), ex);"
"<line2>      log.debug(String.format(""Cache %s for file %s"", id, file));"
"<line8>            logger.debug(getString(""Tracing.4"",introspectedColumn.getActualColumnName(),entry.getKey().toString()));"
"<line3>    LOG.trace(""getCipher({}) -> {}"", transformation, safeClassname(result));"
<line15>        log.debug(searchException, searchException);
"<line10>                    logger.warn(""Interrupting test, it is taking too long{}Server:{}{}{}Client:{}{}"",System.lineSeparator(),System.lineSeparator(),server.dump(),System.lineSeparator(),System.lineSeparator(),client.dump());<line25>      logger.info(""{} requests in {} ms, {}/{} success/failure, {} req/s"",iterations,elapsed,successes,iterations - successes,elapsed > 0 ? iterations * 1000L / elapsed : -1);"
"<line12>              log.error(""Could not determine whether a field is indexed"", ex);"
"<line9>    log.info(""======== SERVER STARTED ========"");<line9>    log.info(""\n====\n%s\n===="", queryRunner.getCoordinator().getBaseUrl());"
"<line2>    LOG.trace(""Clearing composite phenomenon for observable property {}"", observableProperty);"
<line7>      log.error(exception, exception);
"<line5>          log.debug(""Removed Principal "" + principal.getName());"
"<line7>        LOG.info(""Starting service {}"", svc.getClass().getName());"
"<line14>                  LOGGER.trace(""unloading previous view, activate() returned different view on ""+ newModule);<line36>          LOGGER.trace(""Remove from scene graph view of module: "" + model.getActiveModule());"
"<line1>    log.debug(""attaching dirty StgMbGefaehrskatTxt instance"");<line3>      log.debug(""attach successful"");<line4>      log.error(""attach failed"", re);"
"<line1>    logger.debug(""xid = ["" + xid + ""] state = ["" + state + ""]"");"
"<line7>      log.info(""Bug [{}] already exists in backend, analysis will be skipped"", vulnId);<line33>          log.info(chg.toString());"
"<line2>    LOG.trace(""Webhook {} for {} failed after {}"",webhook.getUri(),update,JavaUtils.duration(start),t);"
"<line2>    LOG.trace(""Shutdown Droplet {} : [{}] "", dropletId, action);"
"<line1>    LOG.info(""Creating the embedded Kafka connect instance"");<line8>    LOG.info(""Using the following address for  the listener configuration: {}"", address);<line10>    LOG.info(""Adding the returned directories to the plugin path. This may take A VERY long time to""+ "" complete"");<line11>    LOG.info(""Building the embedded Kafka connect instance"");<line20>    LOG.info(""Built the embedded Kafka connect instance"");"
"<line49>    ResourcesResourceTest.LOG.debug(""try to retrieve configurations of resource '{}'"", updatedComplexResource.getUuid());"
"<line19>    logger.debug(""Subflow response code: {}"", responseCode);<line19>    logger.debug(""Subflow response: {}"", response);"
"<line4>      logger.error(""Failed to get buffered input stream for {}. "", filePath, e);"
"<line1>    log.debug(""reset"");"
"<line7>        log.info(""Interrupted. Stopping loop."");<line10>        log.error(""Error"", e);"
"<line2>    LOGGER.debug(""Iam client factory: {}"", iamClientProperties);"
"<line10>        logger.debug(""got object to update with ID: "" + knownResourceId);<line26>      logger.debug(""to be updated object"");<line26>      logger.debug(objectAsXmlString(conservationCommon, ConservationCommon.class));<line35>        logger.debug(testName + "": status = "" + statusCode);<line57>      logger.debug(""UTF-8 data sent=""+ conservationCommon.getFabricationNote()+ ""\n""+ ""UTF-8 data received=""+ updatedConservationCommon.getFabricationNote());"
"<line8>    log.info(""Running group management api test cases using jmeter scripts"");"
"<line12>    LOG.info(""Execute computer job: {}"", String.join(SPACE, command));"
"<line15>              LOG.trace(""Ignore forwarding '{}' because the leadership runner is no longer the valid""+ "" leader for {}."",forwardDescription,expectedLeaderId);"
"<line2>    LOG.info(""Terminating Reply Processor..."");<line4>    LOG.info(""\tReply Processor Disruptor shutdown"");<line7>      LOG.info(""\tReply Processor Disruptor executor shutdown"");<line8>      LOG.error(""Interrupted whilst finishing Reply Processor Disruptor executor"");<line10>    LOG.info(""Reply Processor terminated"");"
"<line1>    log.debug(""attaching dirty DtsPackageTxt instance"");<line3>      log.debug(""attach successful"");<line4>      log.error(""attach failed"", re);"
<line16>          log.error(sawException.getMessage(), sawException);
"<line5>    logger.debug(""Closing down entity objects...done"");"
"<line4>        logger.trace(""addBinding("" + binding + "") being called"");<line16>        logger.trace(""Adding binding "" + binding + "" into "" + this + "" bindingTable: "" + debugBindings());"
"<line72>      logger.error(""Unexpected error during processing {}"", e.getMessage(), e);"
"<line3>    logger.debug(""Defined rootProjectDirectory='{}'"", rootProjectDirectory);<line10>    logger.debug(""Top folder found, parent pom found at: '{}'"", lastMavenProjectDir);"
"<line2>    LOG.info(""QueryMaster Address:"" + queryMasterHostAndPort);<line22>      LOG.fatal(e.getMessage(), e);"
"<line5>      log.error(""Unable to obtain in progress knowledge base comments count ""+ ""for  group ""+ _kbSuggestionListDisplayContext.getGroupId(),portalException);"
"<line5>    logger.warn(MessageFormat.format(""Could not find any {0} for the given id \""{1}\""."",UnitDefinition.class.getSimpleName(), id));"
"<line1>    log.debug(""Setting service: "" + cls);<line22>        log.debug(""Service: "" + cls + "" set successfully"");"
<line1>    log.info(TIPS);
"<line4>      LOG.warn(""Unable to close "" + file, e);"
"<line10>      LOGGER.error(""Error occurred while creating password"", e);<line10>      Session.get().error(getString(""common.error.unexpected""));"
<line9>          log.debug(_NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey);
"<line2>    LOGGER.info(""OnMessage called '"" + message + ""'"");"
"<line4>    LOG.debug(""Indent options returned by ""+ provider.getClass().getName()+ "" for ""+ file.getName()+ "": indent size=""+ options.INDENT_SIZE+ "", use tabs=""+ options.USE_TAB_CHARACTER+ "", tab size=""+ options.TAB_SIZE);"
<line13>        log.debug(sb.toString());
"<line7>      logger.debug(""Skipping registration of service providers for "" + ExchangeAttributeBuilder.class);<line19>        logger.debug(""Skipping registration of service providers for ""+ ExchangeAttributeBuilder.class+ "" since no service descriptor file found"");"
<line7>      log.error(exception, exception);
"<line2>    logger.debug(""computing CBD of depth {} for {} ..."", resource, depth);<line5>    logger.debug(""CBD size: {}"", cbd.size());"
"<line4>        logger.trace(""Rollback called for "" + _name + "" when there's no transaction: "" + buildName());<line12>          logger.debug(""Rolling back the transaction: Time = ""+ (System.currentTimeMillis() - _txnTime)+ "" Name =  ""+ _name+ ""; called by ""+ buildName());<line18>      logger.warn(""Unable to rollback"", e);"
"<line10>              log.info(""Exporting address list."");"
<line31>      log.error(e.getMessage(), e);
"<line2>    logger.debug(""Getting Serial: "", super.getSerial());"
"<line5>    log.debug(""readFromBigQuery(tableId=%s, projectedColumns=%s, actualParallelism=%s, filter=[%s])"",tableId, projectedColumns, actualParallelism, filter);"
"<line2>      logger.debug(""Distributer unloading: "" + listeners.size());<line10>          logger.error(e.getMessage(), e);"
"<line7>      LOG.warn(""Unhandled exception: {}"", e.getMessage(), e);<line17>                LOG.debug(""Number of hits: "" + hits.getNumberOfHits());<line18>                  LOG.debug(""Hit "" + i + "" Index Location:"" + hits.getHit().get(i).getHitLocation());<line18>                  LOG.debug(""Hit "" + i + "" Score:"" + hits.getHit().get(i).getScore());<line18>                  LOG.debug(""Hit "" + i + "" Data:"" + hits.getHit().get(i).getData());"
"<line19>          LOGGER.warn(""Unable to close "" + file, e);"
<line4>      log.error(bundleException, bundleException);
"<line47>      logger.debug(""Received parent application name. {}"", parentApplicationName);<line87>      logger.warn(""ambiguous span found(bug). span:{}"", span);"
"<line5>      LOGGER.debug(""not in HTTPS, skipping filter."");<line9>      LOGGER.debug(""the user has already been authenticated, skipping filter."");<line16>        LOGGER.debug(""the user-agent matched and no Authorization header was sent, returning a 401."");<line18>        LOGGER.debug(""the user-agent does not match, skipping filter."");<line21>      LOGGER.debug(""Authorization header sent in the request, activating filter ..."");"
"<line2>    LOGGER.debug(""Serializing EncryptedExtensionsMessage"");"
<line5>    log.info(expectedJavascript);<line5>    log.info(generatedJavascript);
"<line3>      log.info(""Moving current data directory away to {"" + dataSourceFolder().getAbsolutePath() + ""}"");"
"<line9>      log.info(""Using custom cache event logger [{0}] for auxiliary [{1}]"", cacheEventLogger, auxPrefix);<line10>      log.info(""No cache event logger defined for auxiliary [{0}]"", auxPrefix);"
<line10>      LOG.warn(ex);
"<line2>    log.debug(""{} Setting timeout to {} ms"", this, timeoutMs);"
"<line13>            LOG.debug(""Null value for {}, ignoring"", schema.getKey());<line18>              LOG.warn(""Invalid value for attribute "" + schema.getKey() + "": "" + valueToPrint, e);"
"<line11>        log.warn(""Not balancing due to {} outstanding migrations."", migrations.size());<line12>          log.debug(""Sample up to 10 outstanding migrations: {}"",migrations.stream().limit(10).map(String::valueOf).collect(Collectors.joining("", "")));"
"<line8>      log.info(""Changing address of equipment {} to: {}"", equipment.getName(), tmpStr);<line13>        log.info(""Changing process id of equipment {} from {} to {}"",equipment.getName(),equipment.getId(),tmpStr);"
"<line11>      Log.warn(this, ""Rows = "" + rows + "" but only "" + indexed + "" indexes"");"
"<line14>      logger.debug(""Could not load appliance"", e);"
"<line13>          logger.error(""TTransportException writing to internal frame buffer"", e);<line15>          logger.error(""Exception writing to internal frame buffer"", e);<line28>          logger.error(""TTransportException inside handler"", e);<line31>          logger.error(""TApplicationException inside handler"", e);<line34>          logger.error(""Exception inside handler"", e);<line42>          logger.error(""Exception writing to internal frame buffer"", ex);"
"<line19>        logger.debug(""Query targeted local bucket not found. {}"", bme.getMessage(), bme);"
"<line5>      logger.info(marker,""{} {} {} of [{}] at [{}], unix [{}]"",operation.toString().toLowerCase(Locale.ENGLISH),entityName,marker.getName().toLowerCase(),entity.getUuid(),now,now.getEpochSecond());"
"<line6>      LOGGER.error(""Giving up on retrying watcher"", cause);"
<line9>      LOG.error(e.getMessage());
"<line9>    log.trace(""Updated in db session activity timestamp for "" + id);"
"<line5>    LOG.debug(""Writing file..."");<line6>    LOG.debug(""Writing file DONE..."");"
"<line6>      logger.error(""error creating (or modifying) counter"", t);"
"<line17>      LOG.debug(""createSession userId: {}, sessionId: {}, resultCode: {}"",user.getUserId(),rbacCreateSessionResponse.getSessionId(),rbacCreateSessionResponse.getLdapResult().getResultCode());"
"<line7>      LOGGER.error(""unexpected exception caught from service center task. "", e);"
"<line2>    log.warn(""Can't append '{}' to comment. SessionInfoService is not supported in {}"", st, jaggerPlace);"
"<line34>      LOGGER.error(""Malformatted sparse row in line "" + line + "" (closing bracket not found)."");"
"<line3>      logger.debug(""Checking if storage pool is suitable, name: ""+ pool.getName()+ "" ,poolId: ""+ pool.getId());<line6>        logger.debug(""StoragePool is in avoid set, skipping this pool"");<line14>          logger.debug(""StoragePool's Cluster does not have required hypervisorType, skipping this pool"");<line21>        logger.debug(""StoragePool does not have required hypervisorType, skipping this pool"");<line57>        logger.warn(String.format(""Could not verify storage policy complaince against storage pool %s due to""+ "" exception %s"",pool.getUuid(), e.getMessage()));"
"<line13>      logger.error(""Cannot save the partition table"", e);<line18>        logger.warn(""Old partition table file is not successfully deleted"", e);<line21>      logger.warn(""New partition table file is not successfully renamed"");<line22>    logger.info(""Partition table is saved"");"
"<line13>          log.info(""Cluster message received to Refresh the cache, but uanble to as it is still being""+ "" populated."");"
"<line6>      logger.error(""Error while adding a role"", t);"
"<line9>          LOGGER.warn(""Invalid name for enumeration: {}, name: {}"", enumeration.getName(), name);<line10>          LOGGER.warn(""Error parsing enumeration: {}, name: {}"", enumeration.getName(), name);<line12>        LOGGER.warn(""Could not convert Solr object to enumeration: {}, unsupported class: {}"",enumeration.getName(),object.getClass().getName());"
"<line4>      log.trace(""#MULTI_SIG# sign credential: {}"", credential);<line13>      log.trace(""#MULTI_SIG# sign multi-signature: {}"", multiSigData.getHexString());"
"<line21>          log.debug(""Failed to close socket"", ioe);<line27>            log.debug(""session established ok with "" + this.address);"
"<line5>    logger.info(""Found multiple message senders: {}."", names.stream().collect(Collectors.joining("", "")));"
"<line2>    log.debug(""StreamWorkManagerTest.onlyLastCoalescingWorkShouldBeExecuted() beginning"");<line15>    log.debug(""StreamWorkManagerTest.onlyLastCoalescingWorkShouldBeExecuted() ending"");"
"<line12>      LOGGER.error(""Failed to insert pattern. Maybe too long with a length of "" + axiomString.length() + ""?"",e);"
<line10>    log.error(msg);
<line18>      LOG.error(msg);<line24>          LOG.error(I18n.err(I18n.ERR_04114_CURSOR_CLOSE_FAIL), e);
"<line2>      logger.debug(""Invalidate: Entry already invalid: '{}'"", event.getKey());"
"<line6>      logger.warn(""redirectAfterValidation parameter may not be true when useSession parameter is false.""+ "" Resetting it to false in order to prevent infinite redirects."");"
"<line1>    log.info(""Installing "" + getEntity() + "" using couchbase-server-{} {}"",getCommunityOrEnterprise(),getVersion());"
"<line1>    log.info(""name:{},location:{}"", name, location);"
"<line8>        LOG.warn(logMsg, e);<line9>        LOG.info(logMsg);<line10>        LOG.info(logMsg, e);<line18>        LOG.warn(""Large response size "" + buf.size() + "" for call "" + call.toString());<line22>      LOG.info(this.getClass().getName() + "" caught: "" + StringUtils.stringifyException(e));"
"<line3>      LOG.debug(""Extra value is null, throw away it."");"
"<line11>          LOGGER.warn(""Map parameter (name: {}, operation ID: {}) not yet supported"",p.paramName,op.operationId);<line30>            LOGGER.warn(""Map response (operation ID: {}) not yet supported"", op.operationId);"
"<line5>    log.info(Color.GREEN + ""Calendar_2_6 : missing column friday"" + Color.NORMAL);"
"<line2>    LOG.info(""Invoke HTTP Put request"");<line8>    LOG.info(""HTTP PUT request {}"", mUrl.toString());<line13>    LOG.info(""HTTP PUT response {}. Response code {}"", mUrl.toString(), responseCode);<line17>      LOG.warn(""Token recreated for {}.  Retry request"", mUrl.toString());"
"<line30>          log.error(""Unable to retrieve individual search result for "" + className, exception);<line34>      log.error(""Unable to display content for "" + className, exception);"
"<line5>      logger.debug(""Failed to parse peer info for SSL engine initialization"", e);"
"<line9>        logger.debug(""Query line: "" + line);"
"<line2>    log.info(""------  testNumericAndRange  ------"");"
"<line15>      LOGGER.warn(""At upper bound on partition.  Increase the bounds or condense the data."");"
"<line20>                  LOGGER.trace(""An error accurred while retrieving stats for pod {}.{}: {}"",pod.getMetadata().getNamespace(),pod.getMetadata().getName(),line.substring(0, line.indexOf(""#failed"")));<line31>      LOGGER.debug(""An error accurred while retrieving stats for pod {}.{}: {}"",pod.getMetadata().getNamespace(),pod.getMetadata().getName(),ex.getMessage());"
"<line4>      LOG.info(""Deleted {} repository {}."", repoEntry.getType(), reponame);"
"<line7>    logger.info(PRINT_BORDER + ""shuffleStations: "" + oldStation + "" -> "" + currentStation + PRINT_BORDER);"
"<line21>    logger.debug(""Mocking the console input for drop id: {}"", dropId);"
"<line23>        logger.error(""Error when attempting to format ID '"" + id + ""' using formatting pattern '"" + pattern);<line25>        logger.error(""Formatted ID '""+ formattedID+ ""' exceeds maximum length of ""+ getMaxOutputLength()+ "" characters.""+ ""Returning ID without formatting."");"
"<line7>    logger.debug(""Required dump report -> code {}"", reportCode);<line8>    logger.debug(""Extracted dump report -> {}"", result);"
"<line2>      logger.debug(""getConnectionAsync("" + intent + "")"");"
"<line2>      log.debug(""Initializing verification "" + verifyProcessClassName);<line7>        log.debug(""Running verification "" + verifyProcessClassName);<line10>        log.debug(""Finished verification "" + verifyProcessClassName);<line13>      log.error(verifyProcessClassName + "" cannot be found"", classNotFoundException);<line14>      log.error(verifyProcessClassName + "" cannot be accessed"", illegalAccessException);<line15>      log.error(verifyProcessClassName + "" cannot be initiated"", instantiationException);"
<line1>    logger.debug(msg, thr);
"<line21>            logger.debug(""Read Called! Item: ["" + item + ""]"");<line30>            logger.debug(""Write Called! Item: ["" + item + ""]"");"
"<line2>    logger.debug(""{} - {}"", t.getMessage(), t);<line3>      logger.info(""downstream connection lost with agent: {}"", getDisplayForLogging(agentId));"
"<line11>      LOG.info(""Retrieved count of {}"", numberOfRecords);"
"<line10>        logger.error(""Failed to get CustomScriptConfiguration. acr: '{}'"", this.authAcr);<line15>        logger.info(""Authentication result for user '{}', result: '{}'"", credentials.getUsername(), result);"
"<line38>      LOGGER.error(""Could not scan for "" + getProbeName(), E);"
<line4>      log.error(e.getMessage(), e);
"<line11>        logger.debug(testName+ "": Role with name \""""+ knownRoleName+ ""\"" should already exist, so this request should fail."");<line11>        logger.debug(testName + "": status = "" + statusCode);<line11>        logger.debug(testName + "": "" + res);"
"<line2>    log.info(""Killing job {}. {}"", jobId, out);"
"<line5>      LOG.warn("" + {} ObjectNames registered in domain \""{}\"""", s.size(), d);<line6>        LOG.warn("" |  "" + on);"
"<line6>    logger.info(name + "": "" + total + "" from "" + results.size() + "" worker threads"");"
"<line9>        LOGGER.warn(""Address resover key:{}'s value:{} is not positive, please check!"", key, val);"
"<line3>        LOG.info(""db connection reconnect.."");<line7>      LOG.error(""check connection open failed.."", e);<line8>      LOG.error(""load jdbc class error when reconnect db.."", e);<line9>      LOG.error(""jdbc io exception.."", e);"
<line71>      log.error(systemException, systemException);
"<line2>    LOGGER.info(""Logical switches returned to client: {}"", logicalSwitches.toJsonString());"
"<line7>      log.debug(""Rebalancing job: {} maxTasksToMove={} maxPerHost={}"",job.getId(),maxTasksToMove,maxPerHost);<line17>        log.debug(""hostsSorted.size={} hostWithMost:{} hostWithLeast:{} mostTasksOnHost: {}""+ "" leastTasksOnHost: {}"",hostsSorted.size(),hostWithMost,hostWithLeast,mostTasksOnHost,leastTasksOnHost);"
<line14>      log.error(exception, exception);
"<line3>      log.debug(""HostObjectComponent activated"");"
"<line15>    log.info(""removed="" + removedStopTimeCount + "" total="" + totalStopTimeCount);"
"<line4>      LOG.debug(""Removing "" + old + "" when adding "" + policy + "" to "" + AbstractEntity.this);"
"<line19>            log.debug(""OAuth2 Keycloak exchange succeeded."");<line23>            log.error(""Access Token Error: {0}."", tokenResult.cause().getMessage());"
"<line9>      logger.warn("""", fex);"
"<line11>      LOG.debug(""retrieving document("" + documentId + "","" + user.getPrincipalName() + "")"");"
"<line7>      log.error(""Failed to find address."");<line13>          log.error(""Failed to close socket."");"
"<line4>      log.info(""Creating models directory {}"", dir);<line6>    log.info(""Saving DL4J computation graph model to {}"", f.getAbsolutePath());<line12>    log.info(""Model saved: {}"", f.getAbsolutePath());"
"<line6>      LOG.debug(readNext.getKey().get() + "" / "" + readNext.getValue().toString());"
<line14>        log.debug(sb.toString());
"<line3>      log.debug(""@PreDestroy - Closing all active queries and query logics before shutdown."");<line3>      log.debug(""Overriding idle and call time thresholds to zero so that all queries and logics""+ "" resources are cleared before shutdown."");"
"<line3>    logger.info(""{}"", grpcTransportConfig);"
"<line7>          logger.debug(""Could not find button component {} for id {}"", component, integrationID);<line10>        logger.debug(""Device to button map not populated"");"
"<line5>      LOGGER.info(""Number of metrics registry: {}"", metricsRegistries.size());"
"<line6>    logger.info(name + "": "" + total + "" from "" + operationCounterList.size() + "" worker threads"");"
"<line14>    LOGGER.info(""Executing VersionCommand..."");<line20>      LOGGER.error(""Exception encountered executing command"", e);"
"<line1>    logger.info(String.format(""Extracting '%s' to '%s'"", zipFile.getAbsolutePath(), destination.getAbsolutePath()));"
"<line2>      log.debug(""Splits PeriodicFetching is running..."");<line4>    log.debug(""Starting PeriodicFetching Splits ..."");"
"<line7>          LOG.error(""Delete by Query is not supported for the Queries which didn't specify Query keys""+ "" with fields."");<line21>        LOG.debug(""Delete by Query was applied : "" + results.wasApplied());<line22>      LOG.info(""Delete By Query method doesn't return the deleted element count."");"
<line11>        log.error(message);<line11>        log.debug(message, e);
"<line12>      logger.warn(""error deserializing event message"", e);"
"<line9>          log.info(""[{}] dispatcher reached to max unack msg limit on blocked-broker {}"",dispatcher.getName(),dispatcher.getTotalUnackedMessages());"
"<line2>    logger.trace(""createOutputStream: {}"", arg0);"
"<line2>    logger.debug(""**** Cm11aApplianceHandler handleCommand command = {}, channelUID = {}"",command,channelUID.getAsString());<line5>      logger.debug(""Unable to handle command. Bridge is null."");<line19>        logger.info(""Received REFRESH command for switch {}"", houseUnitCode);<line24>        logger.debug(""Received invalid command for switch {} command: {}"", houseUnitCode, command);<line26>      logger.debug(""Attempted to change switch to {} for {} because the cm11a is not online"",command,houseUnitCode);"
"<line3>      LOGGER.debug(""[{}] Cancelled SuspicionTimeoutTask for {}"", localMember, memberId);"
"<line12>        logger.warn(""Error deleting settings temp file: "" + file, e);"
<line3>        LOG.debug(message);<line5>        LOG.warn(message);
"<line6>      log.info(""Resubmitting old job {} as {}"", oldID, newID);<line8>    log.info(""{} jobs has been resubmitted."", resubmitcount);"
"<line24>      LOGGER.error(""Cannot index representation: {}"", representation, e);"
"<line8>    log.debug(""runTestQueryWithUniqueness"");<line16>    log.debug(""query: "" + settings.getQuery());<line16>    log.debug(""logic: "" + settings.getQueryLogicName());"
"<line34>          log.trace(""Expected a but got "" + tm.getText());<line37>        log.trace(""*** calling acknowledge"");<line49>        log.trace(""calling recover"");<line54>          log.trace(""Expected b but got "" + tm.getText());<line61>          log.trace(""Expected c but got "" + tm.getText());<line69>      log.error(""Caught exception"", e);"
"<line5>      logger.warn(""Could not convert a Nuxeo integer value to its string equivalent: "" + cce.getMessage());"
"<line17>                log.error(""Waiting is interrupted."", e);"
"<line15>      logger.error(""No such method '{}' of class '{}'"", apiMethod.getSpringBeanMethod(), bean.getClass(), e);<line23>        logger.error(""Error invoking method '{}' of class '{}'"",apiMethod.getSpringBeanMethod(),bean.getClass());<line29>      logger.error(""Error invoking method '{}' of class '{}'"",apiMethod.getSpringBeanMethod(),bean.getClass(),t);"
"<line5>    LOGGER.warn(""Unexpected exception."", cause);"
"<line2>      LOGGER.debug(""Deleting ALL temporary files from `{}`"", tempDirectory.toString());"
<line2>    log.trace(msg);
"<line12>      logger.debug(""Element oy type {} removed fomr the list {}"",currentAttribute.getNestedAttributeTypeCode(),currentAttribute.getName());<line13>      logger.error(""error in removeListElement"", t);"
"<line6>      LOG.debug(""RP updated successfully. RP : {} "", rpObj);<line8>      LOG.error(""Failed to update RP: {} "", rp, e);"
"<line8>        log.warn(""Unable to load presets"", e);"
"<line4>      this.logger.warn(""Could not delete temp directory"");"
"<line4>    LOGGER.debug(""propertyName = {}"", expression.getPropertyName());<line6>    LOGGER.debug(""extractedSearchTerm = [{}]"", extractedSearchTerm);"
"<line3>        LOGGER.info(""Global transaction is disabled."");"
"<line34>      logger.error(""update process definition json workergroup error"", e);"
"<line24>                    log.error(""Unexpected error: "" + e, e);"
"<line5>      log.debug(""After encoding"" + ((Command) msg).toString());"
"<line2>    logger.debug(""Toolbar switch"");<line4>      logger.warn(""Target document doesn't exist or is not a signal"");"
"<line14>        log.debug(""ClientService request failed "" + server + "", retrying ... "", tte);"
"<line12>      LOG.error(""Cannot remove CustomizedStateConfig from cluster: {}, Exception: {}"", clusterId, ex);"
"<line3>      log.debug(""Validating usecase: {}"", usecase);<line8>      log.error(e.getMessage());"
"<line4>      logger.warn(""{}: Only PresetID >6 is allowed"", handler.getDeviceName());"
"<line3>    log.info(""creating CDI container for bean bundle {} with extension bundles {}"", bundle, extensions);"
"<line8>      LOGGER.warn(""Bad PolyLineRecord Detected and Discarded."");"
"<line9>          log.warn(String.format(""Missing web link for file %s"", file));<line24>            log.debug(String.format(""Add range header %s for file %s"", header, file));"
"<line2>      log.debug(""table:""+ metadataTableName+ "" created UpdateHdfs listener for table:""+ metadataTableName);<line19>                log.trace(""table:""+ metadataTableName+ "" stateHasChanged(""+ reader+ "", ""+ value+ "") for ""+ triStateName);<line28>                log.trace(""table:""+ metadataTableName+ "" stateChanged(""+ client+ "", ""+ newState+ "")"");<line34>      log.error(e);"
"<line5>        logger.debug(""Could not add command to command queue because queue is already full. Maybe SolarEdge""+ "" is down?"");<line6>        logger.warn(""Could not add command to queue - IllegalStateException"");"
"<line7>        log.debug(""Starting..."");<line14>        log.debug(""Started."");"
"<line1>    logger.debug(""Activating resource "" + resourceId + "" ..."");<line10>          logger.warn(""Error destroying protocol sessions"", e1);<line14>    logger.debug(""  Obtaining capabilities..."");<line17>    logger.debug(""  Capabilities obtained. Loading bootstrapper..."");<line21>    logger.debug(""  Bootstrapper loaded"");<line24>      logger.debug(""  Loading profile..."");<line25>      logger.debug(""  Profile loaded"");<line29>        logger.debug(""Resource activated: "" + resource.getResourceIdentifier().getId());<line33>      logger.debug(""Rolling back activation..."");<line40>          logger.warn(""Error destroying protocol sessions"", e);<line42>      logger.debug(""Rolling back done"");"
"<line17>          LOGGER.info(""param: "" + p.paramName + "" isReserved ââ> "" + paramNameAlternative);"
"<line6>                log.info(""Received: "" + exchange);<line30>                  log.info(""Throwing exception!"");"
"<line22>        LOG.info(""Request external resource {} with config key {}."",resourceQuantity.getAmount(),configKey);"
"<line2>    LOGGER.info(""Testing capabilities of FileImageInputStreamExt"");<line11>    LOGGER.info(""Testing capabilities of URLImageInputStreamSpi: SUCCESS!!!"");"
"<line13>      LOG.info(""performing apply model phase for view: "" + view.getId());<line29>    ProcessLogger.trace(""apply-model:"" + view.getId());"
"<line7>      logger.debug(""Current Sequence: {}"", currentSequence);<line17>      logger.error(""Exception in postProcessingExecuteBB"", ex);"
"<line1>    LOG.info(""{}: Restoring WebContainer for bundle {}/{}"", this, symbolicName, version);"
"<line7>    LOGGER.info(""Container destroyed"");"
"<line2>    log.debug(""Load annotation attribute {} = {} ({})"", name, value, value.getClass().getName());<line6>        log.debug(""Found plugin {}"", value);<line8>        log.debug(""Found plugins {}"", Arrays.toString((String[]) value));<line10>        log.debug(""Found plugin {}"", value.toString());<line14>      log.debug(""Found point "" + pointClassName);"
"<line8>      log.warn("""", ex);"
"<line5>      LOG.debug(""Request {} was successfully answered after {} ms."", request, durationMillis);<line7>      LOG.debug(""Request {} failed after {} ms"", request, durationMillis, future.cause());"
"<line36>        LOG.warn(""The provided home directory '{}' for namespace '{}' has group '{}', which is different""+ "" from the configured group '{}' of the namespace."",customNamespacedLocation.toString(),namespaceMeta.getNamespaceId(),groupName,namespaceMeta.getConfig().getGroupName());<line38>        LOG.warn(""The provided home directory '{}' for namespace '{}' has group permissions of '{}'. It""+ "" is recommended to set the group permissions to 'rwx'"",customNamespacedLocation.toString(),namespaceMeta.getNamespaceId(),permissions);"
"<line32>        log.warn(""Unable to get guest or current user ID"", principalException);"
"<line4>      this.logger.warn(""Could not find the Dashboard renderer for layout \"""" + layout + ""\"""");"
"<line6>      logger.info(""Unknown OWL entity type for: "" + entity);"
"<line2>    LOG.debug(""Lifecycle changed to {}"", lifecycle);"
"<line13>          log.warn(""Token "" + token + "" is invalid and was deleted"");<line16>          log.debug(exception, exception);"
"<line8>        logger.info(""Could not initiate event tracker from GII provider {}"", provider);"
"<line1>    log.debug(""finding StgMbMasGef instance by example"");<line8>      log.debug(""find by example successful, result size: "" + results.size());<line10>      log.error(""find by example failed"", re);"
"<line3>    logger.info(""jsonResponse: "" + jsonResponse);<line4>    logger.info(""emojisJSONArray.length(): "" + emojisJSONArray.length());"
"<line3>      LOG.debug(""["" + id_ + ""] fillText('"" + text + ""', "" + x + "", "" + y + "")"");"
"<line4>      log.warn(""Variables cannot be used in the 'plugin.path' property, since the property is ""+ ""used by plugin scanning before the config providers that replace the ""+ ""variables are initialized. The raw value '{}' was used for plugin scanning, as ""+ ""opposed to the transformed value '{}', and this may cause unexpected results."",rawPluginPath,transformedPluginPath);"
"<line38>      Log.error(""Failed to read help descriptionModel"", e);"
"<line7>      LOGGER.error(""Error while closing session"", e);"
"<line1>    LOG.debug(""Failed to capture snapshot %d for component %s"", componentId.getSnapshotId(), componentId);"
"<line5>    log.info(""MJpeg Frame grabber stop called"");<line8>      log.info(""Error closing mjpeg frame grabber."", e);"
"<line5>    LOG.info(""Secondary started"");<line11>      LOG.error(e.getMessage(), e);"
"<line8>      LOG.trace(""writing index file to cache failed"", e);"
"<line22>            LOGGER.debug("" Continuing for "" + latData.getLatitudeBand() + index);"
"<line7>      LOG.debug(""Could not parse from natural date: "" + string, e);"
"<line11>        log.warn(""Group with id '""+ id+ ""' not found in UserGroupCache. groupIds string was: ""+ userIds);"
"<line2>    log.debug(""{} receiver link with link correlation id {} was successfully opened"",getLinkInstanceType(),this.linkCorrelationId);"
"<line2>      LOG.debug(""Processing object type: {} (Month: {})..."", objectType, month);<line6>        LOG.debug(""Executing processor {} on month {}..."", processor.getClass().getSimpleName(), month);<line9>        LOG.debug(""Executing processor {} on month {}...DONE"",processor.getClass().getSimpleName(),month);<line12>      LOG.debug(""Processing object type: {} (Month: {})...DONE"", objectType, month);"
"<line10>                  LOGGER.error(""subnetworkId for node ""+ nodeId+ "" (""+ createPoint(graph, nodeId)+ "") already set (""+ sn+ ""). ""+ ""Cannot change to ""+ subnetworkId);"
"<line5>        logger.error(""unable to assign new task to user"");<line8>      logger.error(""Error in assignNewTaskToUser for id : "" + id + "" and userID : "" + userId);"
"<line2>    LOG.debug(""ZK Call - getData [{0}] [{1}] [{2}]"", path, watch, stat);"
"<line2>      LOGGER.info(""Stopping low level grizzly container"");"
"<line20>          log.warn(""{}{} ({}) usage (leaks detected: {}) [pooled: {}, created: {}, acquired: {}""+ "" released: {}, discarded: {}]"",name,id,uri,leaks,pooled,created,acquired,released,discarded);<line21>          log.info(""{}{} ({}) usage [pooled: {}, created: {}, acquired: {} released: {}, discarded: {}]"",name,id,uri,pooled,created,acquired,released,discarded);"
"<line19>      logger.error(""StudyController - resetStudy - ERROR"", e);"
"<line21>                    logger.debug(""Notifying management server join event took ""+ profiler.getDurationInMillis()+ "" ms"");<line23>                  logger.warn(""Notifying management server join event took ""+ profiler.getDurationInMillis()+ "" ms"");<line34>                    logger.debug(""Notifying management server leave event took ""+ profiler.getDurationInMillis()+ "" ms"");<line36>                  logger.warn(""Notifying management server leave event took ""+ profiler.getDurationInMillis()+ "" ms"");<line47>          logger.warn(""Unexpected exception during cluster notification. "", e);"
"<line38>    logger.info(""tenantPerms Req: {}"", pljson);<line57>    logger.debug(""tenantPerms: {} and {}"", permsModule.getId(), permPath);"
"<line2>    LOGGER.info(""Reading properties from: "" + fileName + "". Will try classpath, then file system."");"
"<line15>      LOGGER.error(""Error creating risk incidence in storage"", e);"
"<line5>      log.warn(""Could not parse any records from given xml - returning null."");<line8>      log.warn(""Got multiple records from given xml, but no unitQualifier set - returning first record""+ "" as a guess."");<line17>        log.warn(""Got multiple records from xml but none matched unitQualifier - returning null"");"
"<line12>      LOG.error(""run predictTest failed "", x);"
"<line4>      LOGGER.error(""Error reloading user page"", e);"
<line5>      logger.warn(msg);
<line36>        log.debug(exception, exception);
"<line2>    log.info(""Will send a request to ["" + requestUrl + ""]"");"
"<line4>      log.debug(""Flushing all stores registered in the state manager: {}"", stores);<line6>        log.trace(""Flushing store {}"", store.name());<line17>          log.error(""Failed to flush state store {}: "", store.name(), exception);"
"<line26>    LOGGER.info(""Trying to connect to deviceIdentification: {} at IP address {} using response time-out:""+ "" {}"",deviceIdentification,deviceConnectionParameters.getIpAddress(),this.responseTimeout);<line46>      LOGGER.error(""ProtocolAdapterException: unable to read ServerModel for deviceIdentification {}"",deviceIdentification,e);<line57>    LOGGER.info(""Connected to device: {}, fetched server model. Start time: {}, end time: {}, total time in""+ "" milliseconds: {}"",deviceIdentification,startTime,endTime,endTime.minus(startTime.getMillis()).getMillis());"
"<line13>        LOG.warn(""Error restarting webapp"", e);"
"<line8>      log.debug(""Worlflow Detail ==== generateChartsConfig for kpi {} kpiContentArray  {} "",kpiId,kpiContentArray);<line23>            log.debug(""Worlflow Detail ==== generateChartsConfig for Content {} value is null in""+ "" kpiContentArray  {} "",eachRowArray.get(1).getAsString(),eachData);"
"<line6>    LOG.debug(""{}: Create {} {} via actor {}"", id, store, path, masterActor);"
"<line6>          logger.info(""[run][listening]"" + port);<line15>          logger.info(""[run][new socket]"" + socket);<line20>      logger.warn(""[run]"" + port + "","" + e.getMessage());"
"<line26>      logger.error(""Exception:"", e);"
"<line6>      LOG.error(""I/O error opening the stream: {}"", e.getMessage(), e);<line13>      LOG.error(""I/O error reading the stream: {}"", e.getMessage(), e);"
"<line2>    log.info(""[{}] Detected update of the API state for {}: {}"",state.getEntityId(),state.getEntityType(),result);<line29>                      log.warn(""[{}] Can't send update of the API state to tenant with provided email""+ "" [{}]"",state.getTenantId(),email,e);<line33>        log.warn(""[{}] Can't send update of the API state to tenant with empty email!"",state.getTenantId());"
"<line2>    LOG.debug(""Attempting to sync all data to filesystem"");"
"<line15>          logger.info(""read {} entities"", count);<line19>        logger.info(""Counted {} : query again with cursor"", count);"
"<line3>    LOG.debug(""processing tuple"");"
"<line9>          LOG.warn(""Block {} does not exist while being committed."", blockId);<line10>          LOG.debug(""Invalid worker state while committing block."", e);"
"<line2>    log.info(""Asked IPs -> IDs for: [%s]"", String.join("","", ips));<line6>      log.debug(""Not IPs, doing nothing"");<line29>      log.debug(""Converted to [%s]"", String.join("","", instanceIds));<line31>      log.error(e, ""Unable to convert IPs to IDs."");"
<line9>          log.debug(_NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey);
"<line5>      logger.error(""error happens when merge fragments:"" + fragmentsToMerge, e);"
"<line8>      logger.info(String.format(""Killing following machines with names: \n %s."", vms.toString()));<line18>        logger.warn(String.format(""%s operation has timedout: %s\n"",operation.operationType(), operation.httpErrorMessage()));<line19>        logger.info(String.format(""Operation %s  was successfully done on %s\n."",operation.operationType(), operation.targetLink()));"
"<line4>      log.error("""", e);"
"<line21>    logger.debug(""Mocking the console input"");"
"<line15>      logger.debug(""Invalid packet at offset {}"", old);<line19>        logger.debug(""Adding session {} to session queue."", sessionID);"
"<line5>    log.debug(""Building "" + buildExecutionSession.getId());<line11>                log.error(""Interrupted"", e);"
"<line5>      LOGGER.info(""Executing resource NetworkUsageCommand "" + cmd);"
"<line10>    LOGGER.debug(""enqueueGetConfigurationRequest called with organisation {} and device {}"",organisationIdentification,deviceIdentification);"
<line51>      log.error(systemException, systemException);
"<line1>    log.debug(""Aio Server started, waiting for accept."");"
"<line3>      LOG.trace(""Validating connecting channel request: {} -> {}"", channelFuture, true);<line6>      LOG.trace(""Validating unsuccessful channel request: {} -> {}"", channelFuture, false);<line10>    LOG.trace(""Validating channel: {} -> {}"", channel, answer);"
"<line42>    logger.trace(""Getting all shared analysis output file info for project id="" + projectId);"
"<line6>        logger.error(""Base Session is null for sessionId: {}"", data.getSessionId());<line17>          logger.error(""Caught exception from session object!"", e);<line20>      logger.error(""Failure executing timer task"", e);"
"<line17>                      LOG.debug(""Creating schema: "" + schema);<line20>                      LOG.debug(""Skipping creation of existing schema: "" + schema);<line34>          LOG.debug(""Schema creation failed. Retrying in 1 sec ..."");"
"<line3>      logger.trace(""TelnetSession close called"");<line8>        logger.debug(""TelnetSession close: error disconnecting: {}"", e.getMessage());"
"<line11>          log.debug(""Prepared {} task for committing"", state());<line13>          log.debug(""Skipped preparing {} task for commit since there is nothing to commit"", state());"
"<line3>    LOGGER.info(""Extra unmanaged storage volume attachments returned to client: {}"",JsonPrettyPrinter.print(extraStorageVolumes));"
"<line4>      log.info(""Looking for matching filesystem for {} from options {}"", exportDir, tableDirs);<line9>      log.info(""Chose base table directory of {}"", base);<line12>      log.info(""Using import dir: {}"", dm.importDir);"
"<line41>      log.error(""Error reading file input stream"", e);"
"<line2>    logger.error(""try to reconnect to tunnel server, uri: {}"", tunnelClient.getTunnelServerUrl());<line5>      logger.error(""reconnect error"", e);"
"<line5>      logger.debug(""ACL of "" + mutableAcl.getObjectIdentity() + "" updated successfully."");"
"<line10>    log.debug(""Checking the permission."");<line11>      log.debug(""User does not have permission to add metadata value - no data saved, returning MAV."");<line13>    log.debug(""Creating new FileMetadata object"");<line15>    log.debug(""Setting the metadata value = "" + addFileMetadataCommand.getParamValue());<line16>    log.debug(""Saving new file metadata entry"");<line17>    log.debug(""Returning MAV"");"
"<line11>        logger.debug(""sync snapshot "" + snapshotId + "" from cache to object store..."");"
"<line4>    LOGGER.debug(""Create {}"", contextDto);"
"<line25>                LOG.info(""Count of incomplete tasks now ""+ taskCountAfterAtOld+ "", ""+ unendedTasks+ "" unended""+ (extraAllowedMax > 0 ? "" ("" + extraAllowedMax + "" allowed)"" : """")+ ""; tasks remembered are: ""+ tasks);"
"<line1>    LOG.debug(""{}: Committing finalized checkpoint {}"", this, checkpointMark);"
"<line3>      log.debug(""receive the jvm metrics from service instance, name: {}, instance: {}"",request.getService(),request.getServiceInstance());"
"<line3>      logger.info(""Register task : {} for redriver with time : {}"", taskId, redriveDelay);"
"<line31>        logger.warn(annotation + ANNOTATION_HC_WARN_MSG + "", not "" + walker.getClass().getSimpleName());<line37>        logger.warn(""Annotation will not be calculated, can only be used with likelihood based annotations""+ "" in the HaplotypeCaller"");<line43>        logger.warn(""Annotation will not be calculated, genotype is not called"");"
"<line2>    LegacyProjectResourceTest.LOG.debug(""start POST {}s with legacy entities test"", pojoClassName);<line13>    LegacyProjectResourceTest.LOG.debug(""end POST {}s with legacy entities test"", pojoClassName);"
"<line31>      LOG.warn(e);<line34>      LOG.warn(""JSON exception when getting Country List."", ex);"
"<line7>        log.warn(""[task.revert] node in allocated state {}/{} host = {}"",jobUUID,task.getTaskID(),host.getHost());<line8>      log.warn(""[task.revert] sending revert message to host: {}/{}"",host.getHost(),host.getHostUuid());<line19>      log.warn(""[task.revert] task {}/{}] not found"", jobUUID, taskID);"
"<line24>        log.info(""Assigning schema version {""+ latestSchemaVersion.getId()+ "" / ""+ latestSchemaVersion.getProperty(""uuid"")+ ""} to release"");"
"<line1>    log.info(""Cache updateServiceInventory timer start"");<line5>                () -> update(moduleDefineHolder), t -> log.error(""Cache update failure."", t)),"
"<line2>    logger.error(""TextUnitSearcher couldn't recover for \""call\"": {}\n{}"",textUnitSearcherError.getMessage(),textUnitSearcherError.nativeCriteria.getQueryInfo().toString());"
"<line7>      LOG.error(""While provisioning members of group {}"", model.getObject().getKey(), e);"
"<line3>      LOGGER.debug(""Started refresh with git pull"");<line11>        LOGGER.info(""refresh failed. Running undo local changes"");<line13>      LOGGER.debug(""Finished refresh"");<line14>      LOGGER.error(""Error when refreshing git directory "" + workspaceProvider.getRootDirectory(), e);"
"<line5>      log.info(""*** Starting workspace mode: "" + wsm);"
"<line4>        log.debug(this + "": no values"");<line9>        log.debug(this + "": constant value of size "" + count);<line18>        log.debug(this + "": using offset "" + linearOffset);<line29>          log.debug(this + "": using factor "" + gcd);<line41>      log.debug(this + "": creating values of count "" + count + "", value length "" + valueLength);"
"<line6>      logger.warn(""Organization {} not found!"", orgStr);<line12>    logger.debug(""Canvas URL: {}"", url);<line25>    logger.debug(""Canvas instructor roles: {}"", instructorRoles);<line28>    logger.debug(""Ignored users: {}"", ignoredUsernames);"
"<line4>      log.error(""Could not save RESTXQ Registry to disk!"");<line6>      log.info(""Preparing new RESTXQ registry on disk: {}"", tmpNewRegistry.toAbsolutePath().toString());<line43>          log.info(""Replaced RESTXQ registry: {} -> {}"",FileUtils.fileName(tmpNewRegistry),FileUtils.fileName(registry));<line47>        log.error(ioe.getMessage(), ioe);"
"<line3>    logger.info(getLastFileCount() + "" "" + getLabType() + "" lab files found."");<line6>        logger.info(""Saving and parsing lab files."");<line30>            logger.info(getServiceName() + "" lab files have been saved to local file system: "" + savePath);"
<line28>      log.error(e.getMessage(), e);
"<line12>      LOG.debug(""Qeury vertex {}: {}"", i, id);<line15>        LOG.debug(""Edge of vertex {}: {}"", i, edges.next());<line18>    LOG.debug(""Qeury edges of vertices({}) with thread({}): {}"", totalV, thread, totalE);"
<line4>      LOGGER.error(PRIVATE_KEY_MUST_BE_CONFIGURED_FOR_DECRYPTION);
"<line6>                logger.debug(""Received Subscription discovery response "" + response);<line22>                logger.error(""Error occurred during Subscription discovery"", throwable);<line28>                logger.info(""Completed receiving Subscription data"");<line38>      logger.debug(""Sent Discovery request for type url: "" + Constants.SUBSCRIPTION_LIST_TYPE_URL);<line39>      logger.error(""Unexpected error occurred in API discovery service"", e);"
"<line1>    log.info(""SessionContext got started. Init Object: "" + payload.toString());"
"<line2>    logger.info(name.getMethodName());<line12>                logger.info(name.getMethodName() + "" - invalid byteBufferOut from callback"");<line12>                logger.info(name.getMethodName() + "" - FAILED"");<line24>                logger.info(name.getMethodName()+ "" - callback - caught exception ""+ ((JoynrRuntimeException) error).getMessage());<line25>                logger.info(name.getMethodName() + "" - callback - caught exception"");<line26>              logger.info(name.getMethodName() + "" - FAILED"");<line32>        logger.info(name.getMethodName() + "" - about to wait for callback"");<line35>        logger.info(name.getMethodName() + "" - wait for callback is over"");<line44>    logger.info(name.getMethodName() + "" - OK"");"
<line5>      log.error(exception, exception);
"<line4>      logger.debug(""Skipping Cleanup phase ..."");<line6>    logger.debug(""Cleaning up temporary resources created for testing ..."");"
"<line11>            log.info(""Removed range {} from applicable ranges"", range);<line18>            log.info(""Removed range {} from buildstore"", range);<line24>            log.info(""Rollup Generator Thread interrupted"");<line28>        log.error(""Exception encountered while calculating rollups"", e);"
"<line34>    LOGGER.info(""Checking produced and consumed messages to pod:{}"", defaultKafkaClientsPodName);"
"<line5>        LOG.warn(""sync checkpoint failed, will retry after 2 seconds"", e);<line8>          LOG.debug(""sync checkpoint interrupted"");"
"<line2>      log.warn(""{}.attachSpeechRecognizer(null)"", getName());"
"<line19>    logger.debug(""[refreshCurrentMasterHealthStatus] cluster {}, shard {} remove not exist targetDcId {}"",clusterId,shardId,toDeleteTargetDcIds);"
"<line9>        log.warn(""ORC file %s was written by a newer Hive version %s. This file may not be readable by""+ "" this version of Hive (%s.%s)."",orcDataSource,Joiner.on('.').join(version),CURRENT_MAJOR_VERSION,CURRENT_MINOR_VERSION);"
"<line26>      LOGGER.error(""Error while checking for database"", e);"
"<line6>      LOG.info(""Sending Trans[id=""+ trans+ "", count=""+ count+ "", clientType=""+ clientType+ "", firstID=""+ (messageRover + 1)+ ""]"");<line32>      LOG.info(""Committed Trans[id=""+ trans+ "", count=""+ count+ "", clientType=""+ clientType+ ""], ID=""+ messageRover);"
"<line13>    log.info(""App started:"");"
<line28>                LOGGER.debug(e, e);<line50>      LOGGER.error(error);
"<line1>    LOG.debug(""Received hangup signal, stopping the main instance."");<line3>          LOG.trace(""OnShutdown"");<line27>                LOG.info(msg);<line28>                LOG.trace(msg);<line37>          LOG.trace(""OnShutdown complete"");"
"<line14>              log.info(""Start thread [node="" + node.name() + ']');<line24>              log.error(""Unexpected error: "" + e, e);"
"<line7>    LOGGER.error(""handling error: {} for message type: {}"", e.getMessage(), messageType, e);"
"<line4>        LOG.warn(""Ignoring XMLHttpRequest.setRequestHeader for "" + name + "": it is a restricted header"");"
<line9>      LOGGER.debug(parseException.getMessage(), parseException);
"<line4>      LOG.error(""Audit Request Message is null."");<line19>      LOG.error(""Failed to call the web service ({}).  An unexpected exception occurred. Exception: {}"",NhincConstants.AUDIT_REPO_SERVICE_NAME,e.getLocalizedMessage(),e);<line20>    LOG.debug(""In AuditRepositoryProxyWebServiceUnsecuredImpl.auditLog(...) - completed called to ""+ ""ConnectionManager to retrieve endpoint."");"
"<line5>      log.warn(""Error reading measurement list from storage = '""+ e+ ""' ,""+ "" trying to convert the list records size"");"
"<line7>        logger.warn(""Exception at showUserListMembership"", e);"
"<line12>      log.info(""The hive operation handle: {}"", op);"
"<line6>      LOG.warn(""Failed to set activity stream type with type:"" + name);"
"<line2>    LOGGER.debug(""reset"");<line49>      LOGGER.debug(""Node data for node {} is {}"", node.getName(), data);"
"<line9>      logger.error(""return message error {} - {}"", message.getSubject(), message.getMessageId());"
"<line7>      logger.info(""init: "" + this.toString());"
"<line1>    LOG.debug(""storing key: '{}' and value: '{}'"", key, value);<line11>      LOG.debug(""Cannot store null key"");"
"<line2>    logger.info(""Totally handled "" + mapCounter + "" records!"");"
"<line27>      LOG.trace(""Node {} has changes"", target);"
"<line4>      LOGGER.error(""file "" + file + "" doesn't exists"");"
"<line3>    logger.debug(""Starting CHARGEBACK for payment {} ({} {})"",paymentStateContext.getPaymentId(),paymentStateContext.getAmount(),paymentStateContext.getCurrency());"
"<line14>      log.error(String.format(""Unable to deserialize JSON localized property \""%s\"" "" + ""from request"",propertyName),jsonException);"
"<line2>    LOG.debug(""afterBulk [{}] with {} responses"", executionId, request.numberOfActions());<line19>            LOG.debug(""Doc conflict ID {}"", id);<line20>            LOG.error(""Update ID {}, failure: {}"", id, f);<line25>          LOG.debug(""Acked {} tuple(s) for ID {}"", xx.size(), id);<line29>              LOG.debug(""Acked {} with ID {}"", url, id);<line37>          LOG.warn(""Could not find unacked tuple for {}"", id);<line39>      LOG.info(""Bulk response [{}] : items {}, waitAck {}, acked {}, failed {}"",executionId,itemcount,waitAck.size(),acked,failurecount);<line41>          LOG.debug(""Still in wait ack after bulk response [{}] => {}"", executionId, kinaw);"
"<line4>    LOGGER.info(""requestReadAlarmRegister for organisationIdentification: {} for deviceIdentification: {}"",deviceMessageMetadata.getOrganisationIdentification(),deviceMessageMetadata.getDeviceIdentification());"
"<line2>    logger.debug(""Initializing VigiCrues handler."");<line3>    logger.debug(""config refresh = {} min"", config.refresh);"
"<line2>    logger.debug(""Getting monomers as hashtable"");"
"<line1>    log.debug(""finding StgMsZykTxt instance by example"");<line8>      log.debug(""find by example successful, result size: "" + results.size());<line10>      log.error(""find by example failed"", re);"
"<line39>      LOG.info(""Forwarded the prediction model of ""+ numForwarded+ "" rows. [lastLosses=""+ cvState.getCumulativeLoss()+ "", #trainingExamples=""+ count+ ""]"");"
<line2>    LOGGER.error(throwable.getMessage(), throwable);
"<line8>      LOGGER.debug(""startup address in createclient: {}"", startupAddress.getHostAddress());<line24>        LOGGER.error(""Unknown type of client."");"
"<line11>      LOG.trace(""No task log error regex provided."");<line35>      LOG.error(""Invalid task log error regex supplied: \""{}\"". Received exception: {}"", regex, e);"
"<line3>      log.debug(""Destroying container session : "" + event.sID.getLongId());<line6>      log.debug(""Destroying plan session : "" + event.sID.getLongId());"
"<line8>      log.warn(""The service is already shut down."");"
"<line8>      log.warn(""It was found a group without members. Error ahead."");<line11>      log.debug(""It was found a group without member definitios. Error ahead."");"
"<line40>      logger.error(""error occurred while trying to run jvm tool:\n{}\n{}"",Joiner.on(' ').join(command),errorStreamReader.getOutput().trim());"
"<line2>    logger.info(""Handling error: "" + e.getClass().getSimpleName() + "", "" + e.getMessage());"
"<line3>    logger.error(""Unable to get config {} / {}"", replyContext, error);"
<line7>      LOG.error(I18n.err(I18n.ERR_78, ldif, dn));
"<line20>      LOGGER.debug(""Created shunt {}"", newShunt.getId());"
"<line2>    log.debug(""Sending CommfaultTag tag {} (#{})"", tagName, tagID);"
"<line28>          logger.warn(""Unknown error type in proto serialization, setting unknown "" + errorType);"
"<line11>      log.error(String.format(""%s: failed sending discovery request"", local_addr), ex);"
"<line2>    logger.debug(""Getting Name: "", super.getName());"
"<line5>          log.debug(""Loading document info from index {""+ fullIndexName+ ""} in bucket {""+ bucket+ ""}"");<line12>          log.trace(""Using query {\n"" + query.encodePrettily() + ""\n"");<line17>              log.trace(""Got response {"" + result.encodePrettily() + ""}"");<line25>                  log.debug(""Fetching scroll result using scrollId {"" + currentScroll + ""}"");<line28>                    log.trace(""Got response {"" + scrollHits.encodePrettily() + ""}"");<line33>                      log.debug(""Using scrollId {"" + nextScrollId + ""} for next fetch."");<line43>            log.error(""Error while loading version information from index {"" + indexName + ""}"",e.toString());<line43>            log.error(e);"
"<line3>      logger.info(result.getRows().get(i).getValues().get(0)+ "" ""+ result.getRows().get(i).getValues().get(1));"
"<line3>      LOGGER.warn(""Too many tests have been generated: {}"", tests.size());<line24>      LOGGER.info(""Number of generated test reduced to {}"", reducedTests.size());"
"<line3>      logger.debug(""validate() action="" + action.name());<line27>        logger.error(msg);"
"<line10>      logger.debug(""Invalid json in handleEvent"");<line13>      logger.debug(""Unable to get params form json in handleEvent"");<line23>          logger.error(""This should never happen, pattern should only match integers"");"
"<line13>      LOGGER.debug("""", e);"
"<line13>      logger.info(""Saved FQL Query : {}"", fqlStore.getQuery());"
<line9>      log.error(exception, exception);
"<line17>              LOG.info(""consumer received message :"" + receivedText);<line18>              LOG.info(""committed transaction"");<line21>                LOG.info(""rolled back transaction"");<line22>                LOG.info(e1.toString());<line24>              LOG.info(e.toString());<line34>      LOG.info(""producer sent message :"" + tm.getText());<line37>    LOG.info(""Waiting for latch"");<line39>    LOG.info(""test completed, destination="" + receivedText);"
"<line7>        LOG.warn(""Could not register request RTT (likely caused by clock problems). Consider using the""+ "" 'fixed' backpressure algorithm."",e);<line10>      Loggers.LOGSTREAMlogger.warn(""We encountered an problem on releasing the acquired in flight append. There was no""+ "" listener registered for the given position {}, this should not happen."",position);"
"<line3>    LOGGER.info(""the UA by id {} "", id);"
"<line6>        log.warn(""Exception got when closing the tm socket:"", e);"
"<line8>        LOG.debug(""processorName: {}"", processorName);<line10>          LOG.debug(""adding Extension: {}"", extension);"
"<line9>      logger.debug(""NULL payload on topic: {}"", topic);"
"<line13>      logger.error(""Error removing consumed Token"", t);"
"<line28>                        LOG.debug(""Value is {}, key A is {}, and key B is {}."",c.element(),keyMap.get(""Key_A""),keyMap.get(""Key_B""));"
"<line4>      LOG.info(""rest({}).packages({})"", paths, pkg);<line5>      LOG.info(""No Beans in '{}' found. Requests {} will fail."", pkg, paths);"
"<line3>      log.info(""Using non-artifact based module id: "" + moduleId);"
<line7>        log.debug(portalException, portalException);
"<line7>          log.debug(""receive browser error log"");<line14>          log.error(e.getMessage(), e);<line22>        log.error(throwable.getMessage(), throwable);"
<line6>      log.error(exception, exception);
"<line2>    LOG.debug(""Opening input stream at {}"", profileUrl);"
"<line7>            LOG.debug(""{} finished successfully: {}"", prefix, nodeId.getValue());<line10>            LOG.debug(""{} failed: {} -> {}"", prefix, nodeId.getValue(), errors);<line12>          LOG.debug(""{} reconciliation failed: {} -> null result"", prefix, nodeId.getValue());<line17>        LOG.debug(""{} reconciliation failed seriously: {}"", prefix, nodeId.getValue(), failure);"
"<line1>    logger.debug(""Updating router network ref"");<line24>        logger.debug(""Added reference for router id="" + routerId + "" and network id="" + networkId);<line28>    logger.debug(""Done updating router/network references"");"
"<line3>    LOG.debug(""Executing updateTask - PUT - "" + request.getRequestURL());<line16>        LOG.error(e.getMessage());"
"<line2>    LOG.info(""Connect [{}]"", session);"
"<line6>      logger.warn(""No report history found (normal on first run), existing values will not be reset"");"
"<line14>      LOGGER.trace(""Unable to open a connection to the test database."", ex);"
"<line8>      logger.debug(""Looking for matching API with basepath: {} and version: {}"", basePath, version);"
"<line4>          logger.debug(""Waiting for bulk queue to empty..."");<line7>        logger.warn(""checkIndexStatistics interrupted"", e);"
<line95>      log.error(exception, exception);
"<line3>    logger.info(""Un-binding from endpoint {} to {} for cluster {}"",new Object[] {getEndpointId(), endpoint.getEndpointId(), Integer.valueOf(clusterId)});<line13>      logger.warn(""ZDO_BIND_REQ failed, unable to un-bind from endpoint {} to {} for cluster {}"",new Object[] {getEndpointId(), endpoint.getEndpointId(), new Integer(clusterId)});"
"<line22>      logger.debug(""PLAY_STATUS - no match on message: {}"", message);"
"<line2>    logger.info(""Persisting ""+ coviCodes.size()+ "" covicodes for start ""+ coviCodes.get(0).getStartInterval()+ "" to end ""+ coviCodes.get(coviCodes.size() - 1).getEndInterval());<line6>    logger.info(""Done persisting"");"
"<line8>        LOG.debug(""Reading schematron rules from class path {}"", path);<line12>        LOG.debug(""Error loading schematron rules from class path, attempting file system {}"", path);<line16>          LOG.debug(""Schematron rules not found in the file system {}"", path);<line20>        LOG.error(""Failed to load schematron rules {}"", path);"
"<line10>      logger.warn(""Unable to retrieve client hostname"", e);"
"<line20>      log.debug(""...Ready Go! CountDownRace just begun! (runner="" + runnerCount + "")"");<line25>        log.debug(""All runners finished line! (runner="" + runnerCount + "")"");"
"<line7>      log.error(""Error in updating the webhook.. "", e);"
"<line13>      LOG.info(""Already tried to refresh the access token with the current refresh token"");<line15>    LOG.info(""Trying to refresh the OAuth2 access token"");<line21>          LOG.error(""Unable to refresh the access token, received status: `{}`, response: `{}`"",statusLine,EntityUtils.toString(entity));<line27>      LOG.error(""Unable to refresh the access token"", e);"
"<line21>                logger.debug(""Routing Map Null for collection: {}, PartitionKeyRangeId: {}, forceRefresh:{}"",collectionResourceId,partitionKeyRangeId,forceRefresh);"
<line5>        log.debug(_NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey);
"<line2>      log.info(""Sending Message: softReset to {}"", serial.getName());<line20>      log.error(""softReset threw"", e);"
"<line6>        logger.debug(""Got null bucket region for bucketId={}{}{} for PartitionedRegion = {}"",this.partitionedRegion.getPRId(),PartitionedRegion.BUCKET_ID_SEPARATOR,bucketId,this.partitionedRegion);"
"<line14>      LOGGER.error(L.m(""Fehler beim Versuch, Bereich zu updaten: \""%1\"""", sectionNameComplete), x);"
"<line2>    logger.debug(""Processing the license solution"");"
"<line3>      LOGGER.info(MessageFormat.format(""No dead letter jobs found for process with id {0}"", processInstanceId));"
"<line2>    log.debug("" EngineCorrelatorModule start  ===="");<line14>      log.error(""Error in correlation module "", e);<line18>    log.debug("" EngineCorrelatorModule Completed ===="");"
"<line12>      logger.error(""Application run failed"", failure);"
<line9>                LOG.error(exception.getMessage(), exception);
"<line3>      log.trace(""ID of request correlator header (%s) is different from ours (%d). Msg not accepted,""+ "" passed up"",hdr != null ? String.valueOf(hdr.corrId) : ""null"", this.corr_id);<line8>        log.trace(""%s: dropped req from %s as we are in the exclusion list, hdr=%s"",local_addr, msg.src(), hdr);"
"<line19>          logger.debug(""Set asyncContext {}"", asyncContext);"
"<line19>      LOG.error(""could not start tomcat."", e);"
"<line1>    LOGGER.info(""Sending request message to OSGP."");"
"<line3>      logger.debug(""Request PAgentInfo={}"", MessageFormatUtils.debugLog(agentInfo));<line16>      logger.warn(""Failed to request. Rejected execution, executor={}"", executor);"
"<line14>      log.warn(""Could not describe instance "" + instanceId, e);"
"<line17>              log.warn(""Warn limit reached for ""+ type+ "" beans. Currently there ""+ ""are ""+ size+ "" ""+ type+ "" EJB stubs cached in '""+ name+ ""' ""+ ""beanstalk."");"
"<line4>      log.debug(""onScript"");"
"<line4>      log.debug(""Could not find Beeline configuration file: {}"", DEFAULT_BEELINE_SITE_FILE_NAME);<line6>    log.info(""Beeline configuration file at: {}"", fileLocation);"
<line5>        log.debug(_NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey);
"<line6>        BPELSituationAwareBuildProcessBuilder.LOG.debug(""ServiceTemplate {} has no BuildPlan, generating BuildPlan"",serviceTemplate.getQName().toString());<line8>          BPELSituationAwareBuildProcessBuilder.LOG.debug(""Created BuildPlan "" + newBuildPlan.getBpelProcessElement().getAttribute(""name""));<line11>        BPELSituationAwareBuildProcessBuilder.LOG.debug(""ServiceTemplate {} has BuildPlan, no generation needed"",serviceTemplate.getQName().toString());<line14>      LOG.info(""Created {} situation-aware build plans for CSAR {}"",String.valueOf(plans.size()),csarName);"
"<line2>    log.info(""register XA resource"");"
"<line21>            logger.debug(""MRule matches at Delivery phase to a message: rule: "" + rule + ""message: "" + sms);<line26>      logger.error(""Exception when invoking rule.matches(message) or onPostDelivery(): "" + e.getMessage(),e);"
<line3>      log.warn(throwable);
"<line6>      log.warn(""Cannot read idName, defaulting to 'id'"", e);"
"<line3>      logger.debug(""Not logged in. Not updating."");<line22>      logger.debug(""Session is invalidated. Attempting new login."");"
"<line3>    LOG.debug(""Decoded SAML: \n{}\n"", base64DecodedResponse);"
"<line2>    LOGGER.debug(""config inited, \""{}\"" set to {}, effective key is \""{}\""."",joinedPriorityKeys,finalValue,effectiveKey);"
"<line1>    LOG.debug(""getting list of organisations from registry"");<line7>    LOG.debug(""organisations returned: "" + organisations.size());"
<line14>        log.debug(sb.toString());
"<line2>    logger.error(msg, os);<line2>    logger.error("""", t);"
"<line2>    LOG.info(""Started loadCacheErrorDirectTemp"");<line3>    LOG.info(""Finished loadCacheErrorDirectTemp"");"
"<line5>    LOGGER.info(""InstanceResponsePlanNode.run took: "" + (end - start));"
"<line1>    log.debug(""persisting StgSysNetzMapping instance"");<line3>      log.debug(""persist successful"");<line4>      log.error(""persist failed"", re);"
"<line2>    logger.debug(""Getting connected rings for ring: "", ring);"
<line6>        log.error(ex.toString(), ex);
"<line3>      LOGGER.trace(""Setting {} as readonly because context said so."", itemWrapper);<line8>        LOGGER.trace(""Setting {} as readonly because authZ said so."", itemWrapper);"
"<line9>    logger.debug(""Main Response -> {}"", result);"
"<line2>    LOG.info(""KRADConfigurer:getPrimarySpringFiles: getRunMode => "" + getRunMode());"
"<line6>      log.debug(""Registration at {} attempt {} (timeout={}ms)"", targetName, attempt, timeoutMillis);<line13>                    log.debug(""Registration with {} at {} was successful."", targetName, targetAddress);<line16>                    log.debug(""Registration with {} at {} was rejected."", targetName, targetAddress);<line21>                      log.info(""Registration failure at {} occurred."", targetName, failure.getReason());<line22>                      log.error(""Received unknown response to registration attempt: {}"", result);<line23>                    log.info(""Pausing and re-attempting registration in {} ms"",retryingRegistrationConfiguration.getRefusedDelayMillis());<line37>                  log.debug(""Registration at {} ({}) attempt {} timed out after {} ms"",targetName,targetAddress,attempt,timeoutMillis);<line44>                log.error(""Registration at {} failed due to an error"", targetName, failure);<line44>                log.info(""Pausing and re-attempting registration in {} ms"",retryingRegistrationConfiguration.getErrorDelayMillis());"
<line4>      LOG.debug(msg);
"<line5>      logger.warn(""Client {} is a slow receiver."", new Object[] {proxy.getProxyID()});"
"<line7>    LOGGER.info(""Running test updateTimeAfterEndTimeProcess"");<line26>    LOGGER.info(""Original Feed : "" + Util.prettyPrintXml(oldProcess));<line26>    LOGGER.info(""Updated Feed :"" + Util.prettyPrintXml(processBundle.getProcessData()));<line26>    LOGGER.info(""Update Time : "" + updateTime);"
<line7>      log.error(TIPS);
"<line4>      LOG.info(""BodyTrackController.setFacetMetadata(): Attempting to set metadata for facet [""+ facetId+ ""] for connector [""+ connectorName+ ""] and object type [""+ objectTypeName+ ""]"");"
"<line3>    log.info(""Level changed to "" + level.getName());"
"<line11>        log.debug(""received {} when updating lease lock"", e.getCode(), e);<line12>        log.error(""received {} when updating lease lock"", e.getCode(), e);"
"<line3>      logger.debug(""validate() action="" + action.name());<line22>        logger.error(msg);"
"<line12>            logger.trace(await? ""Signaled new events are available"": ""No signal received for new events, exiting await"");<line19>      logger.warn(""Event consumer thread was interrupted. Returning thread to event processor."", e);"
"<line2>      logger.warn(""Setting limit to {} but in current olap context, the limit is already {}, won't apply"",l,limit);"
"<line4>      log.info(""Loaded unidentified ReadMarker start time {} into {}"", messageTimeStart, this);<line11>      log.info(""Loaded identified ReadMarker start time {} into {}"", messageTimeStart, this);"
<line4>    logger.info(elem);
"<line4>      logger.info(""getPublicLandingPageTemplate: "" + crisisID);<line4>      logger.info(""url: "" + taggerMainUrl + ""/customuitemplate/crisisID/"" + crisisID);<line9>      logger.info(""Received jsonResponse: "" + jsonResponse);<line10>        logger.info(""getPublicLandingPageTemplate"");<line14>      logger.error(""Error while creating new template in Tagger"", e);"
"<line3>      log.debug(""Document {} is not a BlobHolder."", doc::getId);<line7>      log.debug(""Document {} is a BlobHolder without a blob."", doc::getId);"
"<line1>    LOG.debug(""Getting Properties."");<line2>      LOG.debug(""The requested NodeTemplate was not found."");<line6>      LOG.debug(""Properties are not set."");<line10>      LOG.debug(""Properties is not of class Element."");"
"<line1>    log.debug(""attaching clean SysHilfe instance"");<line3>      log.debug(""attach successful"");<line4>      log.error(""attach failed"", re);"
"<line24>        logger.warn(""Initialize DB failed."", e);"
"<line7>        log.warn(""Session  "" + sessionHandle.getPublicId() + "" is invalid."");"
"<line4>    LOG.info(""Checking sys.segments query as stateOnlyNoLdapGroupUser..."");<line6>    LOG.info(""Checking sys.servers query as stateOnlyNoLdapGroupUser..."");<line8>    LOG.info(""Checking sys.server_segments query as stateOnlyNoLdapGroupUser..."");<line10>    LOG.info(""Checking sys.tasks query as stateOnlyNoLdapGroupUser..."");"
"<line4>      log.debug(String.format(""Run with feature %s"", feature));"
"<line9>            log.debug(String.format(""Artifact repository password encrypted: [application-id] %s ""+ ""[tenant-id] %d [repo-url] %s"",applicationSignUp.getApplicationId(),applicationSignUp.getTenantId(),artifactRepository.getRepoUrl()));"
"<line1>    LOGGER.debug(""Receiving records..."");<line2>    LOGGER.info(""Records received ("" + receiveFromAlias + ""): "" + receivedRecords.size());"
"<line15>              LOG.error(""Unexpected exception occur when register owned bundle {}. Shutdown broker now""+ "" !!!"",ownedBundle.getNamespaceBundle(),ex);"
"<line5>        logger.error(""Invalid record: Id missing in "" + record);<line12>        logger.error(""Error creating an Object of type:"" + clazz.getCanonicalName(), e);"
"<line4>    LOGGER.info(""registered Spring DM injection SPI for PAX Wicket."");"
"<line18>      LOG.warn(""Unable to extract assertion from context."", e);"
"<line3>      Log.debug(""Annotating Lane "" + laneSWID);<line10>      Log.error(""IOException while updating lane "" + laneSWID + "" "" + ex.getMessage());<line12>      Log.error(""JAXBException while updating lane "" + laneSWID + "" "" + ex.getMessage());<line14>      Log.error(""ResourceException while updating lane "" + laneSWID + "" "" + ex.getMessage());"
"<line1>    log.error(""getAllModuleTypesFallback()"");"
"<line7>      log.warn(""Failed to complete JMX registration for "" + objectName, e);"
"<line10>        logger.error(String.format(""%s, cannot connect to %s"", m, jmx));<line11>        logger.error(String.format(""%s, interrupted"", m));<line12>        logger.error(String.format(""%s, cannot setup beans"", m), e);"
"<line9>        LOG.warn(""Failed to find artist bio for "" + artistName, e);"
"<line10>    LOG.info(""slop={}"", this.slop);"
"<line13>        logger.debug(description+ "", reducing priority by ""+ ServiceReplicationStatus.INCONSISTENT_WITH_MIRROR_FACTOR);<line20>        logger.debug(description+ "", reducing priority by ""+ ServiceReplicationStatus.INCONSISTENT_WITH_SPACE_FACTOR);"
"<line30>        log.error(""Can not find Authenticator with Name [%s]"", authenticationResult.getAuthenticatedBy());"
"<line5>      log.info(""No applicationInstanceId set, using name of ApplicationDescriptor as""+ "" applicationInstanceId: {}"",applicationInstanceId);<line7>      log.info(""Current applicationInstanceId={}"", applicationInstanceId);"
"<line5>    logger.error(""request failed with HttpMessageNotReadableException"", e);"
"<line43>      logger.error(""DashBoardAndProfileController:  updateProfileDetails()' = "", e);"
"<line12>          logger.debug(""Exception occurred when trying to remove inactive device '{}': {}"",address,ex.getMessage());<line14>        logger.debug(""Exception occurred when trying to remove inactive device '{}': {}"",address,ex.getMessage());"
"<line5>          LOG.info(""Starting crawl of all datasets for newly endorsed org [{}]"", newOrg.getKey());<line10>                  LOG.warn(""Could not send start crawl message for newly endorsed dataset key [{}]"",dataset.getKey(),e);<line15>          LOG.info(""Starting ingestion of all datasets of org [{}] because it has changed country from""+ "" [{}] to [{}]"",newOrg.getKey(),oldOrg.getCountry(),newOrg.getCountry());"
"<line14>      log.error(""The class specified by 'datatypeClassname' (""+ datatypeClassname+ "") must be a subtype of 'org.openmrs.customdatatype.CustomDatatype<?>'."",ex);<line15>      log.error(""The class specified by 'datatypeClassname' (""+ datatypeClassname+ "") could not be found."",ex);"
<line3>    logger.warn(logMessage);
"<line2>    logger.debug(context, ""UserServiceImpl: searchManagedUser called"");<line27>      logger.error(context,""searchManagedUser: Exception occurred with error message = "" + e.getMessage(),e);"
"<line5>    logger.info(""Status of {} changed from {} to {}"", nodeId, currentStatus, status);<line5>    logger.debug(""State of cluster nodes is now {}"", nodeStatuses);<line9>        logger.debug(""Notifying all nodes that status changed from {} to {}"", currentStatus, status);<line10>        logger.debug(""Notifying cluster coordinator that node status changed from {} to {}"",currentStatus,status);<line13>      logger.debug(""Not notifying other nodes that status changed because previous state of {} is same as""+ "" new state of {}"",currentState,status.getState());"
"<line1>    LOG.debug(""Queueing upload of {}"", block);<line12>                LOG.debug(""Uploading part {} for id '{}'"", currentPartNumber, uploadId);<line15>                  LOG.debug(""Completed upload of {} to part {}"", block, partETag.getETag());"
"<line1>    log.debug(""[TaskTracker-{}] receive heartbeat: {}"", instanceId, heartbeatReq);<line8>        log.warn(""[TaskTracker-{}] ProcessorTracker({}) is idle now but have unfinished tasks: {}"",instanceId,idlePtAddress,unfinishedTask);"
"<line25>              logger.debug(""Failed Request: {}\n=>{}"", req, failure.getMessage());"
"<line6>      Log.error(e, ""Couldn't open command server port: "" + port);"
"<line3>    LOG.info(""testSamba"");<line11>      LOG.info(""Sanitize"");<line11>      LOG.info(""[Base]   : "" + base);<line11>      LOG.info(""[Result] : "" + result);"
"<line13>    LOG.warn(""Unsupported push node type {}"", node);"
"<line24>    log.info(""Pre Check Response of Create first account :: ""+ response.getNodeTransactionPrecheckCode().name());<line28>    log.info(""Account ID "" + newlyCreateAccountId1.getAccountNum() + "" created successfully."");<line28>    log.info(""--------------------------------------"");<line37>    log.info(""Transferring 1000 coin from Genesis account to Newly created account...."");<line40>    log.info(""Pre Check Response transfer :: "" + transferRes.getNodeTransactionPrecheckCode().name());<line43>    log.info(""Transfer from Genesis to newly created account completed...."");<line43>    log.info(""-----------------------------------------"");"
"<line9>      log.warn(""Logical constraints evaluation is not supported by the current execution type: ""+ this.getExecutionType());"
"<line15>      logger.error(""Error loading BpmWidgetInfo list"", t);"
"<line29>      logger.warn(""Error deleting manged acl with id '{}': {}"", aclId, e);"
"<line10>    logger.info(""selectIncremental tableName {} dateField {} overlapTime {} lastLoadDate {} backoffTime {}""+ "" unit {}"",tableName,dateField,overlapTime,lastLoadDate,backoffTime,unit.toString());<line12>    logger.info(""Load range with min {} max {}"", range.getMinDate(), range.getMaxDate());<line28>      logger.info(""Executing incremental GetTableData query {}"", ps);"
"<line22>      LOGGER.warn(""Could not adjust TLSContext"");<line22>      LOGGER.debug(E);"
"<line6>    LOGGER.info(""InstanceDataManager is started! "" + getServerInfo());"
"<line3>      logger.error(""Attempting to insert empty task"");<line15>      logger.error(""Error in saving new document for crisisID : "" + crisisID, e);"
"<line18>    log.info(""Got no further schedule, can stop now"");"
"<line8>    LOGGER.trace(""Send Mercury request, seq: {}, uri: {}, method: {}"",seq,request.header.getUri(),request.header.getMethod());"
"<line2>      log.debug(""removing item for key: "" + key);"
"<line4>    logger.info(""Create or replace function "" + objectName);"
"<line19>          LOGGER.debug(""Writing binlog position to {}.positions: {}, last heartbeat read: {}"",c.getCatalog(),newPosition,heartbeat);"
<line25>      log.error(systemException, systemException);
"<line6>      logger.error(""Tried to email a sync user for a non-remote project"", e);<line27>      logger.error(""Error trying to send sync failed email."", e);"
"<line23>    Freedomotic.logger.info(""Sending 'GET /"" + page + "" HTTP 1.1' to relay board"");"
"<line2>    logger.debug(""getCloudsWithGatewayAndPublicRelays started..."");"
"<line6>      log.error(""JargonException in read is converted to IOException for method contract"", e);"
"<line1>    log.warn(""Using old-style extension point""+ "" org.nuxeo.ecm.core.repository.RepositoryService""+ "" for repository \""""+ cdesc.name+ ""\"", use org.nuxeo.ecm.core.storage.sql.RepositoryService instead"");"
"<line7>      LOG.warn(""jobStartedNotification - unrecorgnized operator name found for "" + ""jobId "" + jobIDStr);<line38>      LOG.error(""Error getting job info!"", e);"
"<line5>      log.warn(""Attempted to unregister unknown alias: {}"", alias);<line8>      log.warn(""Attempted to unregister alias {}, but it is the base command name."", alias);"
"<line13>      LOGGER.error(""IccId is probably not supported in this session provider"", e);"
"<line20>          logger.error(""Failure to store the field values to file"" + oldBatch.getUpdates(), ex);"
"<line6>      LOG.debug(""Set parameter "" + name + "" = "" + value);"
"<line5>      log.warn(""Cannot get project descriptor for project '{}'. Physical project name will be used.""+ "" Cause: {}"",project.getName(),e.getMessage(),e);"
"<line3>    LOG.info(""Graph '{}' was successfully configured via '{}'"", name, path);<line4>      LOG.warn(""You may need to support access control for '{}' with {}"",path,HugeFactoryAuthProxy.GRAPH_FACTORY);"
"<line5>      logger.info(this, ""AppHubInit CHECK APP done :uavapp_"" + appName + "" EXIST"");"
"<line6>      LOG.trace(""New Histogram:\n"" + tmp_userIdHistogram);"
"<line4>      log.info(""*Suppress truncating tables"");<line8>      log.info(""*Suppress dropping foreign keys"");<line12>      log.info(""*Suppress dropping tables"");<line16>      log.info(""*Suppress dropping sequences"");"
"<line9>    logger.debug(""Unexpected StandaloneElement in {}:{}: {} [{}]"", line, col, elementName, attributes);"
"<line2>    logger.debug(""allocateConnection(connRequestInfo)..."");"
"<line25>      log.debug(""{} ms spent for fetching {} validators"",System.currentTimeMillis() - temp,validatorNames.size());"
"<line2>    logger.debug(""Sending {} ({}) configuration update commands"", deviceType, macAddress);<line3>      logger.debug(""Sending command to update {} ({}) parameters"", deviceType, macAddress);<line11>      logger.debug(""Sending command to update {} ({}) sleep parameters"", deviceType, macAddress);<line16>      logger.debug(""Sending command to recalibrate {} ({}) daylight override"", deviceType, macAddress);"
"<line13>          log.debug(""A JSON web service action is already registered at "" + path);<line32>        log.warn(sb.toString());"
"<line3>    LOG.trace(""Using NoOp Implementation for Entity Doc Submission Deferred Response Service"");"
"<line2>    log.debug(""Successfully send ENTITY_CREATED EVENT to rule engine [{}]"", device);"
"<line6>      log.warn(""resource not found: "" + resourceId);"
"<line10>      LOGGER.warn(name+ "" (reserved word) cannot be used as model name. Renamed to ""+ camelize(""model_"" + name));<line13>      LOGGER.warn(name+ "" (model name starts with number) cannot be used as model name. Renamed to ""+ camelize(""model_"" + name));"
<line15>        log.debug(sb.toString());
"<line2>      logger.info(""Opening Sense Hat..."");<line3>      logger.info(""Opening Sense Hat...done"");"
"<line6>        LOG.debug(""Ozone security is enabled. Attempting login for SCM user. ""+ ""Principal: {}, keytab: {}"",scmConfig.getKerberosPrincipal(),scmConfig.getKerberosKeytab());<line22>      LOG.info(""SCM login successful."");"
"<line9>      log.info(""refreshSupervisionStatus() - supervision event cache was successfully updated with ""+ allCurrentEvents.size()+ "" events."");<line10>      log.error(""refreshSupervisionStatus() - Could not initialize/update the supervision event cache.""+ "" Reason: ""+ e.getMessage(),e);"
"<line7>      log.error(""Problem rendering RequestStatus"", e);"
"<line12>      LOG.debug(""Requesting subpartition {} of {}."", subpartitionIndex, partition);"
"<line1>    logger.debug(""Start of handleAliphatics"");<line17>        logger.debug(""---start of longest unplaced chain---"");<line18>          logger.debug(""Start at atom no. "" + (molecule.indexOf(atom) + 1));<line18>          logger.debug(AtomPlacer.listNumbers(molecule, longestUnplacedChain));<line19>          logger.debug(exc);<line20>        logger.debug(""---end of longest unplaced chain---"");<line22>            logger.debug(""More than one atoms placed already"");<line22>            logger.debug(""trying to place neighbors of atom "" + (molecule.indexOf(atom) + 1));<line31>            logger.debug(""Done placing neighbors of atom "" + (molecule.indexOf(atom) + 1));<line32>            logger.debug(""Less than or equal one atoms placed already"");<line32>            logger.debug(""Trying to get next bond vector."");<line47>    logger.debug(""End of handleAliphatics"");"
"<line11>      LOGGER.debug(""XMPP user created"");<line13>      LOGGER.debug(""XMPP user existing"");"
<line7>        log.error(reason);
"<line3>        log.debug(""Executing member started extension"");<line10>        log.debug(""Member started script returned:"" + output);<line13>        log.error(""Could not execute member started extension"", e);"
"<line12>      LOG.debug(""Found endpoint properties {}"", names.retainAll(ENDPOINT_CONFIG_FIELDS));"
"<line3>    logger.debug(""Add text unit variant to virtual assetId: {}, with name: {}"", assetId, name);<line17>      logger.debug(msg);"
"<line10>      log.info(""*Not found the file in the copy src directory: "" + baseDir.getPath());"
"<line6>    LOG.debug(""Graph [{}] get user: {}"", graph, id);"
"<line7>        log.trace(cf + "" "" + endCf);"
"<line1>    logger.info(""executing test case testRelevanceHashMapInt2StringArrayWay"");"
"<line2>    LOG.debug(""Getting knoxClient for datasource: "" + serviceName + ""configMap: "" + configs);<line3>      LOG.error(""Connection ConfigMap is empty"");"
"<line1>    logger.info(""SmscStatProviderJmx Stopping ..."");<line2>    logger.info(""SmscStatProviderJmx Stopped ..."");"
"<line5>      LOG.info(""Ledger ID: "" + lh.getId());<line6>      LOG.debug(""Checking that it is empty"");<line9>      LOG.debug(""Going to write one entry"");<line15>      LOG.debug(""Checking that it is still empty even after writing one entry"");<line23>      LOG.info(""Checking that it has an entry"");<line31>      LOG.error(""Test failed"", e);"
"<line1>    log.debug(""bits.length():"" + bits.length() + "", bits.cardinality():"" + bits.cardinality());<line4>        log.debug(""Collapsing shards for ""+ date+ "" down to just the date with ""+ bits.cardinality()+ "" of ""+ bits.length()+ "" shards marked"");<line11>        log.debug(""Appending "" + bits.cardinality() + "" of "" + bits.length() + "" shards for "" + date);"
"<line4>      log.trace(""TransactionFactory does not require a TransactionManager: don't wrap in a JTA""+ "" transaction"");<line8>      log.trace(""No TransactionManager found, do not start a surrounding JTA transaction"");<line12>        log.trace(""No Transaction in progress, needs to start a JTA transaction"");<line18>    log.trace(""Transaction in progress, no needs to start a JTA transaction"");"
"<line3>    log.trace(""Executing findByTenantId [{}]"", tenantId);"
"<line28>        log.error(""Failed at expression "" + expected.getKey() + "" at event #"" + assertionNumber, t);"
"<line10>        log.error(""GC_GRACE_SECONDS should be numeric type, Caused by: ."", nfe);"
"<line3>    LOGGER.debug(""Configuring SAML LogoutRequest for POST."");"
"<line11>              log.warn(""Hive table already exists: {}.{}"", table.getDbName(), table.getTableName());"
"<line10>    logger.debug(""event=delete_message receipt_handle="" + receiptHandle);"
"<line4>      logger.debug(""Length is {}"", new Object[] {dataLength});"
"<line9>      log.debug(""No userCache bean in context"", exc);"
<line8>      logger.error(e.getMessage(), e);
"<line5>      LOG.error(""Trying to delete an already removed node"", e);"
"<line3>      logger.debug(""Unregistering systemInformationPoller from ModbusManager"");<line8>      logger.debug(""Unregistering energyPoller from ModbusManager"");<line13>      logger.debug(""Unregistering systemStatePoller from ModbusManager"");<line18>      logger.debug(""Unregistering systemParameterPoller from ModbusManager"");"
"<line8>        this.logger.error(""Failed to execute job"", e);"
"<line2>      logger.debug(""Checking the hex key value that was loaded determined the key is empty."");"
"<line4>          log.info(""Rebalance happened "" + partition.topic() + "":"" + partition.partition());"
"<line11>      LOG.warn(""Failed to get bean properties on ({})"", clazz, e);"
<line9>        log.warn(exception, exception);
"<line4>    LOG.info(""changePassword({})"", sysUser.toString());<line7>      LOG.error(e.getMessage(), e);"
"<line2>    logger.debug(""Closing embedded node"");"
"<line16>                logger.info(""The partitioned index created on this region "" + "" "" + index);<line16>                logger.info(""Current number of buckets indexed : ""+ """"+ ((PartitionedIndex) index).getNumberOfIndexedBuckets());"
"<line2>    LOG.error(""Job Running"", jobRunningException);"
"<line6>        logger.info(""Excluding {} because it is not a valid Java "" + ""project"", project);"
"<line2>    LOGGER.info(""Changing to {} namespace"", futureNamespace);"
"<line21>      logger.debug(""op=look method={} cost={}"",joinPoint.getSignature().getName(),System.currentTimeMillis() - start);"
"<line7>        LOGGER.info(""Calling commitTask for alias: "" + alias);"
"<line3>      LOGGER.debug(""Trying to connect to address {}."", address);<line11>      LOGGER.debug(""Connected to address {}."", socket.getRemoteSocketAddress());<line15>      LOGGER.debug(""Exchanged token successfully"");"
<line16>      logger.debug(e.getMessage(), e);
"<line5>      log.error(""Encoding not supported"", e);<line21>                log.debug(""Serving request path=""+ portalPath+ "", parameters=""+ parameters+ "" with handler ""+ handler);<line24>                  log.debug(""Starting RequestLifeCycle for handler "" + handler);<line32>                log.debug(""No handler ""+ handlerKey+ "" for request path=""+ portalPath+ "", parameters=""+ parameters);<line39>            log.debug(""Finishing RequestLifeCycle for current request"");<line44>        log.error(""Could not associate the request path="" + portalPath + "" with an handler"");<line47>      log.error(""Missing valid router configuration "" + configurationPathRef.get());"
"<line1>    LOG.debug(""FRS service registered for: {}"", nodeId.getValue());"
"<line12>                        log.debug(""Received exchange: "" + e.getIn());"
"<line5>      log.error(""Parsing "" + test + "" for property '"" + property + ""'"", ex);"
<line1>    logger.error(e.getCause().getMessage());
"<line5>      Log.info(""[gwt-history] document list filter has changed"");"
"<line7>          LOGGER.error(""Error while opening POM file"", e);"
"<line28>      LOG.error(""Could not set AccessControlEntries in the ACL"", e);"
"<line4>        logger.trace(""Closing connection"");"
<line22>      log.error(systemException, systemException);
"<line12>      LOG.debug(""Cache miss for table {}"", mTableName);<line15>        LOG.debug(""Below the min pool size for table {}. Adding to the pool."", mTableName);<line21>      LOG.debug(""Cache hit for table {}"", mTableName);"
"<line1>    LOG.info(""Getting max data time for "" + dataset);"
"<line8>    LOG.trace(""Entering AdapterMpiProxyJavaImpl.findCandidates"");"
<line5>        log.warn(exception, exception);
<line37>      log.error(systemException, systemException);
"<line2>    log.trace(MessageFormat.format(""resolving {0} for Template"", input));"
"<line2>    LOG.info(""path: {}"", path);"
<line20>      log.error(systemException, systemException);
"<line3>      LOGGER.info(""Received message of type: {}"", message.getJMSType());<line8>      LOGGER.error(""Exception while handling a request from OSGP-Core: "", e);"
"<line48>        LOG.debug(""failed to clean empty index directory"", e);"
"<line5>      LOGGER.warn(""Could not match "" + file1 + "" and "" + file2);<line10>      LOGGER.info(f1.getAbsolutePath());<line10>      LOGGER.info(f2.getAbsolutePath());<line12>        LOGGER.info(""Build modified Tests {}"", f2.getAbsolutePath());<line18>      LOGGER.error(""Error when trying to compare "" + f1 + "" and "" + f2);"
"<line7>      logger.error(""Error transferring file: "" + pair.getRemoteStatus().getURL(), e);"
"<line6>      LOG.info(""Kafka Adapter disconnected for topic(s): ""+ optionHolder.validateAndGetStaticValue(ADAPTOR_SUBSCRIBER_TOPIC));"
"<line9>        logger.warn(""deleteData failed"", e);"
"<line6>      logger.error(""unexpected error"", e);"
"<line2>      LOGGER.info(""Stopping the SCMSecurityProtocolServer."");<line5>      LOGGER.error(""SCMSecurityProtocolServer stop failed."", ex);"
"<line3>    LOG.trace(""enter RFC2965Spec.validate(String, int, String, "" + ""boolean, Cookie)"");"
"<line18>          logger.fatal(name + "": Exception in txn "" + state.counter, txnException);<line20>          logger.fatal(name + "": Exception in roll "" + state.counter, rollException);<line23>      logger.fatal(name + "": outer Exception"" + state.counter, e);"
"<line9>          log.info(""Created new discovery listener for cacheName {0} and request {1}"",ilca.getCacheName(), key1);"
<line2>      logger.warn(e);
"<line25>      LOG.error(String.format(""error writing locator batch of size %s, granularity %s"",writeContexts.size(), writeContexts.get(0).getGranularity()),ex);"
"<line12>        LOG.error(""Failed to sleep TaskTracker thread"", e);<line15>      LOG.error(""Caught exception, committing suicide."", t);"
<line9>        logger.error(e.toString(), e);
"<line3>    LOGGER.info(String.format(""Claiming model [%s]"", model.getId()));<line6>      LOGGER.info(String.format(""Removing [%s]"", entry));"
"<line6>      LOGGER.error(""Error occurred while persisting auth resources."", e);"
"<line17>        log.info(""Adding persisted queries {}"", persistedQueries.size());"
"<line10>        logger.info(""Retracting {} from youtube"", mediaPackage);"
"<line1>    logger.debug(""Parsing latency from input {}"", inputLine);<line5>    logger.debug(""Did not find a latency value"");"
"<line8>        LOGGER.warn(""replicator stopped at position: {} -- restarting"", client.getGtidSet());<line13>        LOGGER.warn(""replicator stopped at position: {} -- restarting"",client.getBinlogFilename() + "":"" + client.getBinlogPosition());"
"<line2>    logger.debug(""Initializing the Lutron HomeWorks RS232 bridge handler"");<line15>    logger.debug(""Lutron HomeWorks RS232 Bridge Handler Initializing."");<line15>    logger.debug(""   Serial Port: {},"", serialPortName);<line15>    logger.debug(""   Baud:        {},"", baudRate);"
"<line32>          LOG.debug(""Found but not passing the provided filter {}: {}"", filter, obj);<line35>      LOG.error(""Could not read CSV from provided stream"", e);"
"<line32>      LOGGER.debug(""Connecting to "" + endpoint + "" using AppConfigurationCredentialProvider."");<line36>      LOGGER.debug(""Connecting to "" + endpoint + "" using Client ID from configuration file."");<line40>      LOGGER.debug(""Connecting to "" + endpoint + "" using Connecting String."");<line42>      LOGGER.debug(""Connecting to ""+ endpoint+ "" using Azure System Assigned Identity or Azure User Assigned Identity."");"
<line6>    logger.debug(SEND_MESSAGE, () -> getBytesAsString(message));
"<line3>    logger.debug(serviceparameter + "": "" + isTrue);"
"<line2>      log.trace(""WARNING!!! Overriding verification of the XPath. Your tests may pass even though they""+ "" shouldn't"");"
"<line3>    LOG.debug(""parsing variables body"");<line7>        LOG.warn(""A duplicate variable name found with name: {} and value: {}."", key, value);"
"<line7>      log.debug(""Getting dialect for session: "" + dialect);"
"<line3>      LOGGER.trace(""Unbinding SourceAttributeRestriction instance with id {}"", sourceId);"
"<line11>          logger.error(ex);<line26>          logger.error(""BeobPfad.actionPerformed"", ex);"
"<line4>      log.error(""Email address invalid: "" + desiredMailAddress);<line10>      log.debug(""Email address is owned by user: "" + mbx.getUsr().getMail());"
"<line2>    log.warn(""unsupported"");"
"<line2>      log.warn(""Only Java classloaders should be secondary"");"
"<line3>    this.logger.debug(""Setting {} collaborators to entity record [{}] via REST"",collaborators.getCollaborators().size(),entityId);"
"<line3>    logger.info(""TestEmailController#sendNCBIUploadExceptionEmail called."");"
"<line7>        LOG.info(""Please setup input path for vector and matrix and output path for result."");"
"<line18>      logger.warn(""{} health check failed due to hung write: {lastChannelWriteAttempt: {},""+ "" lastChannelWrite: {}, writeDelay: {}, writeDelayLimit: {}, rntbdContext: {},""+ "" pendingRequestCount: {}}"",channel,timestamps.lastChannelWriteAttempt(),timestamps.lastChannelWrite(),writeDelay,this.writeDelayLimit,rntbdContext,pendingRequestCount);<line25>      logger.warn(""{} health check failed due to hung read: {lastChannelWrite: {}, lastChannelRead: {}, ""+ ""readDelay: {}, readDelayLimit: {}, rntbdContext: {}, pendingRequestCount: {}}"",channel,timestamps.lastChannelWrite(),timestamps.lastChannelRead(),readDelay,this.readDelayLimit,rntbdContext,pendingRequestCount);<line39>                logger.warn(""{} health check request failed due to:"", channel, completed.cause());"
"<line7>      logger.warn("""", fex);"
"<line7>        logger.warn(""Could not load data serializer class {} so both clients of this server and this server""+ "" will not have this data serializer. Load failed because: {}"",this.className,getFullMessage(ex));<line13>        logger.warn(""Could not create an instance of data serializer for class {} so both clients of this""+ "" server and this server will not have this data serializer. Create failed""+ "" because: {}"",this.className,getFullMessage(ex));<line19>        logger.warn(""Could not register data serializer for class {} so both clients of this server and""+ "" this server will not have this data serializer. Registration failed because:""+ "" {}"",this.className,getFullMessage(ex));<line24>        logger.warn(""Could not register data serializer for class {} so it will not be available in this""+ "" JVM. Registration failed because: {}"",this.className,getFullMessage(ex));"
"<line10>            logger.info(""Message: "" + message);"
"<line2>      LOG.debug(MessageFormat.format(""Launching application: {0}{1}"", tool.getClass().getName(), Arrays.toString(args)));"
"<line3>    LOG.debug(""SimpleAuthenticator called with {}"", credentials);"
"<line4>        LOGGER.debug(""Resolve component metadata STARTED for component: {}"", componentModel.getName());<line20>        LOGGER.debug(""Invoking connector's component metadata resolver for component: {} with key: {}"",componentModel.getName(),metadataKey);<line27>        LOGGER.warn(format(""Resolve component metadata has FAILED with code: %s for component: %s"",e.getFailure(), componentModel.getName()),e);<line35>        LOGGER.warn(format(""Resolve component metadata has FAILED with code: %s for component: %s"",metadataResolvingException.getFailure(), componentModel.getName()),cause);<line45>        LOGGER.debug(""Resolve component metadata FINISHED for component: {}"", componentModel.getName());"
"<line7>      LOGGER.debug(""Unable to parse Message-Id: {}"", headers[0]);"
"<line17>              log.warn(StringBundler.concat(""Unable to find key "", key, "" in theme "", _themeId),missingResourceException);"
<line1>    logger.info(s);
"<line7>      log.info(""Failed to read the dispatch file: "" + dispatchFile);"
"<line5>    logger.debug(""sendRawCommand called with raw command: {} value: {}"", command, value);<line6>      logger.warn(""Onkyo Action service ThingHandler is null!"");"
"<line10>      logger.warn(""Failed to handle AgentStatBatch={}"", tAgentStatBatch, e);"
"<line5>      LOG.error(""Failed to rollback transacted session [session={}]"", m_transactedSession, e);"
"<line3>      LOGGER.error(""Pattern used for bucketName used in deletedMessageVault is invalid and end date cannot""+ "" be parsed {}"",bucketName);"
"<line33>    logger.trace(""roleResolverFactory = {}, roleFilter = {}, roleAttribute = {}, noResultsIsError = {}"",roleResolverFactory,roleFilter,Arrays.toString(roleAttribute),noResultsIsError);<line34>    logger.debug(""Retrieved role resolver from factory: {}"", roleResolver);<line36>    logger.debug(""Retrieved search request from factory: {}"", searchRequest);"
"<line38>      LOGGER.error(""Exception while interpreting cypher query"", e);"
<line2>      log.info(sql);<line3>      log.debug(sql);
"<line1>    Logger.debug(""getVisionOffsets({}, {})"", head.getName(), pickLocation);<line17>    Logger.debug(""Move camera to pick location."");<line20>    Logger.debug(""Perform template match."");<line36>    Logger.debug(""matchX {}, matchY {}"", matchX, matchY);<line38>    Logger.debug(""centered matchX {}, matchY {}"", matchX, matchY);<line40>    Logger.debug(""offsetX {}, offsetY {}"", offsetX, offsetY);<line41>    Logger.debug(""negated offsetX {}, offsetY {}"", offsetX, offsetY);<line44>    Logger.debug(""final, in camera units offsetX {}, offsetY {}"", offsetX, offsetY);"
<line10>      log.error(e.getMessage(), e);
"<line5>      LOGGER.error(() -> ""Error closing channel at: "" + remoteAddress, e);"
"<line4>      log.debug(""[{}] Response: filename = {}"", routingContext.get(""request-id""), filename);<line9>      log.warn(""[{}] Response: already ended!"", routingContext.get(""request-id"").toString());"
"<line1>    log.debug(""waitSensorActive[] starts"");"
<line11>      LOG.error(SETTING_NOT_FOUND_TEMPLATE,databaseConceptDefinition.getKey(),HibernateDatasourceConstants.HIBERNATE_DIRECTORY,concept,databaseConceptDefinition.getKey(),concept);
"<line2>      LOG.debug(""returning null"");<line10>        LOG.debug(""checking task: ""+ tip.getTask().getTaskID()+ "" starttime =""+ tip.startTime+ "" lastping = ""+ tip.lastPingedTimestamp+ "" run state = ""+ tip.taskStatus.getRunState().toString()+ "" monitorPeriod = ""+ monitorPeriod+ "" check = ""+ (tip.taskStatus.getRunState().equals(TaskStatus.State.RUNNING)&& (((tip.lastPingedTimestamp == 0&& ((currentTime - tip.startTime) > 10 * monitorPeriod))|| ((tip.lastPingedTimestamp > 0)&& (currentTime - tip.lastPingedTimestamp) > 6 * monitorPeriod)))));<line15>        LOG.info(""adding purge task: "" + tip.getTask().getTaskID());"
"<line15>    log.debug(""Creating view {} in {} with statement {}"", name, this.name, statementStr);"
"<line5>      LOG.info(""Couldn't check existence of key {} {} {}"",keyName,e.getStatusCode(),e.getDetails().toPrettyString(),e);"
"<line7>      logger.info(""LCM Kit input message follows: {}"", inputAsJSON);<line8>      logger.error(""Error in logging LCM Message"", e);"
"<line19>        LOG.debug(""AtlasLdapAuthenticationProvider{""+ ""ldapURL='""+ ldapURL+ '\''+ "", ldapUserDNPattern='""+ ldapUserDNPattern+ '\''+ "", ldapGroupSearchBase='""+ ldapGroupSearchBase+ '\''+ "", ldapGroupSearchFilter='""+ ldapGroupSearchFilter+ '\''+ "", ldapGroupRoleAttribute='""+ ldapGroupRoleAttribute+ '\''+ "", ldapBindDN='""+ ldapBindDN+ '\''+ "", ldapDefaultRole='""+ ldapDefaultRole+ '\''+ "", ldapUserSearchFilter='""+ ldapUserSearchFilter+ '\''+ "", ldapReferral='""+ ldapReferral+ '\''+ "", ldapBase='""+ ldapBase+ '\''+ "", groupsFromUGI=""+ groupsFromUGI+ '}');<line21>      LOG.error(""Exception while setLdapProperties"", e);"
"<line8>      logger.debug(""{} --> cldr size: {}, po size: {}, copy size: {}"", bcp47tag, cldrSize, poSize, copySize);"
"<line27>              log.warn(""Settings problem encountered at "" + problem.getLocation(),problem.getException());"
"<line2>    LOG.trace(""Creating LRUWeakCache with maximumCacheSize: {}"", maximumCacheSize);"
<line6>      logger.debug(HANDLER_IS_NULL);
"<line24>    LOG.info(""timestampDiffs="" + timestampDiffs + ""; timestamps="" + timestamps);"
"<line5>    LOG.debug(""Retrieving information about all "" + report + "" reports for account "" + accountId);"
"<line14>        LOG.warn(""{} field is ignored, couldn't find relevant field in the persistent mapping"", field);"
"<line2>      logger.trace(""setProtocolManagerFactoryStr("" + protocolManagerFactoryStr + "")"");"
<line15>      logger.warn(msg);
<line45>    log.info(sb.toString());
"<line1>    log.debug(""finding NmbNotiz instance by example"");<line8>      log.debug(""find by example successful, result size: "" + results.size());<line10>      log.error(""find by example failed"", re);"
"<line1>    LOG.debug(""Granting all permissions to user {} on instance '{}'."",user.getName(),mInstanceUri.toOrderedString());"
"<line34>      logger.error(""Unable to create MemcachedCache instance."", e);"
"<line36>      LOG.debug(""found {} for select: {}"", resultSeq.getItemCount(), selectStmt);"
"<line1>    logger.debug(""validateCloudResponseDTO started..."");"
"<line17>    LOGGER.info(""original process: "" + Util.prettyPrintXml(bundles[1].getProcessData()));<line19>    LOGGER.info(""updated process: "" + updatedProcess);"
<line8>        log.debug(noSuchFolderException, noSuchFolderException);
"<line2>      log.error(""Sorry, {} is an unknown filter."", filter);"
"<line2>      LOGGER.info(""Plugin cache miss, reload"");"
"<line2>    logger.warn(""Error instantiating revocation registry class - using default registry"");"
"<line31>      log.error(""Cannot write license info into release "" + releaseId + ""."", e);"
<line11>      LOGGER.error(e.getMessage());
"<line8>        log.info(""No internet connection on container host {}"", containerId);<line12>      log.error(""Error configuring environment"", e);"
"<line3>      logger.warn(String.format(""No queue found in the response map: %s"", requestId));<line10>        logger.error(String.format(""No queue found in the response map: %s"", requestId));<line12>      logger.error(""Error reading the queue in the response map."", e);"
"<line3>      logger.trace(""addDoubleField fieldName: {}; value: {}"", fieldName, value);"
"<line2>    LOG.debug("">>> Entering into the Administrative Interceptor, moveRequest"");<line6>      LOG.debug(""Exit from Administrative Interceptor"");<line9>    LOG.error(message);"
"<line11>      LOGGER.warn(""Component Id not found from disk component metadata"");"
"<line15>    log.info(""Clearing DB took {"" + duration + ""} ms."");"
"<line7>    LOG.info(""Entering solr wait with timeout "" + timeoutSeconds);<line13>          LOG.debug(""-"");<line23>              LOG.debug(""rstate:""+ shard.getValue().getStr(ZkStateReader.STATE_PROP)+ "" live:""+ clusterState.liveNodesContain(shard.getValue().getNodeName()));<line37>              LOG.debug(""no one is recovering"");<line40>              LOG.debug(""Gave up waiting for recovery to finish.."");<line53>      LOG.info(""Exiting solr wait"");"
"<line4>      logger.info(""{} disabled"", this.getClass().getSimpleName());<line6>    logger.info(""{} config:{}"", this.getClass().getSimpleName(), config);<line10>        logger.info(""Detected application type : {}"", DubboConstants.DUBBO_PROVIDER_SERVICE_TYPE);<line11>          logger.info(""Application type [{}] already set, skipping [{}] registration."",context.getApplicationType(),DubboConstants.DUBBO_PROVIDER_SERVICE_TYPE);<line14>    logger.info(""Adding Dubbo transformers"");"
<line2>    log.trace(XTCE_ARGUMENT);
"<line5>      LOG.debug(""Removed mapping to term "" + pm.getTerm().qualifiedName());"
"<line8>          LOG.debug(""InvocationType property: {}"", invocationType);"
"<line1>    logger.debug(""checkSystemRequestDTO started..."");"
"<line2>    LOGGER.debug(""Dh Modulus: "" + tlsContext.getServerDhModulus());"
"<line6>      log.info(""message version {} =================== "", version);"
<line29>          LOG.warn(ioe.getMessage(), ioe);
"<line1>    logger.debug(""expectedInitialNodes="" + expectedInitialNodes);<line2>      logger.debug(""Waiting until "" + expectedInitialNodes + "" nodes are up"");<line3>      logger.debug(""All nodes are up."");"
"<line2>    logger.info(""No metadata connection"");"
"<line10>        logger.debug(""Location accuracy is below required threshold: {}<={}"", accuracy, accuracyThreshold);<line11>        logger.debug(""Location accuracy threshold check is disabled."");<line19>        logger.trace(""Region {} center distance from tracker location {} is {}m"",regionName,newLocation,newDistance);<line29>      logger.debug(""Skip update as location accuracy is above required threshold: {}>{}"",accuracy,accuracyThreshold);"
"<line16>        log.debug(""Error while evaluating service expression. Thread got interrupted."");"
"<line3>      LOGGER.info(""Teardown shared!"");"
"<line28>      logger.debug(""No results to filter."");<line39>        logger.warn(""This resource has no resource identifier in the xacml response results!"");<line40>        logger.debug(""Checking: {}"", resource);<line47>        logger.debug(""Removing: {} [{}]"", resource, rid);"
"<line7>      logger.warn(""New Node {} was registered for this cluster, but this Node Identifier conflicts with {}""+ "" others: {}; each of these conflicting Node Identifiers will be removed from the""+ "" cluster"",nodeIdentifier.getFullDescription(),fullNodeIdDescriptions.size(),fullNodeIdDescriptions);"
"<line16>                          LOG.debug(""Metric name:"" + metric.name() + "" metric value:"" + metric.value());<line21>                    LOG.warn(ie);<line23>                    LOG.warn(ee.getCause());"
"<line14>              logger.info(""[doPrevPrimaryDcMigrate][result]{},{},{},{}"",cluster,shard,dc,previousPrimaryDcMessage);<line19>                      ? LogUtils.info(""Succeed, return message null"")<line21>              logger.error(""[doPrevPrimaryDcMigrate][fail]"", e);<line24>                  LogUtils.error(""Ignored:"" + e.getMessage()));"
"<line52>              log.warn(""CPOptionValuePersistenceImpl.fetchByC_ERC(long, String, boolean) with parameters""+ "" (""+ StringUtil.merge(finderArgs)+ "") yields a result set with more than 1 result. This violates the logical""+ "" unique restriction. There is no order guarantee on which result is""+ "" returned by this finder."");"
"<line9>    logger.debug(""scan key from {} to {} limit {} "", startkey, endRowKey, recordcount);<line23>          logger.error(""Error scanning keys: from {} to {} limit {} "", startRowKey, endRowKey, recordcount);<line34>      logger.error(MessageFormatter.format(""Error scanning keys: from {} to {} "", startRowKey, endRowKey).getMessage(),e);"
"<line7>      LOG.debug(""Deleted temp directory {}"", tempFile);<line11>      LOG.debug(""Deleted stale directory {}"", snapshotFile);"
"<line16>        LOG.warn(""Path ""+ status[i].getPath().toString()+ "" is not a valid checkpoint path, just remove it"");<line22>          LOG.warn(""Delete path "" + status[i].getPath() + "" failed "");<line23>          LOG.info(""Delete old checkpoint "" + status[i].getPath() + "" failed "");<line25>        LOG.warn(""Delete path "" + status[i].getPath() + "" failed "", x);"
"<line5>      LOG.debug(""Started channel {}"", channel);<line8>        LOG.warn(""receiveBufferSize (SO_RCVBUF) for input {} (channel {}) should be {} but is {}."",input,channel,expectedRecvBufferSize,receiveBufferSize);<line10>      LOG.warn(""Failed to start channel for input {}"", input, future.cause());"
"<line6>        LOG.error(""Failed to register in JMX bean "" + bean + "" at "" + objectName + "". "" + e, e);"
"<line7>      logger.error(""TTransportException writing to internal frame buffer"", e);<line9>      logger.error(""Exception writing to internal frame buffer"", e);"
<line27>      log.error(systemException, systemException);
"<line5>      LOGGER.error(""organisationIdentification is empty or null"");<line7>      LOGGER.error(""userName is empty or null"");<line9>      LOGGER.error(""applicationName is empty or null"");"
"<line1>    logger.debug(""processMgcpResponseEvent = "" + jainmgcpresponseevent);<line6>        logger.warn(""This RESPONSE is unexpected "" + jainmgcpresponseevent);"
"<line3>    logger.debug(""{} ready"", this.getClass().getName());<line3>    logger.debug(""Folder trashed resources: {}"", this.getResourceTrashRootDiskSubFolder());"
"<line45>          logger.warn(""The customer ({})'s dueTime ({}) was automatically reduced""+ "" to maximumDueTime ({}) because of the depot's dueTime ({})."",customer,dueTime,maximumDueTime,depot.getDueTime());"
<line10>      log.error(e.getMessage(), e);
"<line29>    LOG.info(""node: "" + msg);"
"<line1>    logger.trace(""Bridge alive check with #{} children."", getThing().getThings().size());<line3>      logger.debug(""No data received for {} seconds, restarting port if possible."",TimeUnit.NANOSECONDS.toSeconds(deltaLastReceived));<line7>        logger.trace(""Setting device offline if not yet done, and reset last received time."");"
"<line24>        LOG.warn(""Deadlock detected. Starting over again. Docs: {}; locked: {}. Cause: {}"",docs.getDocumentCount(),lockedDocuments.size(),e.getMessage());"
"<line21>      LOGGER.error(""StudyMetaDataDao - isValidToken() :: ERROR"", e);"
"<line14>        log.debug(""Parameter '""+ paramName+ ""' rejected in component '""+ inputComponent.getClientId()+ ""' in ComponentValidator '""+ this.getClass()+ ""'"");"
"<line1>    log.debug(""update() - structure: {}"", structure);"
"<line12>    log.debug(this + "" set initial port to "" + portStartingPoint);"
"<line2>    LOGGER.warn(""Plugin::"" + getClass().getSimpleName() + ""::stop"");<line6>    LOGGER.warn(""Plugin:SpringAppLoader stoped.."");"
"<line6>      LOG.warn(""Invalid protocol type {}."" + ""Avaiable proxy protocol type HTTP and HTTPS."", protocol);"
"<line9>    logger.info(""Creating new upload job: {}"", job);"
"<line8>        logger.debug(testName + "": status = "" + statusCode);"
"<line3>    LOGGER.debug(""get logbook for owner with id :{}"", id);"
"<line2>    log.debug(""find() - id: {}"", id);"
"<line4>        logger.info(""connect MysqlConnection to {}..."", address);<line10>      logger.error(""the channel can't be connected twice."");"
"<line10>        log.info(""Waiting for catalog service"");"
"<line5>      LOG.info(""Starting gRPC server on address {}"", mRpcConnectAddress);<line12>        LOG.info(""Registered service:{}"", serviceEntry.getKey().name());<line15>      LOG.info(""Started gRPC server on address {}"", mRpcConnectAddress);"
"<line7>      log.error(""The provided algorithm is not found "", e);"
"<line1>    LOG.debug(""parsing URL "" + url);"
"<line8>      log.debug(""Unable to deduce ContractUpdate usage for {}, using defaults"",txn.getTransactionID(),e);"
"<line4>      logger.info(""{} disabled {}"", this.getClass().getSimpleName(), ""'profiler.jdbc.dbcp=false'"");<line6>    logger.info(""{} config:{}"", this.getClass().getSimpleName(), config);"
"<line21>      logger.error(""TTransportException inside handler"", e);<line24>      logger.error(""TApplicationException inside handler"", e);<line27>      logger.error(""Exception inside handler"", e);<line35>      logger.error(""Exception writing to internal frame buffer"", ex);"
"<line23>            logger.debug(""ElementOrderValidationFunction - '""+ elementName+ ""' is not recognized for element '""+ t.getElementName()+ ""'"");<line27>          logger.warn(""ElementOrderValidationFunction - '""+ elementName+ ""' is placed before '""+ lastElementName+ ""'"");"
"<line6>      logger.error(""Error during MQTT message callback: "" + e.getMessage());"
"<line20>        log.warn(""Unhandled encryption level %s - assuming DISABLED."", encryptionLevel.name());"
"<line26>      logger.info(""update retry message success by retryQueryCondition:{}, status:{}, sendStartTime:{},""+ "" sendEndTime:{}"",retryQueryCondition,status);<line27>      logger.error(""update retry message error by retryQueryCondition:{}, status:{}, sendStartTime:{},""+ "" sendEndTime:{}"",retryQueryCondition,status);"
<line8>      log.error(exception, exception);
<line21>      log.error(systemException, systemException);
"<line5>      log.error(""I have a null head"");"
"<line12>          logger.error(""Couldn't convert url '"" + url + ""' to a file"");"
"<line22>      LOG.debug(""Using authorization provider ""+ authProviderName+ "" with resource ""+ resourceName+ "", policy engine ""+ policyEngineName+ "", provider backend ""+ providerBackendName);"
"<line6>    logger.debug(""Create the target repository"");"
"<line8>        LOGGER.warn(""There was an error adding the temporaryFile to S3"");"
"<line30>        logger.debug(""Refreshing persistent login token for profile '""+ persistentLogin.getProfileId()+ ""', id '""+ persistentLogin.getId()+ ""'"");"
<line8>      log.fatal(e);
"<line8>      logger.debug(""REDIS INVOKE START: "" + targetURL + "" action: "" + redisAction, null);"
"<line5>        log.info(""Instance for ["" + lca.toString() + ""] is null, creating"");"
"<line5>        log.debug(""Closing JDBC Connection ["" + connection + ""]"");"
"<line4>      log.error(""Segment "" + segmentName + ""Not found."");"
"<line7>      logger.error(""Error deleting value"", e);"
"<line6>      LOGGER.error(""Cannot render sub-geometries without a render context."");<line17>            LOGGER.debug(""Cannot find the model data for the render to texture geometry. "" + rttg);"
"<line6>    LOGGER.info(""Getting the size of the map on node1 -> load is triggered"");<line7>    LOGGER.info(""Map loading has been completed by now"");<line7>    LOGGER.info(""Map size on node 1: "" + sizeOnNode1);"
"<line4>        logger.info(""Stopped container"");<line6>      logger.warn(""Cannot Stop Tomcat"" + exception.getMessage());"
"<line7>        LOGGER.debug(""ADDRESS :[{}] Address:[{}] Command : [{}] RetCode[{}] Response [{}]"",enhancedMessagingException.computeAction(),me,enhancedMessagingException.computeAddress(),enhancedMessagingException.computeCommand(),enhancedMessagingException.getReturnCode());"
"<line8>        LOGGER.info(""Loaded preferences for key {} and user {} and project {}: [{}]"",aKey,aUser,aProject,result);<line10>        LOGGER.debug(""No preferences found for key {} and user {}"", aKey, aUser);<line13>      LOGGER.error(""Error while loading traits, returning default"", e);"
"<line8>          log.warn(""ServletContext INI resource '""+ servletContextPath+ ""' exists, but it did not contain ""+ ""any data."");"
"<line6>        LOG.error(""Test case has exceeded the maximum allotted time to run of: ""+ getMaxTestTime()+ "" ms."");"
"<line7>      LOG.warn(""Container is not running. Connection is not created."");"
"<line11>        log.error(""invalid memory name for action time variable - {}, for Action \""{}\"", in Conditional""+ "" \""{}\"" ({})"",sNumber,action.getTypeString(),getUserName(),getSystemName());<line19>        log.error(""invalid Millisecond value from memory, \""{}\"" ({}), value = {}, for Action \""{}\"", in""+ "" Conditional \""{}\"" ({})"",getUserName(),mem.getSystemName(),mem.getValue(),action.getTypeString(),getUserName(),getSystemName());"
"<line4>      logger.warn(""Unable to load the current recording for agent '{}': no recording found"", agentId);"
"<line3>    LOGGER.debug(""Using agent stage dir: {}"", agentDir);"
<line14>      log.error(exception, exception);
"<line11>      LOG.info(""Cannot access SecurityEvents!"");"
"<line4>      LOG.debug(""Adding exchange with key {}"", key);<line15>        LOG.debug(""Updating record with key {} and version {}"", key, version);<line17>        LOG.debug(""Inserting record with key {}"", key);"
"<line29>        log.debug(""Class not found for resolving from name as-is '"" + className + ""'"");"
"<line14>      log.warn("".removeFromNode (""+ Thread.currentThread().getId()+ "") Current level exceed parameter length, node=""+ currentNode+ ""  filterCallback=""+ filterCallback);<line56>          log.warn("".removeFromNode (""+ Thread.currentThread().getId()+ "") Could not find the index in index list for removal, index=""+ indexFound.toString()+ ""  filterCallback=""+ filterCallback);"
"<line8>        Log.error(""An exception occurred while trying to configure caches for plugin '{}':"",pluginName,e);"
"<line10>      logger.error(""FileNotFoundException in deleteSymLink"", e);<line12>      logger.error(""IOException in deleteSymLink"", e);"
"<line6>      LOGGER.debug(""findSchemaForPositionSHA: found schema_id: {} for sha: {}"", id, sha);"
"<line13>    logger.info(""generated "" + target);"
"<line2>    logger.info(""initializing InternalDataSerializer with {} services"", services.size());"
"<line21>          logger.warn(""Unsupported method "" + method, e);"
"<line9>    logger.info(""Starting new instance of Event Broker"");<line31>      logger.error(""Failed to start Event Broker"", e);"
"<line6>      logger.debug(""Set disposition as of date for next action '""+ nextAction.getName()+ ""' (""+ nextAction.getNodeRef()+ "") to: ""+ newAsOfDate);"
"<line11>      LOG.error(""The object address '""+ c.getProperty(""address"")+ ""' is not properly formatted. Check it!"");<line13>      LOG.error(port_board + "" is not a valid ethernet port to connect to"");<line24>        LOG.error(""Unable to send the message to host "" + address[0] + "" on port "" + address[1]);"
"<line2>    logger.info(""Starting mongo server at ..........."" + startMongoServerCommand);<line3>    logger.info(""started.............."");"
"<line3>    logger.debug(""hello"");"
"<line6>        log.debug(e, ""error printing status"");"
"<line6>      LOGGER.warn(""Player {} tried to reach {} blocks wide"", player.name(), dist);"
"<line5>        LOGGER.debug(""{} handling {}/{} {}"",getClass().getName(),file.getOwnerID(),file.getAbsolutePath(),evt.getEventType());<line22>            LOGGER.warn(""Can't find method for file data handler for event type {}"", evt.getEventType());<line26>      LOGGER.warn(""Can't find method for "" + MCRFileEventHandlerBase.FILE_TYPE + "" for event type {}"",evt.getEventType());"
"<line4>    LOG.debug(""Sending workflow start notification to listeners with context : {} "", context);<line8>        LOG.error(""Error in listener {}"", listener.getClass().getName(), t);"
"<line13>              log.info(""Creating {} {}"", newResource.getKind(), newResource.getMetadata().getName());<line15>              log.info(""Updating {} {}"", newResource.getKind(), newResource.getMetadata().getName());"
"<line34>      LOGGER.error(""Error occurred when extracting HTTP response body"", e);<line40>          LOGGER.error(""Unable to close buffered reader"", exception);"
"<line2>    LOGGER.info(""Using database name: "" + dbName);"
"<line3>      log.debug(""Getting deployment policy: [deployment-policy_id] "" + deploymentPolicyID);"
<line4>      log.error(exception, exception);
"<line2>    log.info(""Elastic Job: Stop {}"", serviceName());"
"<line2>      LOG.debug(""start writing Solr Update XML for export"");"
"<line20>      LOG.info(""Reading data from {} between {} and {}"",new Object[] {topicPartition, startingOffset, endingOffset});"
"<line12>          LOGGER.error(""Register IoTDB schema failed because "", e);"
"<line3>    LOGGER.info(""Starting Workers..."");<line3>    LOGGER.info(HORIZONTAL_RULER);<line3>    LOGGER.info(format(""Starting %d Workers (%d members, %d clients)..."",count(memberDeploymentPlan) + count(clientDeploymentPlan),count(memberDeploymentPlan),count(clientDeploymentPlan)));"
"<line33>        logger.error(""Error while saving the page metadata record for table {}"", tableName, t);"
"<line9>      log.debug(""Requesting unsubscribe: "" + topic);"
<line3>      logger.debug(format, arguments);
"<line3>      log.info(""...Suppressing Generate task as basicInfoMap.dfprop"");<line7>    log.info(""|                 Generate                 |"");<line7>    log.info(""|                                          |"");<line7>    log.info(""+------------------------------------------+"");"
"<line11>    logger.debug(""DONE consuming"");"
"<line3>    LOG.info(""Syncing target hoodie table with hive table(""+ hiveSyncConfig.tableName+ ""). Hive metastore URL :""+ hiveSyncConfig.jdbcUrl+ "", basePath :""+ cfg.targetBasePath);<line4>    LOG.info(""Hive Conf => "" + hiveConf.getAllProperties().toString());<line4>    LOG.info(""Hive Sync Conf => "" + hiveSyncConfig.toString());"
"<line5>      LOG.trace(""Getting key index {}"", key);<line13>    LOG.debug(""Getting key  [{}] -> {}"", key, answer);"
"<line32>                LOGGER.info(""send success metadata={}"", metadata);<line33>                LOGGER.error(""failed to send the message to Kafka"", t);<line45>                  LOGGER.info(""send success metadata={}"", metadata);"
"<line3>          LOG.trace(""setting connectTimeout: {}"", s);"
<line2>    logger.info(e.toString());
"<line11>        log.warn(""AddressbookDO with id '"" + id + ""' not found. abIds string was: "" + abIds);"
<line15>        log.debug(sb.toString());
"<line18>        logger.debug(""SQL Exception: "" + sqle.getLocalizedMessage());<line30>        logger.debug(""SQL Exception closing statement/connection in executeQuery: ""+ sqle.getLocalizedMessage());"
"<line18>          log.debug(""Garbage collection occurred and some metadata instances got garbage collected!"");<line18>          log.debug(""totalCreatedMetaDataInstances:"" + totalCreatedMetaDataInstances);<line18>          log.debug(""cachedBeanMetaDataInstances:"" + cachedBeanMetaDataInstances);<line23>      log.debug(""Out of memory error occurred."");<line23>      log.debug(""totalCreatedMetaDataInstances:"" + totalCreatedMetaDataInstances);<line23>      log.debug(""cachedBeanMetaDataInstances:"" + cachedBeanMetaDataInstances);"
"<line17>    logger.debug(""Succesfully connected to InfluxDB. Instance ready={}"", createdClient.ready());"
<line3>    LOGGER.warn(aMessage);
"<line7>            LOGGER.debug(""Cookie: "" + value + "" ("" + httpRequest.getRequestURI() + "")"");<line9>            LOGGER.error(e.getMessage(), e);<line12>    LOGGER.debug(""Fail to extract token "");"
"<line16>          log.warn(""Unable to create DiscreteIndexType for class name: "" + typeClass);"
"<line15>        logger.debug(""Exception while interacting with APM Server, trying next one."");"
"<line21>      log.info(""Kubernetes API Server at '"" + masterURL + ""' successfully contacted."");"
"<line2>      log.error(""cannot remove VideoDisplayPanel "" + source);"
"<line17>    logger.debug(""Finished task for {}; next scheduled time is at {} after a delay of {} milliseconds"",connectable,nextSchedule,delay);"
"<line17>      log.warn(""Usage estimation unexpectedly failed for {}!"", txn, illegal);"
"<line3>    logger.info(""Add CustomizedStateConfig to cluster {}, CustomizedStateConfig is {}"",clusterName,customizedStateConfig.toString());"
"<line5>      log.trace(""keyWithGrouping is:"" + keyWithGrouping);<line8>        log.trace(""keyNoGrouping is:"" + keyNoGrouping);<line11>          log.trace(""groupFieldsSet contains "" + keyNoGrouping + "" so grouping with "" + keyWithGrouping);<line16>          log.trace(""delta for "" + attrs + "" is "" + delta);<line17>          log.trace(""delta for "" + attr + "" is "" + delta);"
"<line2>      log.debug(""Positive test:: area: {}, type: {} for user: {}"", area, type, user);"
<line10>      logger.error(String.format(msg, msgArg), ex);
"<line8>      LOG.info(""Shutting down ManagedChannel failed."", e);"
"<line8>          LOG.debug(""field ('{}') has no attributes, cannot parse an attribute from it"",ToStringBuilder.reflectionToString(field));<line16>          LOG.debug(""field ('{}') has no name XML attribute, cannot parse an attribute from it"",ToStringBuilder.reflectionToString(field));<line23>          LOG.debug(""field ('{}') has no name, cannot parse an attribute from it"",ToStringBuilder.reflectionToString(field));"
"<line2>    logger.debug(""removePlanEntryById started..."");<line11>      logger.debug(ex.getMessage(), ex);"
"<line5>      log.error(""Failed to log timeseries delete"", e);"
"<line6>      LOGGER.warn(""Large document detected (id: %s). Size in bytes: %d"",item.getVertexiumObjectId(), sizeInBytes);"
"<line2>    logger.trace(""WebSocketClient stopping"");"
"<line2>    log.debug(""running in TestTransactionUtil"");"
"<line2>    LOG.info(""NoopTask.TaskStopHandler.send() invoked."");"
"<line4>        log.info(""Deleting topic {}"", topic);<line5>        log.info(""Deleted Zookeeper topic {}"", topic);<line6>        log.info(""No need to delete topic {} as it does not exist"", topic);"
<line7>      logger.error(e.getMessage(), e);
"<line6>      log.warn(""Expected plugin to have PluginClassLoader; instead found: {}"",plugin.classLoader.getClass().getName());"
"<line8>      logger.warn(""PDU String should be always valid"", e);"
"<line23>      LOG.debug(String.format(""Executing non-blocking process %s"", commandLine.toString()));<line26>      LOG.debug(String.format(""Executing blocking process %s"", commandLine.toString()));"
<line2>    log.info(s);
"<line4>    LOGGER.debug(""auto-configures HybridJwtDecoder."");"
<line22>      logger.debug(CLASS_NAME + s);
"<line4>        log.debug(""Entity {} no longer managed; ignoring scheduled connect "" + ""sensors on rebind"",OpenShiftEntityImpl.this);<line8>      log.warn(""Problem connecting sensors on rebind of "" + OpenShiftEntityImpl.this, e);"
"<line2>    LOGGER.debug(""On Data Set Post Process: {}"", dataSet.getUUID());<line7>      LOGGER.debug(""Data Set query duration: {}s"", duration);"
"<line13>      LOG.error(""run trainOnLocalClusterTest failed "", x);"
"<line3>    LOGGER.debug(""Writting Collection file "" + pathToWrite + "" "");<line4>      LOGGER.debug(""Collection file "" + pathToWrite + "" already written"");<line21>        LOGGER.error(""Error writting collection to file"", e);"
"<line11>      LOG.error(""Failed to execute the command due the exception "" + e);"
"<line4>      LOGGER.info(""{} activated"", LogLevelOverrideExtension.class.getSimpleName());"
"<line6>        log.warn(""Exception while interrupting channel: "", ex);"
"<line6>      log.debug(""setStat({})[{}]: {}"", getClientChannel(), handle, attributes);"
"<line3>    logger.debug(""Added {} unique statements, deduped {}"", addedStmts.size(), dedupCount);"
<line7>      log.error(exception, exception);
"<line36>      LOGGER.error(""Unable to append message to mailbox {}"", mailboxPath, e);"
"<line6>    LOG.debug(""got {} rows"", rowsSize);"
"<line5>        LOGGER.debug(""File was unable to be deleted: {}"", file.getAbsolutePath());"
"<line16>        log.error(msg);<line17>        log.info(msg);<line23>      log.error(""Could not find column for entity '""+ entityName+ ""' with name '""+ propertyName+ ""'."");"
"<line5>    LOGGER.warn(""Asyncrequest: {} "", asyncRequest);"
"<line2>    logger.debug(""instanceId: "" + instanceId + "" status: "" + status.name());<line13>      logger.error(""Failed to set subscription end state for instanceId "" + instanceId, e);"
"<line3>    log.info(""Running import for %s"", table.getTableName());<line9>    log.info(""Imported %s in %s"",0, table.getTableName(), nanosSince(start).convertToMostSuccinctTimeUnit());"
"<line6>            logger.warn(""Cache is being evicted because it exceeded max capacity {}. Consider increasing""+ "" cache size with the {} system property, or using an unbound cache with the""+ "" {} system property"",upperBound,SystemProperties.BOUNDED_QUERY_CACHE_SIZE,SystemProperties.ENABLE_BOUNDED_QUERY_CACHE);"
"<line4>        log.trace(""Going to evict ""+ event.getEntries().size()+ "" entries from the cache ""+ event.getCache().getName());<line5>        log.trace(""Evicted ""+ event.getEntries().size()+ "" entries from the cache ""+ event.getCache().getName());"
<line23>      log.error(systemException, systemException);
"<line5>        log.warn(""{}: queue does not exist - creating queue: address = {}, name = {}"",this.getClass().getSimpleName(),queueName.toString(),queueName.toString());"
"<line7>          log.debug(""Status was lacking createdTime, set to {} for {}"", createdTime, file);<line8>          log.warn(""Failed to get file status, will retry"", e);<line10>          log.warn(""Failed to write status mutation for replication, will retry"", e);<line13>      log.info(""Creating order record for {} for {} with {}"",file,tableId,ProtobufUtil.toString(stat));<line25>        log.warn(""Failed to write order mutation for replication, will retry"", e);"
"<line1>    logger.debug(""Running ""+ step.getId()+ ""     ""+ step.getName()+ ""       sessionId: ""+ sessionId+ ""!"");<line19>    logger.debug(orchestrationResultList);"
"<line48>      logger.debug(""Exception polling"", e);<line52>      logger.debug(""Authorization Exception during polling"", e);"
"<line17>        LOGGER.warn(""{} - using dataSource and ignoring dataSourceClassName."", poolName);<line20>        LOGGER.error(""{} - cannot use driverClassName and dataSourceClassName together."", poolName);<line23>        LOGGER.warn(""{} - using dataSourceClassName and ignoring jdbcUrl."", poolName);<line26>      LOGGER.error(""{} - jdbcUrl is required with driverClassName."", poolName);<line28>      LOGGER.error(""{} - dataSource or dataSourceClassName or jdbcUrl is required."", poolName);"
"<line1>    LOGGER.debug(""Searching Artifactory url {}"", url);"
"<line15>          logger.info(""Switchover occured where fromJvmRoute=""+ fromJvmRoute+ "" and toJvmRoute=""+ toJvmRoute+ "" with ""+ updatedRoutes+ "" updated routes."");<line18>          logger.info(""Switchover failed where fromJvmRoute=""+ fromJvmRoute+ "" and toJvmRoute=""+ toJvmRoute);<line22>        logger.info(""Switchover failed where fromJvmRoute=""+ fromJvmRoute+ "" and toJvmRoute=""+ toJvmRoute);<line22>        logger.info(""This is not a fatal failure, logging the reason for the failure "", t);"
"<line16>      logger.error(""TTransportException inside handler"", e);<line19>      logger.error(""TApplicationException inside handler"", e);<line22>      logger.error(""Exception inside handler"", e);<line30>      logger.error(""Exception writing to internal frame buffer"", ex);"
"<line14>      logger.error(""INVALID configured test protocols [{}]."", protocols);"
"<line18>        LOG.debug(""Authenticating with "" + authscope);<line27>          LOG.warn(""Required credentials not available for "" + authscope);<line28>            LOG.warn(""Preemptive authentication requested but no default "" + ""credentials available"");"
"<line9>      logger.debug("">> powering off %s ..."", RegionAndId.slashEncodeRegionAndId(regionAndId));<line12>    logger.debug("">> destroying %s ..."", RegionAndId.slashEncodeRegionAndId(regionAndId));"
"<line7>      logger.debug(""Cannot process audioStream. No coordinator has been initialized."");"
"<line10>      LOGGER.warn(""Exception while compressing security group rules"");"
"<line12>    logger.info(""start "" + pairlist);<line21>    logger.debug(""#parallel list = "" + G.size());<line22>    logger.info(""end   "" + pairlist);"
"<line4>    LOGGER.info(""Last page HTML:\n"" + html);"
"<line3>      log.debug(""Returning edges of element: ""+ source.getTitle()+ "", target type is: ""+ elementTypeId+ ""..."");<line10>          log.debug(""Edge found, source: ""+ source.getTitle()+ "", target: ""+ edge.getTarget().getTitle()+ "", edge type: ""+ edge.getType());"
"<line18>        logger.error(""meet error when recover upgrade process, file path:{}"",UpgradeLog.getUpgradeLogPath(),e);"
"<line1>    logger.debug(""Getting clientID ["" + this.clientID + ""]"");"
"<line4>      logger.debug(this.getClass().getName() + ""/handleRequest!"");<line27>      logger.error(e.getMessage(), e);"
"<line1>    LOG.debug(""Creating entities: {}"", entities);<line3>    LOG.debug(""Create entities returned results: {}"", results);"
"<line22>          logger.error(""Could not parse date ""+ billActionMatcher.group(1)+ "" for ""+ daybreakBill.getDaybreakBillId());<line22>          logger.error(ex.getMessage());"
"<line12>        logger.debug(String.format(""[Host Allocation]: intermediate failure; ""+ ""because of pagination, will start over allocation again; ""+ ""current pagination info %s; failure details: %s"",JSONObjectUtil.toJsonString(paginationInfo), ofe.getErrorCode().getDetails()));<line18>      logger.warn(""unhandled throwable"", t);"
"<line5>      log.error(""Failed to get host address"", e);"
"<line2>      logger.warn(""{} denies to handle request for {} since query is still undefined"", this, query);"
<line12>      log.error(portalException, portalException);
<line6>    LOG.info(LOG_MESSAGE_AUTH_INITIATING, new Object[] {request.getRemoteAddr()});
"<line9>        logger.warn(""Error-cell occurred: {}"", _value);<line37>        logger.error(""Unsupported data type: {}"", _dataType);"
"<line3>    LOG.debug(""ise harvest finished, outcome: {}"", counter);"
"<line16>      logger.warn(""Could not parse '{}' to a valid date"", value);"
"<line3>    LOG.debug(""Starting..."");"
"<line4>      logger.error(""Failed to get buffered writer for {}. "", filePath, e);"
"<line2>      logger.error(""Unexpected error happened while executing the task ""+ ""(if the error is known to happen it should be caught and wrapped ""+ ""into an checked exception to stop logging it as an error)"",t);<line5>      logger.debug(""Error happened during task execution"", t);<line10>      logger.error(msg, t);"
"<line2>    log.info(""Waiting until provisioned service will be completed"");"
<line3>      this.logger.debug(buildMessage(message), e);
"<line1>    log.info(""Started event send for read"");<line9>      log.error(""Exception encountered: "" + ex.getMessage(), ex);<line11>    log.info(""Completed event send for read"");"
"<line17>          log.info(String.format(""Cache distribution %s"", distribution));"
"<line3>    log.debug(""stopMonitor accountId={}, holdingId = {} holdingType={}, property={}"",accountId,holdingId,holdingType,property);<line34>        log.debug(""{} monitor stopped for funding account {}, property '{}', holding {}"",holdingType.name(),monitor.getAccountName(),monitor.getProperty(),monitor.getHoldingId());<line36>    log.debug(""Is stopped Monitor ? '{}' accountId={}, holdingId = {} holdingType={}, property={}"",wasStopped,accountId,holdingId,holdingType,property);"
"<line34>      LOGGER.debug(""******** mailfound=\""{}\"""", found);<line35>      LOGGER.debug(""*******6 mailfoundfound=\""{}\"" after cleanup 6"", found);"
"<line12>    logger.warn(""Cannot convert object to long: {}"", possibleLong);"
"<line2>    logger.debug(""getUDR() user ="" + authorId + "", title="" + title);"
"<line3>    LOGGER.error(""captured error"");"
<line7>      logger.error(e, e);
"<line5>      LOG.error(""Exception in reading data sources file {}"", dataSourcesUrl, e);"
"<line12>    log.debug(String.format(""Split SDK ready in %d ms"", (System.currentTimeMillis() - startTime)));"
"<line4>    LOGGER.info(""Starting the following bundles:[{}]"", bundlesNames);<line15>    LOGGER.info(""Finished starting bundles in [{}] ms"", (System.currentTimeMillis() - startTime));"
"<line17>      logger.error(""queryApis error:"", throwable);"
"<line3>      log.debug(""Read '{}'"", url.getFile());"
"<line6>      LOG.debug(""Ignored error: "", e);"
"<line6>      LOG.warn(""No paths specified on command line"");<line12>      LOG.warn(""Failed to run paths: {}"",paths.stream().map(Objects::toString).collect(Collectors.joining("", "", ""["", ""]"")),t);"
"<line8>    LOG.debug(""Received: {}"", message);"
"<line3>      log.info(""OS: "" + U.osString());<line3>      log.info(""OS user: "" + System.getProperty(""user.name""));"
<line10>      log.debug(line);
"<line1>    logger.trace(""setResponse(): cannot handle response {} ({})."",Command.get(responseCommand).toString(),new CommandNumber(responseCommand).toString());<line1>    logger.debug(""Gateway response {} ({}) cannot be handled at this point of interaction."",Command.get(responseCommand).toString(),new CommandNumber(responseCommand).toString());"
"<line5>        logger.info(LogMarker.DISK_STORE_MONITOR_MARKER,""The disk volume {} for log files has returned to normal usage levels and is {} full."",args);<line8>        logger.warn(LogMarker.DISK_STORE_MONITOR_MARKER,""The disk volume {} for log files has exceeded the warning usage threshold and is {}""+ "" full."",args);"
<line23>      LOG.debug(txt.toString());
"<line8>          log.warn(fieldName + "" is not of type ThreadLocal. Skip binding."");<line13>          log.warn(fieldName + "" is not a static ThreadLocal. Skip binding."");<line19>          log.warn(fieldName + "" is not initialized. Skip binding."");"
"<line3>      log.info(""{}"", Serial.getPorts());<line19>      log.info(""rest is {}"", servo.getRest());<line28>      log.error(""main threw"", e);"
"<line2>    LOG.debug(""Scheduler Info Result : {} "", jsonResult);"
"<line6>      LOG.warn(""Unable to load id {}"", id, e);"
"<line3>        log.warn(""Saw request to split root tablet, ignoring"");"
"<line6>      log.trace(""Could not find jar with key: "" + entryKey);"
"<line14>      LOG.trace(""Removing correlation key {} from timeout"", key);<line33>      LOG.debug(""Aggregation for correlation key {} discarding aggregated exchange: {}"", key, aggregated);"
"<line2>    ResourcesResourceTest.LOG.debug(""start get resource test"");<line3>    ResourcesResourceTest.LOG.debug(""created resource = '{}'"", resourceJSON);<line6>    ResourcesResourceTest.LOG.debug(""try to retrieve resource '{}'"", resource.getUuid());<line14>    ResourcesResourceTest.LOG.debug(""end get resource test"");"
"<line14>      logger.error(""error in get widgets"", t);"
"<line6>        Log.warn(""An exception occurred while dispatching a 'componentSecretUpdated' event!"", e);"
"<line2>      log.debug(""Page unload for request id: {}"", requestId);"
"<line19>        logger.warn("""", t);"
<line6>      log.error(exception, exception);
"<line5>        logger.warn(""while reloading configuration of "" + bean, e);"
"<line25>          LOG.warn(""Unable to parse pool size: "" + strId + "". Applying default value"");"
"<line23>    logger.debug(""child_ids :"" + res);"
"<line12>                    logger.warn(""Exception at showUser"", e);"
"<line2>    log.info(""Manager reports: I just got pinged!"");"
"<line10>      logger.error(""Could not compress sensei request "", e);"
<line10>                    logger.debug(ex, ex);
"<line11>    log.debug(""PUT SUCCESS: "" + resolvedPartUri);"
"<line8>          log.info(""Removed {}/ from zookeeper"", ZKUserPath);<line13>      log.error(""{}"", e.getMessage(), e);"
"<line22>      LOGGER.debug(""Error reading dependency or connecting to NPM Audit API"", e);<line34>      LOGGER.error(""YarnAuditAnalyzer failed on {}"", dependency.getActualFilePath());"
"<line1>    LOG.info(""Executing: "" + sql);"
"<line2>    LOG.info(""Subscription status changed {} : {}"", subscription.getSubscriptionId(), status);"
"<line21>      logger.debug(""Executing lucene query: {}, on region {}"", query, region.getFullPath());<line37>            logger.debug(""Executing search on repo: "" + repo.toString());<line51>      logger.debug(""Exception during lucene query function"", e);<line56>        logger.warn(""The lucene query should have waited for the index to be created"");"
<line45>            logger.error(e.getMessage(), e);
<line13>      log.error(e);
"<line1>    LOG.info(""Connection {} leaked at:"", leakInfo.getResourceDescription(), leakInfo.getStackFrames());"
"<line2>    log.warn(""Exception caught while serving static files"", cause);"
"<line8>      LOGGER.error(""Could not find "" + RABBITMQ_CONFIGURATION_NAME + "" configuration file."");"
"<line17>        logger.warn(""Unknown type requested {}, returning StringAttribute"", type);<line20>      logger.warn(""Error parsing attribute value {} of type {}: {}"", value, type, e.getMessage());"
"<line13>      LOGGER.error(""Exception during sendMessage()"", e);"
"<line3>      LOG.info(""Profiling Agent found. Per-step profiling is enabled."");<line5>      LOG.info(""Profiling Agent not found. Profiles will not be available from this worker."");"
"<line5>      log.warn(""getServiceIcon threw"", e);"
"<line2>    logger.debug(""Netatmo webhook servlet stopped"");"
"<line4>      logger.debug(""WIRETAP: properties size: "" + properties.size());<line6>        logger.debug(""key: "" + pairs.getKey() + "" value: "" + pairs.getValue());"
"<line13>        logger.error(""unsupported arg scheme: {}"", argScheme);"
"<line3>      LOG.debug(""==> RangerBasePlugin.grantAccess("" + request + "")"");<line16>      LOG.debug(""<== RangerBasePlugin.grantAccess("" + request + "")"");"
"<line8>      log.warn(""All attributes are selected !"");"
"<line3>    logger.debug(""Setting request in RequestInfo: {}"", requestInfo);"
"<line2>    log.debug(""findAllModules() - pageable: {}, moduleSearchForm: {}"", pageable, moduleSearchForm);"
"<line4>      logger.debug(""Couldn't get the global lock"");<line7>      logger.debug(""Couldn't lock the db"");<line25>      logger.warn(""Caught the following exception on transition checking"", e);"
"<line6>    LOG.trace(""Actions for Droplet {} : page {} / {} per page [{}] "",dropletId,configuration.getPage(),configuration.getPerPage(),actions.getActions());"
"<line8>      logger.warn(""Attempt to post slack message to invalid channel: "" + ex.getChannelName());"
"<line1>    log.debug(""persisting CmMasState instance"");<line3>      log.debug(""persist successful"");<line4>      log.error(""persist failed"", re);"
"<line2>    logger.info(""CounterIT.testCounters"");"
"<line3>    logger.trace(""added entry {} originally aimed at time {}"", ne, String.format(""%tc"", new Date(time)));"
"<line3>    logger.info(""{} => local. onCompleted"", getAgentInfo().getAgentKey());"
"<line10>        Log.info(""Loaded "" + idCounter + "" documents from index at "" + indexKey());"
"<line2>    logger.debug(""JarDeployer Undeploying artifactId: {}"", artifactId);<line3>      logger.debug(""JarDeployer deployedJars list before remove: {}"",Arrays.toString(deployedJars.keySet().toArray()));<line7>      logger.debug(""JarDeployer deployedJars list after remove: {}"",Arrays.toString(deployedJars.keySet().toArray()));"
<line21>      log.error(systemException, systemException);
"<line9>      logger.error(""Critical error: Study {} not found in catalog."", studyId);"
"<line5>      log.error(""Failed to submit the task for the execution [task="" + this + ']');<line17>      log.debug(""Starting clearing [grp=""+ grpEvictionCtx.grp.cacheOrGroupName()+ "", topVer=""+ grpEvictionCtx.grp.topology().readyTopologyVersion()+ "", task""+ this+ ']');"
"<line1>    LOG.info(""Spell checking file: {}"", file);"
"<line6>    logger.debug(""Search for target assets that are already localized"");"
"<line18>    logger.debug("" {} account_quota of ContainerQuota have been updated."", updatedCounter);"
"<line11>    log.info(""Starting monitoring http server in verticle {""+ getClass().getName()+ ""} on port {""+ options.getPort()+ ""}"");<line19>              log.info(""Started monitoring http server.. Port: "" + options.getPort());"
"<line6>          log.debug(String.format(""[one-after-another algorithm] [scale-up] [partition] %s has space to create ""+ ""members. [non terminated count] %s [max] %s"",partitionContext.getPartitionId(),partitionContext.getNonTerminatedMemberCount(),partitionContext.getMax()));"
"<line21>          LOGGER.info(""Unable to store certificate: {}"", certificate, e);<line32>      LOGGER.info(""Unable to add certificate(s) to trust store from URL: {}"",(decodedUrl != null) ? decodedUrl : url,e);"
"<line27>        log.warn(""unknown permission: "" + permission);"
"<line7>      log.debug(""= "" + value);"
"<line12>    LOG.info(""Succeeded after ... warning is expected..."");"
"<line10>        log.debug(""Cannot handle protected node ""+ protectedParent+ "". It nor one of its parents represent a valid Authorizable."");"
"<line11>      log.debug(""No file item was found"");"
"<line6>              logger.trace(""Transaction maintenance task started."");<line13>                logger.error(""An exception occurred while maintaining transactions"", e);<line14>              logger.debug(""Transaction maintenance task finished. originalSize={}, currentSize={}"",originalSize,transactions.size());"
"<line9>        logger.debug(""An exception occurred while getting the volume of sink '{}' : {}"",sink.getId(),e.getMessage(),e);<line14>          logger.debug(""An exception occurred while setting the volume of sink '{}' : {}"",sink.getId(),e.getMessage(),e);<line19>        logger.warn(""Error playing '{}': {}"", audioStream, e.getMessage(), e);<line24>            logger.debug(""An exception occurred while setting the volume of sink '{}' : {}"",sink.getId(),e.getMessage(),e);<line28>      logger.warn(""Failed playing audio stream '{}' as no audio sink was found."", audioStream);"
"<line2>    logger.debug(""Creating Mojito Folder Structure"");<line8>      logger.debug(""Created Mojito Folder: "" + mojitoFolder.getID());<line11>      logger.debug(""Created Project Requests Folder: "" + projectRequestFolder.getID());"
"<line4>    logger.info(""{}"", JsonCodec.INSTANCE.encode(set));"
"<line6>        LOGGER.debug(""Checking duplicate connections for newly discovered peer: {}.""+ "" All connections:\n{}"",peer,connectionsWithSameAddress.stream().map(Object::toString).collect(Collectors.joining(""\n"")));<line21>          LOGGER.debug(""Closing duplicate incoming connection with {}:{}"" + "" (established remote port {})"",incomingConnection.getRemotePeer().getInetAddress(),outgoingConnection.getRemotePort(),incomingConnection.getRemotePort());"
"<line2>      LOGGER.info(""Creating directory {}"", SAVE_DIRECTORY_PATH.toAbsolutePath());"
"<line5>    log.info(""Stopping pipelines-to-avro-from-dwca service"");"
"<line28>    LOG.info(""Init server info: {}"", serverInfo);"
"<line2>    logger.info(""New thread <""+ Thread.currentThread().getName()+ ""> created for subscribing to redis channel: ""+ channelRegEx);<line5>      logger.error(""AIDR Predict Channel pSubscribing failed for channel = "" + channelRegEx, e);<line9>    logger.info(""Exiting thread: "" + Thread.currentThread().getName());"
"<line2>    logger.info(""Diffs saved in: {}"", targetDir.getAbsolutePath());<line11>      logger.info(""Compare query: {}"", queryString);<line22>        logger.error(""Query '{}' has differences"", queryString);"
"<line4>      logger.debug(""insert trace: {}"", spanBo);"
"<line3>    LOG.info(""Snapshot of counter "" + numElements + "" at checkpoint "" + checkpointId);"
"<line1>    LOGGER.debug(""getInstanceState('{}') entered"", instanceId);<line9>        LOGGER.debug(""  InstanceState: {}"", state);<line12>    LOGGER.debug(""getInstanceState('{}') left"", instanceId);"
"<line8>              Log.error(""[execute] The history cannot be fetched."", e);<line9>            N10N.warn(I18N.CONSTANTS.historyError(), I18N.CONSTANTS.historyErrorDetails());"
"<line2>      logger.warn(""Request has not been sent for {}."", this);"
"<line2>    logger.info(""[update]{}"", args);"
"<line2>    logger.info(""Handling error: "" + e.getClass().getSimpleName() + "", "" + e.getMessage());"
<line11>    Log.info(listOutput);
"<line4>      logger.debug(""Enforcing emit on {}"", operatorThread.getName());"
"<line6>        LOGGER.debug(""Unregistering RESTEasy servlet with an alias '"" + _alias + ""'"");"
"<line6>        log.debug(""Blob s3://"" + bucketName + ""/"" + bucketKey + "" already exists"");"
"<line11>        log.info(""Skipping index settings contributor"");"
"<line50>        logger.error(""xxl-rpc remoting (url="" + url + "") response content invalid("" + resultJson + "")."", e);<line55>      logger.error(e.getMessage(), e);<line66>        logger.error(e2.getMessage(), e2);"
"<line10>            log.info(""watchdog {} apply corrective action {}"", parent.getName(), action);<line15>            log.info(""watchdog {} apply corrective global action {}"", parent.getName(), globalAction);<line26>      log.error(""watchdog threw"", e2);"
"<line11>          LOG.debug(""Created directory {}, options: {}"", path.getPath(), mergedOptions);"
"<line4>        LOGGER.info(""Trying to send query : {}"", query);<line6>        LOGGER.error(""Getting erro for query : {}"", query);"
<line5>    LOGGER.info(new JsonUtils().convertBandInfos(objects));
"<line7>      log.error(""Unable to get asset entries count"", exception);"
<line15>          log.error(e.getMessage(), e);
"<line4>      logger.info(""Terminating solver early."");"
"<line4>    log.debug(""Creating SubEquipment configuration: id="" + subEquipmentId + "" name="" + subEquipmentName);<line15>      log.debug(""SubEquipment has no alive tag id."");<line20>      log.debug(""SubEquipment has no alive tag interval."");"
"<line16>      LOG.fatal(""Fatal error in View Lifecycle worker"", t);"
"<line4>    LOGGER.info(""handle SetRandomisationSettings response for MessageType: {}"",deviceMessageMetadata.getMessageType());<line6>      LOGGER.error(DEVICE_RESPONSE_NOT_OK_LOG_MSG, exception);"
"<line5>        logger.debug(""after comma: "" + tt);"
"<line11>        logger.error(""Python operation encountered an exception: "" + ex.toString());"
"<line14>        logger.info(""Feed contains illegal 'link' element content:"" + uri);<line31>          logger.trace(""Got geometry from feed: "" + where);"
"<line3>    logger.trace(""    Shuffled cachedEntityList with size ({}) in entitySelector({})."",cachedEntityList.size(),this);"
<line13>        log.debug(sb.toString());
"<line4>    LOG.info(""Worker container: "" + connectWorkerContainer.getEndpointUrl() + "" was detected"");"
"<line10>                LOG.info(""Creating Schema History table ""+ table+ (baseline ? "" with baseline"" : """")+ "" ..."");<line23>                            LOG.debug(""Created Schema History table ""+ table+ (baseline ? "" with baseline"" : """"));<line31>                  LOG.debug(""Schema History table creation failed. Retrying in 1 sec ..."");"
"<line8>      LOG.debug(""queued job "" + id + "" in "" + elapsed + "" ms"");"
"<line3>    LOGGER.trace(""PUT: {}"", id.getFullIdPath());"
"<line12>                    LOG.warn(""skipping malformed NetworkConfig properties [{}]"", fileName);<line14>                  LOG.warn(""error reading NetworkConfig file [{}]"", fileName, readAttempt.cause());"
"<line16>        logger.info(""rootBound = "" + M);"
"<line2>    LOGGER.info(""Save: "" + profile);"
"<line5>      log.error(""Failed to execute shutdown hook"", e);"
"<line1>    log.info(""config service override of configFile="" + file);"
"<line2>      LOG.debug(""Removing completion key: {}"", key);"
<line7>        log.debug(interruptedException, interruptedException);
"<line3>    logger.debug(""Received channel: {}, command: {}"", channelUID, command);"
"<line4>    log.trace(""{} {} {} {}"", isEnter ? "">>>>"" : ""<<<<"", description, method, transaction);"
<line7>      log.debug(error, e);
<line21>    logger.debug(exception.getErrorMessage());
"<line1>    log.debug(""attaching dirty MBstnStatus instance"");<line3>      log.debug(""attach successful"");<line4>      log.error(""attach failed"", re);"
"<line2>    LOGGER.info(""FindProjects by owner fuzzy search test start................................"");<line52>    LOGGER.info(""FindProjects Response : "" + response.getProjectsList());<line56>    LOGGER.info(""FindProjects by owner fuzzy search test stop ................................"");"
"<line2>    log.info(""------ Test a*.* AND b ------"");"
"<line4>      logger.debug(""page complete: "" + complete);"
"<line4>      LOGGER.debug(format(""Preparing static statements for entity of type %s"", entityClass.getCanonicalName()));"
"<line17>      LOGGER.error(""Unexpected error on nestedForeach"", t);"
"<line11>        LOG.error(""Unable to find specified default adapter class: "" + className, cnfe);"
"<line5>          log.debug(""Rollback: "" + task.getDescription());<line8>        log.error(""Exception during NodeChangeListener's rollback task: "" + task.getDescription(), e);"
"<line14>        log.debug(""Deploying "" + servletContextName + "" from queue"");<line20>          log.error(hotDeployException, hotDeployException);<line48>          log.info(sb.toString());<line57>            log.info(sb.toString());"
"<line11>            logger.error(""Failure handling buffer send failure"", e1);<line20>          logger.error(""Failure handling queued event"", e);"
"<line3>      LOG.debug(""==> RangerTagEnricher.preCleanup()"");<line16>      LOG.debug(""<== RangerTagEnricher.preCleanup() : result="" + true);"
"<line5>    log.info(Color.BLUE + ""4-Network-1 pattern lowercase"" + Color.NORMAL);"
"<line8>      LOG.error(""Exception in flush - write/commit data to Hive"", e);"
"<line32>            log.error(e1, ""Exception thrown when closing maker.  Logging and ignoring."");"
"<line6>      LOG.info(""close {} connection and {} sessions"", m_impl != null ? 1 : 0, n);<line9>        LOG.info(""closing session {} of {}"", i, n);<line15>        LOG.info(""closing connection"");<line16>        LOG.info(""connection closed"");"
"<line8>            LOGGER.error(""Unable to migrate input '{}' (keep previous value)."", s);"
"<line16>      logger.error(""create and mapped file error."", ex);"
"<line18>            logger.error(""property:{} can not to be empty, please set!"", Constants.FS_DEFAULTFS);<line23>          logger.info(""get property:{} -> {}, from core-site.xml hdfs-site.xml "",Constants.FS_DEFAULTFS,defaultFS);<line33>          logger.warn(""hdfs.root.user is not set value!"");<line47>      logger.error(e.getMessage(), e);"
"<line10>    LOG.debug(""model: {}"", changedModel);"
"<line28>        LOG.warn(""interrupted while waiting for journal stats thread to shut down."");"
"<line2>    LOGGER.info(""Destroying Web application"");"
"<line6>        log.warn(""Message could not be built. [exception=({})]"", e.getMessage(), e);"
"<line7>        logger.debug(""The provided DN [""+ sanitizedDn+ ""] had been encoded, and was reconstituted to the original DN [""+ decodedDn+ ""]"");<line12>        logger.warn(""The provided DN [""+ sanitizedDn+ ""] had been escaped, and was reconstituted to the dangerous DN [""+ unsanitizedDn+ ""]"");"
"<line4>      LOG.error(""Exception when executing "" + r, t);"
"<line4>      LOG.debug(""BEGIN createAcl: objectIdentity: "" + objectIdentity);<line19>      LOG.debug(""END createAcl: acl: "" + acl);"
"<line4>        LOGGER.info(""Failure device message status received: {}"", status);<line6>        LOGGER.info(""Rejected device message status received: {}"", status);<line8>        LOGGER.info(""OK device message status received: {}"", status);<line10>        LOGGER.warn(""Unknown device message status received: {}"", status);"
"<line5>        log.info(""Load property {} with value {} from ENV."", key, value);"
"<line4>        logger.debug(""File created: {}"", file.getName());<line15>        logger.debug(""File already exists. Writting data to: {}"", csvFile);<line17>      logger.error(""An error occurred while creating csv file."");"
"<line2>    LOGGER.debug(""Leaving legacy_change_substitution"");"
"<line2>      LOG.debug(""calling setuseWorkflowPessimisticLocking '"" + useWorkflowPessimisticLocking + ""'"");"
"<line13>      log.debug(""{}: Creating AdminClient for {}"", reconciliation, bootstrapHostnames);"
"<line8>        LOG.trace(""Sending query to bigquery standard sql: {}"", translatedQuery);<line17>        LOG.trace(""Result of query {} is {}"", translatedQuery, result.toString());"
"<line12>        logger.debug(""property "" + name);<line12>        logger.debug(""pool name "" + strPoolName);"
"<line5>        logger.error(""Could not register ConfigDescription: {}"", configDescription.getUID(), e);"
"<line22>    log.info(""Removed topic {} with ID {}."", topic.name, record.topicId());"
"<line6>    logger.info(String.format(""Processing input index=%s, item=%s, in (%s)"", index, input[index], this));<line10>          logger.info(String.format(""Throwing exception index=%s, item=%s, in (%s)"", index, input[index], this));"
"<line11>    LOG.info(""Shutdown Hook Attached"");"
"<line2>    logger.info(""Failed connection: "" + address);"
<line9>      logger.warn(e.toString(), e);
"<line6>      log.error(""Expected "" + JexlStringBuildingVisitor.buildQuery(expectedScript));<line6>      log.error(""Actual   "" + JexlStringBuildingVisitor.buildQuery(actualScript));<line6>      log.error(""Expected "" + PrintingVisitor.formattedQueryString(expectedScript));<line6>      log.error(""Actual   "" + PrintingVisitor.formattedQueryString(actualScript));"
"<line9>      LOGGER.debug(""There was a problem converting the input to ReferenceTermType"");"
<line41>                  LOG.error(cause.getMessage(), cause);
"<line10>      LOG.error(""run trainOnLocalClusterTest failed "", x);"
"<line23>              .doOnSubscribe(() -> LOG.debug(""subscribed to write response observable""))<line30>                LOG.error(""error in here"", e);<line35>      LOG.debug(""processed {} data resource into data model '{}'"", type, dataModel.getUuid());<line39>      ConverterEventRecorder.LOG.error(message, e);"
"<line5>        LOG.info(""ForceRegistration enabled, unregistering existing MBean with ObjectName: {}"", name);<line7>        LOG.debug(""MBean already registered with ObjectName: {}"", name);<line11>      LOG.trace(""Registering MBean with ObjectName: {}"", name);<line15>      LOG.debug(""Registered MBean with ObjectName: {}"", registeredName);"
"<line5>      LOGGER.debug(""Deregistering of JDBC driver(s) is disabled!"");"
"<line1>    log.debug(""Setting sourceHelper on "" + this.name);"
"<line1>    logger.trace(""Starting pairing openHAB Client with Bosch Smart Home Controller!"");<line1>    logger.trace(""Please press the Bosch Smart Home Controller button until LED starts blinking"");<line4>      logger.trace(""Pairing with SHC {}"", ipAddress);<line20>      logger.trace(""Pairing response complete: {} - return code: {}"",contentResponse.getContentAsString(),contentResponse.getStatus());<line21>        logger.debug(""Pairing successful."");<line23>        logger.info(""Pairing failed with response status {}."", contentResponse.getStatus());<line29>      logger.warn(""Pairing failed with exception {}"", e.getMessage());<line31>      logger.trace(""Pairing failed - Details: {}"", e.getMessage());<line31>      logger.warn(""Pairing failed. Was the Bosch Smart Home Controller button pressed?"");"
"<line5>      LOG.debug(""Safe close on stream using {}"", onClose);<line8>      LOG.error(""Unable to invoke onClose closure."", e);"
"<line2>    LOG.debug(""Opening '{}' for reading."", f);"
"<line28>        logger.warn(""Receieved null temperature format, could not update Temperature Sensor channels!"");<line30>      logger.debug(""Received null bridge while updating Temperature Sensor channels!"");"
"<line2>      LOGGER.info(""Looking up service A and creating new service for A"");<line4>      LOGGER.info(""Looking up service B and creating new service for B"");"
"<line19>    log.info(""added, at: "" + addedCatalogItemUri);<line23>    log.info("" item: "" + locationItem);<line27>    log.info("" summary: "" + locationSummary);"
"<line2>    LOGGER.debug(""Creating a new JAXB context for the following class: "" + entityClass.getSimpleName());<line5>      LOGGER.error(""Unable to create a new JAXB instance"", e);<line7>    LOGGER.debug(""Start the initialize phase for the type "" + entityClass.getSimpleName());"
"<line6>                log.trace(""Get fresh values of capacity limits"");"
"<line42>        LOGGER.error(""Error cleaning up policies after test due to: "" + e.getMessage(), e);"
"<line4>      log.info(""new test run\n\n\nTEST RUN "" + count + ""\n"");<line11>        log.error(""Failures: "" + failedReporter.getFailedTests());<line14>    log.info(""\n\nCompleted "" + count + "" runs in "" + Duration.of(sw));"
"<line5>      log.trace(""Creating directory service"");<line7>      log.trace(""Creating ldap server"");"
"<line19>      logger.debug(""ASTNode.deriveUnit - before simplify - units = ""+ UnitDefinition.printUnits(derivedUnit));<line19>      logger.debug(""ASTNode.deriveUnit - after simplify  - units = ""+ UnitDefinition.printUnits(simplifiedUnit));"
"<line13>    log.trace(""Read negotiate response"");"
"<line2>    log.trace(""[{}][{}] Processing local rpc call to device actor [{}]"",request.getTenantId(),request.getId(),request.getDeviceId());"
"<line1>    logger.debug(""Encrypting vpn_users password"");<line29>    logger.debug(""Done encrypting vpn_users password"");"
"<line1>    log.debug(""attaching dirty FilterCol instance"");<line3>      log.debug(""attach successful"");<line4>      log.error(""attach failed"", re);"
"<line9>    log.info(""Interface class is undefined for the factory. Generated interface is used."");"
"<line3>      LOGGER.info(""Received message"");<line8>      LOGGER.error(""Exception: {}, StackTrace: {}"", e.getMessage(), e.getStackTrace(), e);<line9>      LOGGER.error(""UnknownMessageTypeException"", e);"
"<line3>      log.debug(""Loading database script from :"" + scriptName);"
"<line5>    log.info(""Fetch IAMCertificate info start"");<line40>        log.info(""List is empty"");<line42>      log.error(expPrefix + InventoryConstants.ERROR_CAUSE + e.getMessage() + ""\""}"");"
"<line11>      log.info(""Start session result for '{}': '{}'"", userName, ""start"", externalResult);"
"<line1>    LOG.info(""Executing operation putView"");"
<line4>      LOGGER.error(e, WebJcrI18n.cannotLoadRepositoryNames.text());
"<line10>          logger.debug(""Request message validated"");"
"<line6>        logger.warn("""", t);"
<line5>      logger.error(e);
"<line9>              LOG.trace(""{}: running {}"", this, task);"
"<line18>        logger.debug(""An unexpected status code was returned: '{}'"", statusCode);"
"<line11>        logger.trace(""Converted zero-length HBase startKey byte array to null"");<line14>        logger.trace(""Converted zero-length HBase endKey byte array to null"");<line17>        logger.debug(""HBase table {} has a single region {}"", tableName, regionInfo);<line19>        logger.debug(""Found HRegionInfo with null startKey on server {}: {}"", serverName, regionInfo);<line24>        logger.debug(""Found HRegionInfo with null endKey on server {}: {}"", serverName, regionInfo);<line35>        logger.debug(""Found HRegionInfo with non-null end and start keys on server {}: {}"",serverName,regionInfo);"
"<line10>    logger.debug(""{} : {}"", configItemName, xml);"
"<line5>      LOG.error(""Can't close "" + this.getClass().getSimpleName(), ex);"
"<line1>    log.debug(""attaching clean MbPrioritaet instance"");<line3>      log.debug(""attach successful"");<line4>      log.error(""attach failed"", re);"
<line6>        log.debug(_NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey);
"<line2>      LOGGER.info(""Skipping configure resources step"");"
"<line7>        log.debug(""Compute pool "" + computeVirtualPool.getLabel() + "" using static matching"");<line8>          log.debug(""Compute pool ""+ computeVirtualPool.getLabel()+ "" has ""+ computeVirtualPool.getMatchedComputeElements().size()+ "" statically assigned compute elements"");<line13>    log.debug(""Found ""+ computeVirtualPools.size()+ "" compute pools using static matching containing the following compute elements: ""+ staticallyAssignedComputeElementUriStrings);"
"<line74>    logger.info(""Read {} HET_ALT variants from sample {}"", numVariants, sample);"
"<line12>        log.error(""Failed to write PEM"", e);"
"<line4>      logger.debug(""Successfully published message for token:"" + token);"
"<line14>          logger.warn(""{}: Could not parse string \""{}\"" with pattern {} in element \""{}\"". Parameters: {}"",new Object[] {getReplacement(),sources[0].toString(),""yyyy-MM-dd'T'HH:mm:ssZ"",caller,getParametersAsString(sources)});"
"<line1>    logger.debug(""removing mapping from key {} to websocket session {}"", key, session.getId());"
"<line1>    logger.info(""The identifier of this node has been set to {}"", identifier);<line7>      logger.error(""Cannot save the node identifier"", e);"
"<line2>    log.debug("""");<line9>      log.error(""Recieved Message Exception."", e);"
"<line2>    LOG.debug(""save note, note: {}"", note);"
"<line66>    logger.info(this.getClass().getSimpleName() + "" altered scores for "" + altered + "" documents"");"
"<line2>    LOGGER.debug(""SaltLength: "" + msg.getSaltLength().getValue());"
"<line4>      logger.info(""Generating error at state {} before new state is set"", state);<line8>      logger.info(""Generating error at state {} after new state is set"", state);"
"<line22>                  logger.warn(""Fail to produce records into BlockingQueue due to: "" + e);<line52>        logger.warn(""Fail to join consumer thread: "" + e);"
"<line8>      LOG.warn(""Closing the writer failed: "" + ex.getLocalizedMessage());<line8>      LOG.debug(""Exception follows."", ex);<line9>    LOG.info(""Stopped dataset sink: "" + getName());"
"<line8>    logger.info(""LC status: {}"", pmHandler.getLifecycleStatus());<line11>        logger.info(""LC running not reached - wait"");"
"<line17>    LOG.debug(""Log context is prepared: {}"", registered);"
"<line2>      _Employee.LOG.debug(""updating hireDate from "" + hireDate() + "" to "" + value);"
"<line5>        logger.warn(""A JSON metadata entry's size is not supposed to exceed""+ "" kylin.metadata.jdbc.small-cell-meta-size-warning-threshold({}), resPath: {},""+ "" actual size: {}"",smallCellMetadataWarningThreshold,resPath,content.length);"
"<line44>    log.error(""could not load Mat {}"", infile);"
"<line11>        logger.warn(""A non numeric hue ID '{}' was assigned. Ignoring!"", metadata.getValue());"
"<line18>          logger.warn(""Overriding a valid <profileId,clusterId> endpoint with this {}"", adding);<line19>        logger.trace(""Adding <profileId,clusterId> <{},{}> to sender2EndPoint hashtable"",adding.profileId,adding.clusterId);"
"<line6>          LOG.info(""IndexFileManager.getFirstWriteInProgressRecord(source={}): current file={}"",this.source,record.getPath());"
"<line4>    logger.debug(blue, ""hello"");<line4>    logger.info(blue, ""hello"");<line4>    logger.warn(blue, ""hello"");<line4>    logger.error(blue, ""hello"");<line4>    logger.debug(blue, ""hello {}"", ""world"");<line4>    logger.info(blue, ""hello {}"", ""world"");<line4>    logger.warn(blue, ""hello {}"", ""world"");<line4>    logger.error(blue, ""hello {}"", ""world"");<line4>    logger.debug(blue, ""hello {} and {} "", ""world"", ""universe"");<line4>    logger.info(blue, ""hello {} and {} "", ""world"", ""universe"");<line4>    logger.warn(blue, ""hello {} and {} "", ""world"", ""universe"");<line4>    logger.error(blue, ""hello {} and {} "", ""world"", ""universe"");"
"<line3>      logger.info(""Will avoid re-indexing {} events because the newest index is defunct, so it will be""+ "" re-indexed in the background"",events.size());<line23>        logger.debug(""Received Provenance Event {} to index but it contained no information that should be""+ "" indexed, so skipping it"",event.getEventId());<line47>      logger.error(""Failed to reindex some Provenance Events"", ioe);"
<line22>    log.debug(stacktrace.toString());
"<line3>    logger.debug(""GetDisseminationHandler/handleRequest!"");<line11>      logger.debug(""Retrieved SOAP Request Objects"");<line12>      logger.error(""Error obtaining SOAP Request Objects"", af);<line20>      logger.error(""Error obtaining parameters"", e);<line22>    logger.debug(""Extracted SOAP Request Objects"");<line45>      logger.error(e.getMessage(), e);"
"<line3>    logger.info(""Unregistering Service {}@{} and cleaning its running jobs"", serviceType, baseUrl);"
"<line1>    logger.info(""Producers connected - {}"", (Object) wires);"
"<line11>    log.info(b.toString() + ""]"");"
"<line2>    LOG.info(""Loading resource. RelativePath = {}."", relativePath);"
"<line1>    LOG.trace(""converting homeCommunityId ["" + homeCommunityId + ""] to assigning authority"");"
"<line44>        logger.error(""Exception while invoking onAsCreated"", ee);"
"<line2>    LOG.info(""delete specific not existing user"");"
"<line8>      LOG.error(""MessageResourcesFactory.createFactory"", t);"
"<line6>        LOG.warn(""Exception while closing channel for log file:"" + logId);"
"<line3>      log.info(""clear all metaData cache"");"
"<line28>      LOG.warn(""The cluster does not contain node: {}"", NodeBase.getPath(node1));<line31>      LOG.warn(""The cluster does not contain node: {}"", NodeBase.getPath(node2));"
"<line3>      logger.debug(""Starting the job with job id {}"", execution.getJobId());<line11>        logger.error(""Unable to invoke dead event on job"", t);<line15>    logger.debug(""Starting job {} with execution data {}"", job, execution);"
"<line24>      LOG.warn(""Unable to find a ParamConverterProvider for type Date"");"
"<line17>      LOGGER.debug(""Recorded DDL statements for database '{}': {}"",schemaChange.getDatabase(),schemaChange.getDdl());"
"<line18>      uiMessageStack.error(e.getMessage());<line24>      LOGGER.info(""JsonSyntaxException"", e);<line28>      LOGGER.error(""Internal Server Error"", e);"
"<line7>          LOGGER.info(""alerts evaluated so far: {}"", jobCounter.get());<line10>        LOGGER.info(""Execution was interrupted."");<line13>        LOGGER.error(""Exception in alerter: {}"", ExceptionUtils.getFullStackTrace(ex));<line15>    LOGGER.warn(""Alerter thread interrupted. {} alerts evaluated by this thread."", jobCounter.get());"
<line7>        log.debug(exception, exception);
"<line6>      logger.warn(""Could not create a short from the string "" + value);"
"<line23>    logger.info(""Saving {}"", user);"
"<line5>    LOGGER.info(""{} ssl reloaders registered"", reloaders.size());"
"<line14>        logger.warn(""Exception in dict\n {}"", msg);<line16>        logger.info(msg);"
"<line2>    logger.trace(""getHomeDirectory"");"
"<line2>    this.logger.debug(""ConnectGPRSRequest"");"
"<line16>            log.info(""Message {}"", i);"
"<line2>    logger.error(""Exception thrown while initializing consumers."", throwable);"
"<line6>      LOG.debug(String.format(""File '%s' can't be converted to URL"", file), e);"
"<line6>    logger.info(""{} try to add Node {} To ReplicaSet {}"", user, nodeID, replicaSetID);"
"<line13>        logger.warn(""SlotManager in {} read size does not equal to file size: {}/{}"",slotFilePath,read,slotFile.length());<line17>      logger.warn(""Cannot deserialize slotManager from {}"", slotFilePath, e);"
"<line3>      Log.debug(""Annotating WorkflowRun "" + workflowSWID + "" with skip="" + skip + "", Att = "" + att);<line5>        Log.info(""Processing does not have a skip column!"");<line13>      Log.error(""IOException while updating study "" + workflowSWID + "" "" + ex.getMessage());<line15>      Log.error(""JAXBException while updating study "" + workflowSWID + "" "" + ex.getMessage());<line17>      Log.error(""ResourceException while updating study "" + workflowSWID + "" "" + ex.getMessage());"
"<line14>        LOGGER.warn(""Multiple identifiers found for Person: "" + pTO.getId());"
"<line6>        logger.warn(""Can not load lang detector models"", e);"
"<line26>              Log.error(""Error while updating a reminder or monitored point."", caught);<line28>              N10N.warn(I18N.CONSTANTS.reminderUpdateError(),I18N.CONSTANTS.reminderUpdateErrorDetails());<line29>              N10N.warn(I18N.CONSTANTS.monitoredPointUpdateError(),I18N.CONSTANTS.monitoredPointUpdateErrorDetails());"
"<line6>    LOG.info(""### closed down the test case: "" + getName());"
"<line9>    LOG.debug(""Init output for table '{}' with schema: {}"", tableName, tableSchema);"
<line10>      log.error(bundleException, bundleException);
"<line23>      LOG.info(""Authorization Response url is: "" + driver.getCurrentUrl());"
"<line12>        LOG.error(""Unable to obtain AtlasAuthorizer"", e);"
"<line10>    LOG.info(""{}"", details);<line12>      LOG.info(""Consumer: "" + result.getElement());<line16>      LOG.info(""Producer: "" + result.getElement());"
"<line21>      log.error(""Failed ({}) to send channel open packet for {}: {}"",e.getClass().getSimpleName(),channel,e.getMessage());"
"<line1>    LOG.info(""Simulating a task which takes "" + getEndpoint().getDelay() + "" millis to reply"");<line4>      LOG.info(""Simulating a failure at attempt "" + count);<line10>      LOG.info(""Setting reply "" + reply);<line11>    LOG.info(""Callback done(false)"");"
"<line29>      logger.error(""Error generating new version type - {}"", formTypeCode, t);"
"<line3>    logger.debug(""Connecting to NiFi instance"");<line5>    logger.debug(""Established connection to NiFi instance."");<line6>    logger.debug(""Sending {} Command to port {}"", request, port);"
"<line11>            LOGGER.debug(""copying database {} to {}"", db.toPath(), temp.toPath());"
"<line12>              LOG.debug(""Closing cache for feneric compiler "" + compiler.getId());"
"<line15>    LOGGER.info(""Setting number of reducers : "" + job.getNumReduceTasks());<line19>    LOGGER.info(""Input path dir: "" + inputPathDir);<line20>      LOGGER.info(""Adding input:"" + inputPath);"
"<line14>                LOGGER.info(""Job stats: {} pending, {} finished, {} cancelled"",state.size() - done - cancelled,done,cancelled);"
"<line2>    logger.debug(""[scheduledRecoverAlertReport] unRecoveredAlerts: {}"", unRecoveredAlerts);"
"<line1>    logger.trace(""getHouseStatusCommsJob() initiated by {} will process HouseStatus."",Thread.currentThread());<line2>      logger.trace(""getHouseStatusCommsJob(): => GetHouseStatus() => updates received => synchronizing"");<line4>      logger.trace(""getHouseStatusCommsJob(): => GetHouseStatus() => no updates"");<line5>    logger.trace(""getHouseStatusCommsJob() initiated by {} has finished."", Thread.currentThread());"
"<line3>      this.logger.debug(""Finalized classloader "" + toString());"
"<line6>      log.error(""Bad record "" + input, e);"
<line18>      log.error(systemException, systemException);
"<line12>                    log.error(""Print error."", ex);"
"<line17>      logger.debug(""results of getAllAuthorities1(): "" + res);"
"<line1>    LOGGER.debug(""Checking if field {} is queried by value comparison {}"", field, q);"
"<line9>      log.debug(""setNegotiationResult({}) Kex: server->client {} {} {}"",this,guess.get(KexProposalOption.S2CENC),guess.get(KexProposalOption.S2CMAC),guess.get(KexProposalOption.S2CCOMP));<line9>      log.debug(""setNegotiationResult({}) Kex: client->server {} {} {}"",this,guess.get(KexProposalOption.C2SENC),guess.get(KexProposalOption.C2SMAC),guess.get(KexProposalOption.C2SCOMP));"
"<line13>            log.debug(""cls classloader: ""+ cls.getClass().getClassLoader()+ "" uid: ""+ cls.getClass().getClassLoader().hashCode());<line20>            log.debug(""cls2 classloader: ""+ cls.getClass().getClassLoader()+ "" uid: ""+ cls.getClass().getClassLoader().hashCode());<line20>            log.debug(""cls==cls2: ""+ String.valueOf(cls == OpenmrsClassLoader.getInstance().loadClass(classString)));"
"<line4>      log.debug(""Skipping reset of partition {} since it is no longer assigned"", tp);<line5>      log.debug(""Skipping reset of partition {} since reset is no longer needed"", tp);<line6>      log.debug(""Skipping reset of partition {} since an alternative reset has been requested"", tp);<line7>      log.info(""Resetting offset for partition {} to position {}."", tp, position);"
"<line9>        LOG.warn(""stop: InvalidPathException resolving syncPoint {}, exception {}"", syncPoint, e);"
"<line17>      log.debug(""Determined user DN prefix [{}] and suffix [{}]"", prefix, suffix);"
"<line5>      logger.debug(""ServerToClientFunctionResultSender setting exception {} "", exception);<line17>          logger.warn(String.format(""Exception on server while executing function : %s"", this.fn),exception);<line18>            logger.debug(""ServerToClientFunctionResultSender sending Function Exception : "");"
"<line10>      logger.info(""Failed to save pushMessageInformation: {}"", e.getMessage());<line10>      logger.debug(""Details:"", e);"
"<line3>      LOG.info(""A dead datanode is detected. {}"", datanodeDetails);<line9>      LOG.error(""DeadNode event for a unregistered node: {}!"", datanodeDetails);"
"<line15>      logger.error(""Cannot find delivery Tag in path:"" + deliveryTagPath + "" for this experiment"");"
"<line3>      LOG.info(""SnapshotString in AlertSnapshotBean {} is empty, return an empty map instead"", getId());<line11>            LOG.error(""Unable to parse String {} to Status"", statusString);"
"<line15>            .forEach(s -> logger.debug(""Parameter: {}"", s));<line16>            .forEach(e -> logger.debug(""Property: {}={}"", e.getKey(), e.getValue()));<line18>            .forEach(e -> logger.debug(""Env: {}={}"", e.getKey(), e.getValue()));<line18>        logger.debug(""Option: {}"", options);<line38>                logger.debug(""Destroying LaContainer.."");<line52>        logger.debug(""SuggestCreator is stopped."", e);<line53>        logger.info(""SuggestCreator is stopped."");<line56>      logger.error(""Suggest creator does not work correctly."", t);<line63>    logger.info(""Finished SuggestCreator."");"
"<line1>    logger.debug(""Cancel all jobs."");"
"<line3>    LOGGER.debug(""get agency identifier={}"");"
"<line27>            LOGGER.warn(""Unable to process drop file"", mue);"
"<line23>              logger.error(""Authz policy file VFS read error: "" + file.getFileName(), e);"
"<line6>      log.error(""SHERPA URI missing for API response item"");<line11>      log.error(""SHERPA internal ID missing for API response item"");"
"<line4>        log.info(""Request to terminate work. Max duration reached"");<line9>        log.info(""Request to terminate work. Number of samples reached"");<line20>      log.info(""Request to terminate work. All planned invocations are finished"");"
<line1>    LOG.info(content);
"<line15>          () -> LOGGER.error(String.format(""Attachment %s not found"", attachment.getBlobId())));<line16>      LOGGER.error(String.format(""Attachment %s is not well-formed"", attachment.getBlobId()), e);"
"<line1>    LOG.info(""update broker state from {} to {}"", BrokerState.codeOf(brokerState), newState);"
"<line8>      log.debug(""START object '{}' from = {}"", operationParams.tableName, recoveryValue);<line11>      log.debug(""SKIP object '{}' by = {}"", operationParams.tableName, recoveryValue);<line17>        log.debug(""RESTORED object '{}' from = {}"", operationParams.tableName, recoveryValue);<line19>        log.debug(""GO NEXT object '{}' by = {}"", operationParams.tableName, recoveryValue);"
"<line3>      log.info(""getMappedIdOrGtfsId (contains) ("" + someId + "")="" + util);<line6>    log.info(""getMappedIdOrGtfsId (config) ("" + someId + "")="" + util);"
"<line22>      logger.warn(String.format(""%s"", LogUtil.getStackTrace(e)));"
"<line6>      LOGGER.error(""Error while delete documents"", e);"
"<line1>    LOG.trace(""[{}][{}] bratRenderLaterCommand"", getMarkupId(), vis.getMarkupId());"
"<line3>      LOG.error(""A valid path is needed for config setting {}"", ScmConfigKeys.OZONE_SCM_DATANODE_ID_DIR);"
"<line4>    logger.debug(""About to skip task with id '{}' as user '{}'"", taskId, userId);"
"<line31>          LOG.warn(""Error generating json binary type from object."", e);"
"<line20>        log.warn(""[cloneGroupOffset], topic config not exist, {}"", topic);<line32>          log.warn(""[cloneGroupOffset], the consumer group[{}], topic[{}] not exist"",requestHeader.getSrcGroup(),topic);"
"<line12>    LOGGER.info(""progress:"" + percent);"
"<line11>      log.error(""Failed to list hooks."", e);"
<line7>      log.error(exception, exception);
"<line4>      log.trace(""Mem: QUERY "" + key1 + "" = "" + value1 + "" AND "" + key2 + "" = "" + value2);<line17>      log.trace(""Mem:    -> "" + list.size());"
"<line10>      LOG.debug(""Closing connection: {} with timeout: {} ms."", conn, closeTimeout);"
"<line33>              LOG.warn(""No texture id or no texture coordinates for texture(""+ i+ ""), ignoring this texture."");<line44>        LOG.warn(""No texture id or no texture coordinates for texture(0), this fragments textures."");"
"<line41>      LOGGER.info(getClass() + "" "" + infoLog + "", payload:\n"" + soaException.getMessage());<line42>      LOGGER.error(e.getMessage(), e);"
<line20>      log.error(systemException, systemException);
"<line13>      log.debug(""#createGroup - Failed to create the group {}. The exception occure"",groupNameValue.getKey(),e);"
"<line2>    log.info(""Checking input file(s) is/are present..."");"
"<line1>    logger.info(""develop mode is: {}"", CommonUtils.isDevelopMode());<line4>        logger.warn(""task: {} exec local path is empty."", taskExecutionContext.getTaskName());<line7>        logger.warn(""task: {} exec local path is '/', direct deletion is not allowed"",taskExecutionContext.getTaskName());<line11>        logger.info(""exec local path: {} cleared."", execLocalPath);<line12>        logger.error(""delete exec dir failed : {}"", e.getMessage(), e);"
"<line23>                          logger.info(""DatasetVersion id=""+ ds.getGlobalId().toString()+ "" v""+ versionNumber+ "" submitted to Archive at: ""+ dv.getArchivalCopyLocation());"
"<line32>                      LOG.warn(""Failed to find user: "" + uid);<line48>                LOG.info(""Thread "" + count + "" execution time: "" + duration);<line61>    LOG.info(""Total execution time: ""+ duration+ "" after execution: ""+ (threadCount * threadIterationCount));"
"<line18>        LOGGER.info(""Nothing to do since operation has not been configured yet"");"
"<line2>      log.warn(String.format(""Bundle %s already loaded"", bundleName));<line5>      log.info(String.format(""Loading bundle %s"", bundleName));"
"<line20>        log.info(""Instrumentation"", ""deploy /resource/bin/"" + srcName + "" to "" + trg);"
"<line2>    LOGGER.debug(""ServerNameListLength: "" + msg.getServerNameListLength().getValue());"
<line14>      logger.warn(t);
<line5>        log.info(Hex.toHexString(actual.getContractCallResult().toByteArray()));
"<line13>                  log.debug(""Inverted a document in {}us"", (System.nanoTime() - start) / 1000);<line17>                log.warn(""Exception while inverting a document"", e);"
"<line8>      LOG.error(""Catch exception with ldap host["" + host + ""]."", e);<line10>      LOG.info(""Failover to ldap host:"" + host + ""."");"
"<line4>    logger.debug(""channel {} updated with {} ({})"",channel.getUID().getAsString(),value,channel.getAcceptedItemType());"
"<line6>        log.info(""Creating a new SAML keystore at "" + samlKeyStoreFile);"
"<line15>    log.info(""downloading JFlex 1.4.3 from '"" + DOWNLOAD_URL + ""' ..."");<line16>    log.info(""finished downloading. Now extracting to "" + downloadTo);"
"<line2>    log.debug(""teamExists: "" + exists);"
"<line2>    LOG.trace(""enter CookieSpecBase.parse("" + ""String, port, path, boolean, String)"");"
"<line15>      logger.debug(""Can't write file {}: {}"", currentLocalListingFile, e.getMessage());"
"<line6>        LOGGER.debug(""checking: %s"", element.getId());"
"<line6>    LOG.debug(""Audit: {}/{} performed request {} ({}) at time {}"",who,fromAddress,whatURL,whatAddrs,whenISO9601);"
"<line3>    LOG.debug(""Calling OpenstackSwiftContainerResource.getMetadataShouldSucceed()"");"
"<line6>      logger.error(""Failed to deserialize device {} from buffer"", pathString);"
"<line1>    log.debug(""Closing rmi ..."");<line3>      log.debug(""Closing socket ..."");"
"<line6>      log.error(""Error in checking Mongo config availability"", e);"
<line5>    log.info(new String(writer.getBody()));
"<line5>    LOGGER.debug(""Registering {} component IDs."", componentIds.size());"
"<line11>          LOGGER.warn(""Match all mapping ignored: {} doesn't match expected format of""+ "" metacardAttribute=userAttribute"",mapping);"
"<line12>      logger.error(""Order by and group by on different field are not supported simultaneously"");"
"<line10>    LOG.info(""Copy job progress: {}/{}"", progress.getProgress(), progress.getProgressMax());"
"<line12>                    logger.warn(""Exception at destroyFavorite"", e);"
"<line2>    LOGGER.debug(""Processing public lighting set transition response message"");<line20>      LOGGER.error(""UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up."", e);<line20>      LOGGER.debug(""correlationUid: {}"", correlationUid);<line20>      LOGGER.debug(""messageType: {}"", messageType);<line20>      LOGGER.debug(""messagePriority: {}"", messagePriority);<line20>      LOGGER.debug(""organisationIdentification: {}"", organisationIdentification);<line21>      LOGGER.debug(""responseMessageResultType: {}"", responseMessageResultType);<line21>      LOGGER.debug(""deviceIdentification: {}"", deviceIdentification);<line21>      LOGGER.debug(""osgpException"", osgpException);<line24>      LOGGER.info(""Calling application service function to handle response: {}"", messageType);"
"<line1>    log.debug(""getting SysExportNotiz instance with id: "" + id);<line6>        log.debug(""get successful, no instance found"");<line7>        log.debug(""get successful, instance found"");<line10>      log.error(""get failed"", re);"
"<line5>              LOG.info(""Exiting process now."");"
"<line1>    LOG.warn(""Lost a agent {}"", agentId);"
"<line5>    logger.debug(""Registering Channel Listener for monitoring..."");"
"<line3>      logger.debug(""channel is null"");<line7>      logger.debug(""channelTypeUID for channel {} is null"", channel);<line11>      logger.debug(""channelTypeId for channelTypeUID {} is null"", channelTypeUID);"
"<line7>        LOG.info(""loading configuration ["" + currentConfigurationName + ""]"");<line16>            LOG.debug(""configuration [""+ currentConfigurationName+ ""] got exception creating/retrieving classloader type [""+ classLoaderType+ ""] errorMessage [""+ e.getMessage()+ ""]"");<line18>          LOG.debug(""configuration [""+ currentConfigurationName+ ""] found classloader [""+ ClassUtils.nameOf(classLoader)+ ""]"");<line34>        LOG.info(""configuration ["" + currentConfigurationName + ""] loaded successfully"");"
"<line3>      LOGGER.info(""process() event:{}"", watchedEvent);"
<line18>      LOG.warn(e.getLocalizedMessage());
"<line18>        log.debug(""Setting up KEYCLOAK-SAML auth method for WAR: "" + deploymentUnit.getName());<line23>            log.warn(""Failed to set up KEYCLOAK-SAML auth method for WAR: ""+ deploymentUnit.getName()+ "" (loginConfig == null)"");"
"<line3>    LOGGER.debug(""get logbook for profile with id :{}"", id);"
"<line1>    LOG.trace(""Removing table "" + tableName + ""."");<line5>      LOG.error(e.getMessage(), e);<line6>    LOG.trace(""Table "" + tableName + "" removed."");"
"<line35>      logger.debug(""register command consumer [tenant-id: {}, device-id: {}, adapter-instance-id {},""+ "" lifespan: {}s]"",tenantId,deviceId,adapterInstanceId,lifespan.getSeconds());"
"<line16>    LOGGER.debug(type.getSimpleName() + ""-queryNames took "" + (System.currentTimeMillis() - start) + ""ms"");"
"<line30>    logger.debug(LoggingMarkers.PERFORMANCE,""Saving {} indexer search entities took {}ms"",countEntities,stopwatch.elapsed(TimeUnit.MILLISECONDS));"
<line33>        logger.error(message, e);
"<line6>      log.info(""Checking if user {} is active: {}"", name, foundUser);<line13>    log.info(""User {} is ready for use"", name);"
"<line28>        LOG.error(""Inconsistent ""+ fileElement.getElementType()+ "" tree in ""+ this+ ""; nodeLength=""+ nodeLength+ ""; fileLength=""+ fileLength,attachments.toArray(Attachment.EMPTY_ARRAY));"
"<line5>      LOG.warn(""Unable to determine local hostname "" + ""-falling back to \"""" + LOCALHOST + ""\"""", e);"
"<line6>      LOGGER.trace(""Received initial result partition read request for JobId: ""+ jobId+ "" partition: ""+ partition+ "" on channel: ""+ ccb);<line11>      LOGGER.warn(""Failed to initialize result partition reader"", e);"
"<line5>    logger.debug(crfName);<line5>    logger.debug(itemGroupLabel);<line11>    logger.debug(""OID : "" + oid);"
"<line3>      logger.debug(""Given time unit {} is not supported. Will keep current configuration."", timeUnit);<line8>      logger.debug(""Validation of new configuration values failed. Will keep current configuration."", e);"
"<line4>        log.debug(""HTTP header name: "" + key);<line5>          log.debug(""  "" + h);"
"<line2>    LOG.info(""\n===CommitDaoTest.testGetByModule===\n"");<line4>      LOG.info(commit.toString());"
"<line2>    LOGGER.debug(""Setting cipher for client to use "" + keySetType);"
"<line2>    LOGGER.debug(""SessionIdHit: "" + message.getSessionIdHit().getValue());"
"<line10>      log.warn(""Unable to send notification using template "" + templateId + "" to entity "" + entityId, e);"
"<line15>              logger.debug(""BucketRegionQueue: mark event {} as possible duplicate due to""+ "" change of primary bucket."",object);<line20>            logger.debug(""The value against key {} in the bucket region queue with id {} is NULL for the""+ "" GatewaySender {}"",key,getId(),this.getPartitionedRegion().getParallelGatewaySender());"
"<line8>      logger.error(""Error in searchIdeas"", t);"
"<line8>                LOG.info(""Succeeded in getting bundle {} for topic - [{}]"", bundle2, topicName);<line11>                LOG.warn(""Failed to get bundle for topic - [{}] {}"", topicName, ex.getMessage());"
"<line30>      LOG.error(""Expression {} could not be evaluated"", toString());<line32>    LOG.trace(""Evaluating expression {}"", toString());"
"<line13>      LOG.warn(String.format(""Failed to resolve endpoint builder from resource '%s/%s'"", RESOURCE_PATH, builder));"
"<line15>        LOG.info(""download file:{} to local:{}"", filePathOnSftp, fileLocalPath);"
<line18>                log.error(portalException, portalException);
"<line12>        log.error(""Exception in getClientAuthorization: "" + exception.getMessage());<line25>      log.error(""Exception in loginProxy: "" + exception.getMessage());"
"<line4>      LOG.trace(""Checked if full and remaining capacity is {}"", remaining);"
"<line9>        logger.warn(""Thread was interrupted"");<line11>    logger.error(""Can't recover from the Journal - ""+ ""the server hasn't initialized after ""+ i+ "" seconds."");"
"<line2>      logger.debug(""Checking sip Servlet Mapping for following request : "" + sipServletRequest);<line7>        logger.debug(""Following mapping rule didn't match : servletName => ""+ sipServletMapping.getServletName()+ "" | expression = ""+ sipServletMapping.getMatchingRule().getExpression());"
"<line2>    log.info(""runTests{}"");"
"<line6>        Log.debug(""JDBCAuthProvider: Automatically creating new user account for "" + username);"
"<line16>    LOG.info(""Scanner running with "" + connectionPoolForThreads.size() + "" kudu connections"");"
<line13>      LOGGER.error(e.getMessage());
"<line7>      log.info(""Restart node [node="" + nodeIdx + "", client="" + clientMode + ']');"
"<line9>        LOGGER.debug(""Accepting..."");<line16>      LOGGER.warn(""Error: "", e);"
"<line4>      LOG.debug(""Opened database from '{}'."", runtimeDirectory);"
"<line8>      log.warn(""Error during attribute statement condition evaluation, condition '""+ statement.getCondition()+ ""' is invalid. Skipping statement.\n""+ e.toString());<line9>        log.trace(""Full stack trace of the problematic attribute statement error"", e);<line12>      log.debug(""Condition evaluated to null value, assuming false"");<line15>      log.trace(""Condition ""+ statement.getCondition()+ "" evaluated to ""+ result+ "" for ""+ context.get(ContextKey.entityId.name())+ "" in ""+ context.get(ContextKey.groupName.name()));"
"<line8>      log.info(""Container succesfully stopped!"");<line10>      log.debug(""Stop Container"", e);"
"<line13>        LOG.info(""wrong csrf"");"
"<line15>      LOG.error(""Fatal error while running command line interface."", strippedThrowable);"
"<line4>      LOG.info(getName() + "" completed successfully!"");<line5>      LOG.error(getName() + "" did not complete"", e);"
"<line12>      LOG.warn(""Unable to read property: {})"", propertyName, ex);"
"<line12>        log.error(""EOF ACK received but EOF not sent"");<line13>        log.info(""CFDP received EOF ACK"");"
"<line16>      LOG.debug(""Transform value to ODocument (UNION), type:{}, storeType:{}"",new Object[] {innerSchema.getType(), type1, storeType});"
"<line18>      LOG.error(""Error finding objects"", th);"
"<line11>        logger.error(""Failed to perform tasks when enumerator was finished"", e);"
"<line5>      LOG.debug(""{}: No encoding parameter using default charset: {}"", type, encoding);"
"<line4>      LOG.debug(""{}: got seq={} (first request), set nextToProcess in {}"",requests.getName(),seqNum,this);<line5>      LOG.debug(""{}: got seq={} in {}"", requests.getName(), seqNum, this);"
"<line3>    logger.debug(""Entered executePatchAAI method with distrubutionId: {} and distributionStatus: "",distributionId,distributionStatus);<line10>      logger.debug(""Executed RequestDB getWatchdogServiceModVerIdLookup with distributionId: {} ""+ ""and serviceModelVersionId: {}"",distributionId,serviceModelVersionId);<line10>      logger.debug(""ASDC Notification ServiceModelInvariantUUID : {}"", serviceModelInvariantUUID);<line13>        logger.debug(error);<line21>      logger.debug(""Target A&AI Resource URI: {}"", aaiUri.build().toString());<line24>      logger.debug(""A&AI UPDATE MODEL Version is success!"");<line25>      logger.debug(""Exception occurred on executePatchAAI : {}"", e.getMessage());<line25>      logger.error(""Exception occurred"", e);"
"<line14>              logger.info(""event=lp_receive vto=120 wt=20"");<line18>              logger.info(""event=message_found duration="" + (ts2 - ts1));<line22>      logger.info(""event=send_message queue_url="" + queueUrl);<line36>        logger.info(""event=receive"");<line38>          logger.info(""event=message_found delay="" + (System.currentTimeMillis() - ts));<line43>      logger.error(""test failed"", ase);"
"<line6>      logger.info(""Member {} is {}equivalent or in the same redundancy zone."", member, relationship);"
"<line2>    logger.debug(""======== Azure Policy Evaluation Rule started ========="");<line39>          logger.debug(""======== Azure Policy Evaluation Rule ended with annotation {} : ========="",annotation);<line44>      logger.error(""error: "", exception);<line46>    logger.debug(""======== Azure Policy Evaluation Rule ended========="");"
"<line7>          log.debug(String.format(""Invalidating StorageEntry stored at key %s form L2 cache"", key));<line11>          log.debug(String.format(""Invalidating StorageEntry stored at key %s form L1 cache"", key));"
"<line1>    logger.debug(""LIVE IS STOPPING?!? message="" + finalMessage + "" enabled="" + enabled);<line2>      logger.debug(""LIVE IS STOPPING?!? message="" + finalMessage + "" "" + enabled);"
"<line21>      LOG.info(""Unexpected error during container creation"", e);"
"<line3>    LOG.debug(""Retrieving communication channels for user id "" + userId);"
"<line4>      LOG.error(""Registration of ServletDescriptor under mountpoint {} fails with unexpected""+ "" RuntimeException!"",servletDescriptor.getAlias(),e);<line5>      LOG.error(""Unable to mount servlet on mount point '{}', either it was already registered under the""+ "" same alias or the init method throws an exception"",servletDescriptor.getAlias(),e);<line6>      LOG.error(""Unable to mount servlet on mount point '{}', another resource is already bound to this""+ "" alias"",servletDescriptor.getAlias(),e);"
"<line7>    LOGGER.info(""Get data called for logical device {}{}"", DEVICE.getDescription(), logicalDeviceIndex);<line22>        LOGGER.warn(""Unsupported data attribute [{}], skip get data for it"", filter.getNode());"
"<line2>    logger.info(""terminating"");<line21>                logger.warn(""giving up on "" + br);<line27>              logger.info(""server+ "" + br + "" terminated"");<line35>      logger.info("""" + svs + "" broadcasters terminated "" + scopy);<line36>    logger.debug(""DHTBroadcasters terminated"");<line41>    logger.info(""DHT time: encode = ""+ enc+ "", decode = ""+ dec+ "", enc raw = ""+ encr+ "", dec raw wait = ""+ decr+ "", dec raw est = ""+ drest+ "", sum est = ""+ (enc + dec + encr + drest));<line50>        logger.warn(""server terminated "" + mythread);<line55>    logger.info(""terminated"");"
"<line5>      LOG.debug(""Start executing SQL commands: "");<line8>          LOG.debug(""Execute: {}"", cmd);"
<line49>      log.debug(newConfig);
"<line9>          log.debug(""Skip indexing because company delete is in process"");<line10>          log.debug(""Skip indexing because the index is read only"");<line22>        log.debug(""Skipping indexing read only index for "" + indexer.getClassName());"
"<line6>        log.debug(""checking if group '""+ potentialGroupName+ ""' is an element which has quality requirements"");<line8>        log.debug(""It is. Group {} has quality requirements"", potentialGroupName);<line20>    log.warn(""List of quality requirements was empty. Not requirements found. Returning null"");"
<line3>    LOGGER.info(MessageFormat.format(Messages.PURGE_DELETE_REQUEST_SPACE_FROM_CONFIGURATION_TABLES, result));
"<line6>        LOG.debug(""call: thread "" + diskId + ""'s next IO command is: "" + command);<line26>          LOG.debug(""call: thread ""+ diskId+ ""'s command ""+ command+ "" completed: bytes= ""+ bytes+ "", duration=""+ duration+ "", ""+ ""bandwidth=""+ String.format(""%.2f"", (double) bytes / duration * 1000 / 1024 / 1024)+ ((command instanceof WaitIOCommand)? """": ("", bandwidth (excluding GC time)=""+ String.format(""%.2f"",(double) bytes / (duration - timeInGC) * 1000 / 1024 / 1024))));<line48>      LOG.info(""call: out-of-core IO thread "" + diskId + "" terminating!"");"
"<line2>    logger.warn(""Skipping URL: {}, StatusCode: {}, {}, {}"", urlStr, statusCode, contentType, description);"
"<line10>    log.info(""Unassign Policy on File System : request received for {}  with {}"", id, filePolicyUri);<line37>      log.info(""No Errors found proceeding further {}, {}, {}"", new Object[] {_dbClient, fs, fp});<line45>      op = _dbClient.error(FilePolicy.class, fp.getId(), task, e);<line45>      log.error(""Error Unassigning File policy {}, {}"", e.getMessage(), e);<line47>      log.error(""Error Unassigning Filesystem policy {}, {}"", e.getMessage(), e);"
"<line2>    LOG.info(""{}"", params);"
"<line8>    log.info(""Something new."");<line9>    log.error(""There was an error."", new RuntimeException(""Exception message.""));"
"<line8>      logger.debug(""Entity Type '{}' defined"", entity.getTypeCode());<line9>      logger.error(""Error extracting entity type"", t);"
"<line16>      logger.debug(""Sent: {}"", command);<line18>      logger.error(""Got Exception while sending command"", e);"
"<line7>        LOG.warn(""update clock to master failed. task=""+ request.getTaskIndex()+ "", matrix=""+ request.getMatrixId()+ "", clock=""+ request.getClock());"
"<line5>          log.debug(""tryNext({}) starting authentication mechanisms: client={}, server={}"",session,clientMethods,serverMethods);<line8>          log.debug(""tryNext({}) no initial request sent by method={}"", session, userAuth.getName());<line17>          log.debug(""tryNext({}) successfully processed initial buffer by method={}"",session,userAuth.getName());<line29>          log.debug(""tryNext({}) exhausted all methods - client={}, server={}"",session,clientMethods,serverMethods);<line43>        log.debug(""tryNext({}) attempting method={}"", session, method);"
"<line29>          logger.debug(""ActivityObject not valid"");"
<line29>            logger.error(e1, e1);
"<line3>      LOG.info(""conn - "" + connection.get(""h""));<line3>      LOG.info(""conn1 - "" + connection1.get(""h""));"
<line6>        log.debug(exception, exception);
"<line3>      LOG.debug(""Skip migration because migration source folder is not specified or otherwise invalid."");"
"<line12>        log.error(""Invalid Report Template file format. "");<line15>      log.error(""Error in Report Template file {} "", ex.getMessage());"
"<line8>          logger.warn(String.format(""%s: Unexpected IOException during operation for region: %s key: %s messId: %s"",serverConnection.getName(),serverConnection.getModRegion(),serverConnection.getModKey(),transId),e);<line9>          logger.warn(String.format(""%s: Unexpected IOException: "", serverConnection.getName()), e);"
"<line26>      logger.error(""Exception occurred in SDNCActivateTasks activateVfModule process"", ex);"
<line16>        log.debug(layoutFriendlyURLException, layoutFriendlyURLException);
"<line3>      logger.warn(""no mapping for '"" + name + ""'"");"
"<line6>        logger.warn(""Agent already started."");<line9>    logger.info(""Starting {} Agent."", ProductInfo.NAME);"
"<line2>    logger.info(""No metadata connection"");"
"<line2>    log.debug(""Attempting to locate resource '{}' using classloader the servlet context"", path);"
"<line17>      log.error(""Error creating clearing request for project: "" + clearingRequest.getProjectId(), e);<line36>      log.error(""Cannot write JSON response for clearing request id ""+ requestSummary.getId()+ "" in project ""+ clearingRequest.getProjectId()+ ""."",e);"
<line7>      log.error(e.getMessage(), e);
"<line1>    log.debug(""Setting text: "" + text);<line9>          log.trace(""ConceptSource not found by ID or UUID"");"
"<line13>    LOGGER.info(""nextNodeOfFrom to process : "" + nextNodeOfFrom.toString());<line21>    LOGGER.info(""nextToDestinationNode to process : "" + nextToDestinationNode.toString());<line24>    LOGGER.info(""selectNode to process : "" + selectNode.toString());"
"<line3>        logger.debug(""sendCommand getSerialNumber :: {}"", TelitModemAtCommands.getSerialNumber.getCommand());"
"<line21>          LOG.warn(""Unsupported qop detected: "" + variant);"
"<line2>    logger.trace(""Counting coverage for file "" + sequencingObject);<line22>      logger.warn(""Not running coverage as not all files have fastqc results.  Object ID: ""+ sequencingObject.getId());"
"<line7>      LOG.warn(""[onStart] Erreur de connexion au serveur {} ({})"",remoteWebResource.getUri(),c.getMessage());"
"<line7>      log.info(""no existing description, do an add"");<line13>      log.info(""description is being deleted:{}"", irodsDescriptionValue);<line17>      log.info(""no change in description, no update"");<line21>    log.info(""update done"");"
"<line14>        LOG.debug(""Initialized component "" + _component);"
<line12>      LOG.error(ex.getMessage(), ex);<line23>      LOG.error(e.getMessage(), e);<line30>            LOG.error(ex.getMessage(), ex);
"<line6>      log.trace(""Cannot find any classes in bundles, not trying regular classloaders scanning: {}"",packageName);"
"<line4>      logger.error(""Got error message: {}"", message.toString());<line7>      logger.error(""No match found for action {}Â and message {}"", action, message);<line12>      logger.error(""Got exception while handling callback"", e);"
<line14>      log.error(exception, exception);
<line22>      log.error(e);
"<line17>      log.error(""Error while receiving messages "", e);<line19>      log.error(""Error while writing message to file"", e);"
"<line2>    log.debug(""Producing event for custom resource id: {}"", customResourceUid);"
"<line27>              LOG.error(""Failed to write file."", throwable);"
"<line7>      logger.info(""Creating project "" + projectName);"
"<line15>        logger.error(""Null content mapping by existed channel for content type {}"",currentContent.getTypeCode());"
"<line4>      logger.debug(""Return existing event, type: {}"", nextEvent.getEventType().toString());<line7>      logger.debug(""There are TextUnitDTOs available, create a text unit and return the text unit event"");<line9>      logger.debug(""No more TextUnitDTO, create end document event and return it"");"
"<line4>    log.info(""UserHostAndPort serialized as: "" + result);"
"<line23>        log.trace(""receive({}) check iteration #{} for id={} remain time={}"",this,count,id,idleTimeout);"
"<line1>    log.debug(""finding CmZosState instance by example"");<line8>      log.debug(""find by example successful, result size: "" + results.size());<line10>      log.error(""find by example failed"", re);"
"<line1>    logger.info(context,""ShadowUserProcessor:processClaimedUser:started claming shadow user with processId: ""+ shadowUser.getProcessId());<line9>    logger.info(context,""ShadowUserProcessor:processClaimedUser:started: flag value got from es ""+ esUser.get(JsonKey.FLAGS_VALUE));<line11>    logger.info(context, ""ShadowUserProcessor:processClaimedUser:Got Flag Value "" + flagsValue);"
"<line7>      LOGGER.warn(""Job({},{}) does not exist."", jobKey.getGroup(), jobKey.getName());"
"<line2>    LOGGER.debug(""Processing admin update key message"");<line8>      LOGGER.error(""UNRECOVERABLE ERROR, unable to read ObjectMessage instance, giving up."", e);"
"<line23>        log.error(""HTTP GET [uri="" + uri + ""] caused an exception: "" + e.getMessage());<line24>        log.error(""HTTP GET [uri="" + uri + ""] caused an exception: "" + e.getMessage(), e);"
"<line3>      log.debug(""Generating CA with subject={}"", subject);"
"<line1>    log.info(""Finding matches"");<line4>      log.trace(""input: "" + sInputURI);<line5>      log.trace(""vivo: "" + sVivoURI);<line6>      log.trace(""score: "" + score);<line6>      log.debug(""Match found: <"" + sInputURI + ""> in Input matched with <"" + sVivoURI + ""> in Vivo"");<line7>    log.info(""Found "" + resultSet.size() + "" links between Vivo and the Input model"");"
<line6>        logger.error(e.getMessage(), e);
"<line9>      log.error(""main threw"", e);"
"<line7>      logger.debug("">> deleting securityGroup(%s)"", groupName);<line9>      logger.debug(""<< deleted securityGroup(%s)"", groupName);"
"<line2>    LOG.debug(""setWorkingDirectory({})"", path);"
"<line6>    log.info(""multimap serialized as: "" + result);"
"<line8>    LOG.info(""Logged into hdfs with principal {}"", configuration.get(STORM_USER_NAME_KEY));"
"<line5>      LOGGER.warn(""Could not close plugin classloader"", e);"
"<line9>          logger.info(""Rotating the registration access token for "" + client.getClientId());<line17>        logger.error(""Couldn't parse a known-valid token?"", e);"
"<line4>    LOG.trace(""Netconf WRITE transaction started. RetryCounter: {}"", retryCounter);<line12>      LOG.trace(""Netconf WRITE transaction done for {}"", data);<line15>        LOG.warn(""Netconf WRITE transaction failed to {}. Restarting transaction ... "", e.getMessage());<line17>        LOG.warn(""Netconf WRITE transaction unsuccessful. Maximal number of attempts reached. Trace: {}"",e);"
"<line2>    LOGGER.debug(""Validating SCEP message exchange"");<line5>      LOGGER.debug(""Matched transaction IDs"");<line9>      LOGGER.debug(""Matched request senderNonce and response recipientNonce"");<line11>      LOGGER.warn(""Response senderNonce is null"");<line17>      LOGGER.debug(""{} has not been encountered before"", res.getSenderNonce());<line18>    LOGGER.debug(""SCEP message exchange validated successfully"");"
"<line1>    logger.debug(""getManagedConnectionFactory()..."");"
"<line4>      log.trace(""Changing device status. DeviceId {}, dashId {}"", device.id, dashBoard.id);"
"<line7>      LOG.error(""Failed to get response, Error is : "" + e.getMessage());"
"<line7>      log.error(""read(key:{}) error."", key, e);<line11>    log.debug(String.valueOf(response.getHeader()));"
"<line24>      LOG.error(""Update of user {} failed, trying to pull its status anyway (if configured)"",userUR.getKey(),ex);"
"<line15>    logger.info(""[doInsertInstances]{}"", toAdd);"
"<line14>      logger.warn(""{} encountered Exception when trying to set bypass period: {}"",HeliosEasyControlsHandler.class.getSimpleName(),e.getMessage());"
"<line7>    LOG.info(mMsg + mWorkerId + "" just finished."");"
"<line6>      LOG.info(""Dropping datagram with command: "" + commandId);"
"<line2>    logger.info(""START initialisation of vehicle service proxy."");<line7>    logger.info(""FINISHED initialisation vehicle service proxy."");"
"<line2>    LOG.debug(""undoSoftDeleteOnCascade: {id: {}}"", id);"
"<line7>      log.debug(sb.a("", after="" + toString() + ']').toString());"
"<line3>      logger.warn(""Found hook with invalid {}, not registering"", ConfigurationService.KURA_SERVICE_PID);<line7>      logger.warn(""Found duplicated hook with id {}, not registering"",ConfigurationService.KURA_SERVICE_PID);<line10>    logger.info(""Hook registered: {}"", hookId);"
"<line4>          LOGGER.error(""HealthCheck failed for {} : {}"",result.getComponentName().getName(),result.getCause().orElse(""""),result.getError().get());<line5>          LOGGER.error(""HealthCheck failed for {} : {}"",result.getComponentName().getName(),result.getCause().orElse(""""));<line9>          LOGGER.warn(""HealthCheck is unstable for {} : {}"",result.getComponentName().getName(),result.getCause().orElse(""""),result.getError().get());<line10>          LOGGER.warn(""HealthCheck is unstable for {} : {}"",result.getComponentName().getName(),result.getCause().orElse(""""));"
"<line3>      logger.debug(LogMarker.PERSIST_ADVISOR_VERBOSE,""{}-{}: Member offine. id={}, persistentID={}"",shortDiskStoreId(),regionPath,distributedMember,persistentID);<line20>          logger.warn(""Unable to persist membership change"", e);"
"<line2>    LOG.info(""Running testUpdateParagraphConfig"");"
"<line1>    logger.debug(""Cleaning up temporary certificate file"");"
"<line23>          logger.warn(""Failed to remove the ip alias on the router, marking it as removed in db and freed""+ "" the allocated ip ""+ ipAlias.getIp4Address());<line26>      logger.info(""Unable to delete the ip alias due to unable to contact the virtualrouter."");"
"<line5>      log.debug(""requesting:"" + url);<line6>      log.debug(""response:"" + json);"
"<line2>    log.debug(""Waiting for Service "" + serviceId + "" to be "" + state);<line7>        log.debug(""Service "" + serviceId + "" state updated to "" + lastState);"
"<line8>      log.debug(""Init existing SHARD using db version'{}' in {} ms"",dbVersion,System.currentTimeMillis() - start);"
"<line31>      logger.info(""Index: {}, {}..."", index, Format.formatSize(index * 1024));"
"<line3>    LOG.info(""assign is a no-op"");"
"<line5>        log.debug(""Fetching the details of all ${rootArtifactId} devices"");<line9>      log.error(msg, e);"
"<line2>    logger.info(""Stopping LB!"");<line3>    logger.info(""Stopping SMPP server!"");<line4>    logger.info(""SMPP server stopped!"");<line7>    logger.info(""Done. Exiting"");"
"<line1>    log.debug(""parse Status called with bus {} address {} and value {}"", bus, address, value);<line13>        log.debug(""Setting Sensor INACTIVE"");<line17>        log.debug(""Setting Sensor ACTIVE"");"
<line11>      logger.debug(e.getMessage(), e);
"<line4>    LOGGER.trace(""openModule - set active module to "" + module);"
"<line6>    log.error(""Conditional '{}' has already been added to Logix '{}'"", systemName, getSystemName());"
"<line7>            LOGGER.info(""About to reactivate record, orcid={}"", orcid);<line48>              LOGGER.info(""Name for orcid={} successfully set"", orcid);<line49>            LOGGER.info(""Record orcid={} successfully reactivated"", orcid);"
"<line3>      LOG.info(""Starting Alluxio job master gRPC server on address {}"", mRpcBindAddress);<line15>      LOG.info(""Started Alluxio job master gRPC server on address {}"", mRpcConnectAddress);"
"<line17>            LOGGER.debug(""A response was generated from the cache with "" + ""no requests sent upstream"");<line19>            LOGGER.debug(""The response was generated directly by the "" + ""caching module"");<line21>            LOGGER.debug(""The response came from an upstream server"");<line23>            LOGGER.debug(""The response was generated from the cache ""+ ""after validating the entry with the origin server"");"
"<line2>    log.trace(""WbListener::Add"");"
"<line1>    logger.debug(""Disconnecting from {} port for thing {} at IP {}"",conn.getName(),thingID(),conn.getIP());<line12>      logger.debug(""Error closing {} port for thing {} at IP {}: exception={}"",conn.getName(),thingID(),conn.getIP(),e.getMessage());"
"<line6>      Logger.error(this, ""Failed to find farm repo: %[exception]s"", err);"
"<line2>      LOGGER.trace(""columnData from current stack: {}"", Arrays.toString(columnData));"
"<line2>    LOGGER.debug(""testFindUsersByBadHeaderValueThenReturnBadRequest"");"
"<line30>      logger.debug(""Sending transformed FCM payload: {}"", fcmMessage);<line33>      logger.debug(""Message batch to FCM has been submitted"");"
"<line7>                  LOGGER.info(""succeed to close ""+ RegistrationManager.INSTANCE.getMicroservice().getServiceName());"
"<line18>      LOGGER.warn(""Exception happened after test case."", e);"
"<line1>    log.info(""Runtime exec {}"", Arrays.toString(cmd));"
"<line6>      logger.debug(""Dependency for {} is satisfied because it has no dependencies"", propertyDescriptor);<line10>      logger.debug(""Dependency for {} is not satisifed because its dependency chain contains a loop: {}"",propertyDescriptor,propertiesSeen);<line18>          logger.debug(""Dependency for {} is not satisfied because it has a dependency on {}, which has no""+ "" property descriptor"",propertyDescriptor,dependencyName);<line22>          logger.debug(""Dependency for {} is not satisfied because it has a dependency on {}, which does not""+ "" have a value"",propertyDescriptor,dependencyName);<line26>          logger.debug(""Dependency for {} is not satisfied because it has a dependency on {}, which has a""+ "" null value"",propertyDescriptor,dependencyName);<line31>          logger.debug(""Dependency for {} is not satisfied because it has a dependency on {} and {} does not""+ "" have its dependencies satisfied"",propertyDescriptor,dependencyName,dependencyName);<line35>          logger.debug(""Dependency for {} is not satisfied because it depends on {}, which has a value of""+ "" {}. Dependent values = {}"",propertyDescriptor,dependencyName,dependencyValue,dependentValues);<line38>      logger.debug(""All dependencies for {} are satisfied"", propertyDescriptor);"
"<line7>        LOGGER.info(""Failed to remove filterless subscription registered for id {} for csw source with id""+ "" of {}"",filterlessSubscriptionId,this.getId());"
"<line63>      logger.error(""error in searchIdeaInstances"", t);"
"<line3>    logger.info(String.format(""Creating security group '%s' ..."", securityGroupUniqueName));<line34>        logger.info(String.format(""Ports became open for '%s'"", securityGroupUniqueName));<line47>      logger.info(String.format(""Ports became open for '%s'"", securityGroupUniqueName));<line48>    logger.info(String.format(""Security group '%s' was created :)"", securityGroupUniqueName));"
<line9>    logger.debug(result);
"<line6>    log.info(serviceCheckName + tagString + "" - "" + System.currentTimeMillis() / 1000 + "" = "" + status);"
<line19>      log.error(systemException, systemException);
"<line2>    logger.info(""Resetting local state of the client, because of a cluster change."");"
"<line3>      LOG.info(""Test properties was not provided, therefore not loading any test properties"");<line9>      LOG.error(""Test properties provided at {} does not exist, therefore aborting the test execution"",fileName);<line11>      LOG.error(""I/O error reading the test properties at {}: {}"", fileName, e.getMessage(), e);"
"<line7>            log.warn(""[HashedWheelTimer] impossible, please fix the bug"");<line11>              log.warn(""[HashedWheelTimer] timerFuture.totalTicks < currentTick, please fix the bug"");"
"<line1>    log.info(""Auto-starting scheduling tasks in schedule service..."");<line8>    log.info(""Auto-start completed."");"
<line21>      log.error(systemException, systemException);
"<line1>    LOGGER.trace(""Setting attribute mappings to: {}"", attributeMappingsList);"
"<line3>      LOG.error(""Error processing auth message, closing connection"");"
"<line29>      log.error(exception, ""Exception in unzip retry loop"");"
"<line14>      log.error(""Error calling Roller.release()"", e);"
"<line7>        LOG.info(""inactive durable subs on "" + broker + "" : "" + Arrays.asList(subs));"
"<line4>    logger.debug(""Invalid Search Param Exception: "", ex);"
"<line1>    log.debug(""attaching clean TmpItvSel instance"");<line3>      log.debug(""attach successful"");<line4>      log.error(""attach failed"", re);"
"<line4>    logger.info(""[doMigrationPublish]Cluster:{}, Shard:{}, NewPrimaryDc:{}, NewMaster:{}"",clusterName,shardName,primaryDcName,newMaster);"
"<line4>      log.warn(""Failed to serialize exception "", e);"
"<line2>    LOGGER.debug(""SerializedPSKIdentityLength: "" + msg.getIdentityHintLength().getValue());"
"<line4>      LOGGER.warn(""Error occured while getting session"", e);"
"<line3>    LOGGER.debug(""get logbook for users with id :{}"", id);"
"<line3>    log.debug(""Processing REST method return value \"""" + returnType.getName() + ""\""."");"
"<line4>      LOGGER.warn(""Could not perform reverse DNS for \"""" + ip + ""\"""", ex);"
"<line2>      log.debug(""Creating frame for console"");<line8>          log.error(""Exception creating system console frame: {}"", ex);<line10>      log.debug(""Frame created"");"
"<line6>    LOG.info(""entity -submit -type cluster -file "" + filePath);<line11>    LOG.info(""Submit datatsource entity {} via entity -submit -type datasource -file {}"",dsName,filePath);<line14>    LOG.info(""Submit feed with datasource {} via entity -submitAndSchedule -type feed -file {}"",dsName,filePath);<line18>    LOG.info(""update datasource {} via -update -type datasource -file {}"", dsName, filePath);"
"<line19>        LOG.debug(""Authorize success"");<line22>        LOG.error(""Authorize failed for session {}"", session, throwable);<line27>        LOG.warn(""Authorize cancelled"");"
<line8>      log.error(msg, e);
"<line4>      LOGGER.warn(name + "" (reserved word) cannot be used as parameter name. Renamed to "" + name + ""_"");"
"<line15>      LOG.info(""First token "" + loginToken);<line18>      LOG.info(""Refreshed token "" + loginToken);"
"<line6>      LOGGER.debug(""Desc2 "" + urlBase + e.select(""a"").first().attr(""href""));"
"<line6>      log.info(String.format(""RP Placement: Storage pool %s does not have connectivity to a protection system."",storagePool.getLabel()));"
"<line6>        log.debug(""saving {} positions of {} servos"", filename, positions.size());<line8>        log.error(""could not save servo positions"", e);"
"<line23>          LOG.debug(""Detected source to target format match for video"");"
"<line3>      LOG.info(""verinice runs in designer mode - retrieving server configuration from ODA driver."");"
"<line6>      LOGGER.trace(format(""Select async with execution info : %s"",statementWrapper.getBoundStatement().preparedStatement().getQueryString()));"
"<line11>    LOG.debug(""query: "" + result);"
"<line3>    logger.debug(""Processing affinity group "" + group.getName() + "" for VM Id: "" + vm.getId());"
"<line8>      logger.warn(String.format(""unable to add baremetal RCT[%s]"", getRctUrl()), e);"
"<line5>      LOGGER.error(""error trying to sleep waiting for external view to change : "", e);"
"<line3>      LOG.debug(""{} - Unlocking {} after {}ms"", name, requestId, duration);<line4>      LOG.trace(""{} - Unlocking {} after {}ms"", name, requestId, duration);"
"<line10>      LOG.error(""Not all the new directories are empty. New directories that are not empty are: ""+ nonEmptyDirs);"
<line30>      log.warn(errMsg, e);
"<line28>        logger.warn(""Failed to delete: {}"", file);"
"<line8>      LOG.warn(""Could not load form XML [{}]"", getClass().getName(), e);"
"<line6>      log.debug(""Unable to parse URL for string {}"", urlString, e);<line13>        log.warn(""Parameter `default` for to_url() is not a valid URL: {}"", defaultUrl.get());"
"<line7>      log.error(message);<line10>      log.info(""Lucene query: "" + nativeSearchString);<line10>      log.info(""Lucene query parsed as: "" + luceneQuery.toString());<line10>      log.info(""Querying for objects of type: "" + getEntityNames());"
"<line2>    log.debug(""** Failing connection"");<line3>    log.debug(""** Fail complete"");"
"<line5>      logger.info(jarName + "" not found."");<line9>      logger.info(""found "" + jarName + "" path:"" + file);<line11>    logger.warn(""too many "" + jarName + "" found. "" + jarFIleList);"
<line8>      log.error(ex);
"<line3>      LOG.info(getInfo().toString());<line4>      LOG.info(""Count for component "" + componentName + "" is "" + emitCount);<line5>      LOG.info(""Component "" + componentName + "" has "" + executorCount + "" started executors"");"
"<line25>    LOG.debug(""Copying {} to {}."", toPath(sourceBlob.getBlobId()), toPath(destination));<line29>      LOG.debug(""Successfully copied {} to {}."", toPath(sourceBlob.getBlobId()), toPath(destination));"
"<line5>      LOGGER.warn(""Exception validating metacard ID {}"", metacard.getId(), e);"
"<line4>      logger.info(""Removing groups {}"", navGroup.getName());"
"<line3>          logger.warn(""Checkout conflicts warning"");"
"<line2>    log.trace(""Processing ARCRecord with offset: {}"", sar.getMetaData().getOffset());"
"<line10>    LOG.info(""tableName:{}, fieldName:{}, fieldVal:{}, equalFieldName:{}, equalFieldVal:{}, dataId:{}"",tableName,fieldName,fieldVal,equalFieldName,equalFieldVal,dataId);<line23>      LOG.error(e.getMessage(), e);<line27>      LOG.info(""This value is available"");<line32>      LOG.info(""This value already exists is not available!"");"
"<line27>      LOGGER.warn(""The resolvedMoveThreadCount ({}) is higher ""+ ""than the availableProcessorCount ({}), which is counter-efficient."",resolvedMoveThreadCount,availableProcessorCount);"
"<line2>    LOG.debug(""Fetching new Txn Batch for {}"", endPoint);<line12>      LOG.info(""Acquired Transaction batch {}"", batch);"
<line16>      log.error(exception, exception);
"<line31>      log.info(""Created RDD {} for table {}"", rdd.name(), tempTableName);"
"<line13>                    log.info(""server listens on standard secure port [{}:{}]"",bindAddress,server.actualPort());<line14>                    log.warn(""server listens on non-standard secure port [{}:{}], default is {}"",bindAddress,server.actualPort(),getPortDefaultValue());<line17>                  log.error(""cannot bind to secure port"", bindAttempt.cause());"
"<line14>                    logger.error(""scheudle applySnapshotToDB faield"", e);<line21>                    logger.error(""scheudle snapshotExpire faield"", e);"
"<line25>      LOGGER.error(""Remote config center ["" + settings + ""] is not available."", e);"
"<line2>    log.info(""------  testRangeOpsInDiffSubTree  ------"");"
"<line2>      LOGGER.trace(String.format(""Is '%s' Update statement ? "", statement.toString()));"
"<line6>      logger.error(""Error resetting method status"", t);"
"<line9>        logger.warn(""job-""+ job.getId()+ "" is scheduled for wakeup run, but there is no joining info anymore"");"
"<line3>      LOGGER.debug(""Animation plan set to "" + plan);"
"<line1>    log.info(""Shutting down thread monitoring tables."");"
"<line4>    LOG.info(""Destroyed WebSocketServer."");"
"<line11>        log.debug(""Invoke a Change Listener"");"
"<line10>        LOG.error(""Failed validating JWT {} from {}"", jwt, WebUtils.toHttp(req).getRemoteAddr());<line10>        LOG.debug(""exception"", e);"
"<line1>    logger.info(""updating gc_grace_seconds on TWCS/DTCS tables..."");<line13>    logger.info(""updating gc_grace_seconds on TWCS/DTCS tables - complete"");"
<line12>        log.debug(sb.toString());
"<line17>      LOGGER.debug(""Retrieving group names for user [{}]"", sr.getName());<line24>            LOGGER.debug(""Groups found for user [{}]: {}"", username, groupNames);"
"<line7>        logger.debug(""sending status 410 GONE"");<line9>        logger.debug(""sending status 404 NOT FOUND"");<line19>        logger.debug(""sending status {}"", status);<line21>        logger.debug(""sending status 200 OK"");<line23>        logger.debug(""sending status 304 NOT MODIFIED"");"
"<line2>    log.warn(""Spinning up test cluster..."");<line15>    log.warn(""Spun up test cluster."");"
"<line23>      log.debug(""send {} to {}"", bundle, sb);"
"<line2>    log.debug(""Controller for transforming POJO object to resources of semantic web"");<line10>    log.debug(""Creating output stream"");<line13>    log.debug(""Generating OWL"");"
"<line1>    log.trace(""Partition forming"");<line4>    log.trace(""New views installed"");"
"<line1>    LOGGER.debug(""Checking if network name setup is done on "" + config.getAgentHostname());<line39>        LOGGER.error(msg);"
"<line58>      logger.error(""Error updating page {} status"", pageCode, e);"
"<line18>        LOGGER.warn(MessageFormat.format(""Found multiple implementations for ActionTrace {0}. Returning first""+ "" implementation"",scriptParameterDesignTraceKey.toString()));"
"<line27>    LOGGER.debug(""Username encryption key: "" + ArrayConverter.bytesToHexString(key));<line37>    LOGGER.debug(""Username: "" + ArrayConverter.bytesToHexString(msg.getUsername()));"
"<line5>    LOGGER.debug(""Build Google sitemap start."");<line10>    LOGGER.debug(""Build Google number of URL files {}."", Integer.toString(number));<line20>      LOGGER.info(""Write Google sitemap file {}."", fn);<line25>        LOGGER.info(""Write Google sitemap file {}."", fn);<line28>    LOGGER.debug(""Google sitemap request took {}ms."", System.currentTimeMillis() - start);"
<line9>      log.error(message);
"<line8>    LOG.info(""Running ingest"");<line8>    LOG.info(""Cluster size:""+ util.getHBaseClusterInterface().getClusterMetrics().getLiveServerMetrics().size());<line14>      LOG.info(""Intended run time: ""+ (runtime / 60000)+ "" min, left:""+ ((runtime - (System.currentTimeMillis() - start)) / 60000)+ "" min"");<line33>        LOG.error(errorMsg);<line41>        LOG.error(errorMsg + "" Rerunning verification after 1 minute for debugging"");<line47>          LOG.error(""Rerun of Verification failed with error code "" + ret);"
"<line5>      log.error(""Failed to set uninstall commands"");"
"<line11>      LOGGER.trace(""Received versions from client."");<line34>      LOGGER.trace(""Adding type filter"");"
<line9>      log.error(e);
"<line3>      log.debug(""Received message ["" + data + ""]"");<line11>        log.debug(""Found a matching contract with an output message. Will send it to the [""+ destination+ ""] destination"");"
"<line10>      LOG.warn(""exception while writing counters data to files"", e);"
"<line12>                    logger.warn(""Exception at getUserListMemberships"", e);"
"<line3>    LOG.debug(""Enable SQS RPC: {}"", enabled);"
"<line3>    LOG.info(String.format(""Because you have configured truncate is true,KuduWriter begins to truncate table %s ."",userTable));<line7>        LOG.info(String.format(""table  %s has been deleted."", userTable));"
"<line17>        LOGGER.warn(""Partition strategy is necessary when using more than 1 partition, defaulting to 'hash'""+ "" partitioning."");"
<line1>    log.trace(XTCE_POLYNOMIAL_CALIBRATOR);
"<line2>      logger.debug(""Searching index using custom query '"" + query + ""'"");"
"<line10>      logger.info(""query-{}: segment-{} memory store scan finished, take {} ms"",queryProfile.getQueryId(),segmentName,profileStep.getDuration());"
"<line1>    log.debug(""attaching dirty StgMPersbezTxt instance"");<line3>      log.debug(""attach successful"");<line4>      log.error(""attach failed"", re);"
"<line27>      LOG.error(""Failed to get running instances"", e);"
<line19>      log.error(systemException, systemException);
"<line7>        log.debug(m.toString());<line8>        log.debug(ent + "" | "" + ""Offset: "" + ent.getOffset());"
"<line3>      ActiveMQRALogger.LOGGER.trace(""getBooleanProperty("" + name + "")"");"
<line17>      log.error(portalException, portalException);
"<line6>      LOGGER.warn(MessageFormat.format(this.getActionExecution().getAction().getType()+ "" does not accept {0} as type for iterationCondition"",iterationCondition.getClass()));"
"<line2>      log.trace(""Not bouncing a message stanza, as bouncing is disabled by configuration."");<line5>      log.trace(""Not bouncing a stanza that included an error (to prevent never-ending loops of""+ "" bounces-of-bounces)."");<line8>      log.trace(""Not bouncing a stanza that was sent by the server itself."");<line11>      log.trace(""Bouncing a message stanza."");<line17>      log.error(""An exception occurred while trying to bounce a message."", e);"
"<line4>        log.debug(""closed {}"", rs);<line8>      log.debug(e.getMessage(), e);"
"<line5>      logger.trace(""searching process list for pid{}"", pid);"
"<line6>    LOGGER.debug(""CompleteResultinMessage: ""+ ArrayConverter.bytesToHexString(msg.getCompleteResultingMessage().getValue()));"
<line24>      log.error(systemException, systemException);
"<line21>                logger.info(""GCDProxy done e1 "" + e1.getClass().getName());<line26>              logger.info(""GCDProxy e1 "" + e);<line38>                logger.info(""GCDProxy done e2 "" + e2.getClass().getName());<line43>              logger.info(""GCDProxy e2 "" + e);<line43>              logger.info(""GCDProxy P = "" + P);<line43>              logger.info(""GCDProxy S = "" + S);<line50>      logger.info(""InterruptedException "" + ignored);<line52>      logger.info(""ExecutionException "" + e);"
<line11>          log.debug(portalException, portalException);
"<line8>      logger.warn("""", fex);"
"<line2>    logger.info(""GeoIT.testMovingTarget"");"
"<line18>            Log.warn(""Deleted null property "" + key + "" for group: "" + name);<line22>          Log.warn(""Ignoring null property key for group: "" + name);<line25>      Log.error(sqle.getMessage(), sqle);"
"<line1>    log.error(""The portlet could not be loaded. Check if properly deployed."",ExceptionUtil.getRootCause(e));"
"<line6>        logger.debug(""{} DECODE COMPLETE: {}"", context.channel(), response);"
"<line2>    LOG.info(""Started retrievePutConcurrent"");<line16>    LOG.info(""Async tasks finished"");<line19>    LOG.info(""Finished retrievePutConcurrent"");"
"<line2>    LOGGER.debug(""Init for method "" + pluginAnnotation.getMethod());<line7>      LOGGER.error(""Unable to register resources for annotation {} on method {} class {}"",e,pluginAnnotation.getAnnotation(),pluginAnnotation.getMethod().getName(),pluginAnnotation.getMethod().getDeclaringClass().getName());"
"<line9>      log.debug(executionException.getCause(),""Error while caching all catalogs metadata. Falling back to default flow"");"
"<line8>      logger.debug(""Given poster image URI '{}' is not valid"", posterimageUrlOpt);<line19>      logger.warn(""Poster image could not be found at '{}'"", url);<line21>      logger.warn(""Error getting poster image: {}"", e.getMessage());<line23>      logger.warn(""Given URL '{}' is not a valid URI"", url);"
<line22>      log.error(systemException, systemException);
<line17>        log.debug(throwable, throwable);
"<line7>        logger.debug(""Failure trying to obtain (Terminal-Information) MEID AVP value"", ex);"
"<line4>      logger.debug(""Could not find class, but will keep looking"", e1);<line20>            logger.debug(""added method {} to interface {}"", method.getName(), controller);<line27>          logger.debug(""added getter {} to interface {}"", getter.getName(), controller);"
"<line6>        LOG.warn(""Configuration value overridden by system property: {}, with value: {}"",property,value);"
"<line3>      log.debug(""Synchronizing all folders for accountId "" + account.getAccountId());"
"<line11>    log.info(""No data for report for user {} and reportId {}."", key.user.email, report.id);"
"<line8>        LOGGER.error(""Unable to copy files: "" + e.getMessage(), e);<line11>      LOGGER.error(""Directory copy failed, could not create destination directory: ""+ destination.getAbsolutePath());<line29>          LOGGER.error(""Unable to copy files: "" + e.getMessage(), e);"
"<line21>        log.error(sb.toString(), t);<line22>        log.trace(sb.toString());<line29>          log.debug(""Unable to write message to file"", e);<line34>      log.trace(""An error occurred logging an evaluation event"", e);"
"<line8>    LOG.debug(""Canonical path: {}"", fileName);"
"<line3>      LOG.error(I18n.err(I18n.ERR_01308_ZERO_LENGTH_TLV));<line8>      LOG.debug(""KrbSafe created"");"
"<line28>      logger.info(""Tuple row key: "", output.getReceivedTuples());<line28>      logger.info(""Received tuple number {}, instance is {}."",output.getReceivedTuples() == null ? 0 : output.getReceivedTuples().size(),System.identityHashCode(output));"
"<line1>    logger.info(""executing test case testOrFilter2"");"
"<line13>      log.debug(""Finished {} with session expiration in {} ms and session re-authentication on or after""+ "" {} ms"",authenticationOrReauthenticationText(),positiveSessionLifetimeMs,sessionLifetimeMsToUse);<line14>      log.debug(""Finished {} with no session expiration and no session re-authentication"",authenticationOrReauthenticationText());"
"<line2>    LOGGER.info(""Retrieving "" + this.url);"
"<line6>          CacheManager.logger.error(Messages.getInstance().getString(""CacheManager.ERROR_0005_UNABLE_TO_BUILD_CACHE""));<line11>        CacheManager.logger.warn(Messages.getInstance().getString(""CacheManager.WARN_0002_REGION_ALREADY_EXIST"", region));<line13>      CacheManager.logger.warn(Messages.getInstance().getString(""CacheManager.WARN_0001_CACHE_NOT_ENABLED""));"
"<line2>    log.info(""Start of addIP call"");<line9>    log.info(""End of addIP call"");"
"<line12>        LOGGER.info(""Skipping test: "" + tcCtx.toString());"
"<line3>      LOGGER.debug(""Data Groups Changed: Request Tree Rebuild"");"
"<line9>        LOG.info(""Shutting down Journal with checkpoint..."");<line12>          LOG.error(""An error occurred whilst writing a checkpoint to the Journal: {}"",e.getMessage(),e);<line19>      LOG.error(""Unable to close Journal file: {}"", e.getMessage(), e);"
"<line2>    log.debug(""Looking for any previously completed tasks on disk[%s]."", completedTaskDir);<line16>        log.error(ex,""Failed to read completed task from disk at [%s]. Ignored."",taskFile.getAbsoluteFile());<line19>      log.info(""Found %,d complete tasks from previous run: %s"",completedTasks.size(),completedTasks.values().stream().map(taskAnnouncement ->StringUtils.format(""%s (%s)"", taskAnnouncement.getTaskId(), taskAnnouncement.getStatus())).collect(Collectors.joining("", "")));"
"<line10>                  log.info(""Check failed, will retry: "" + e);"
<line16>      log.error(systemException, systemException);
"<line4>      log.info(""Iteration: "" + i);"
"<line5>      logger.warn(""Failed to stop web server"", ex);"
"<line5>    LOGGER.info(""bundle size for process is "" + bundleIds.size());<line8>      LOGGER.info(""number of coordJobs in bundle "" + bundleId + ""="" + coordJobs.size());<line11>        LOGGER.info(""number of actions in coordinator "" + coordJob.getId() + "" is "" + actions.size());<line15>    LOGGER.info(""default coordID: "" + coordId);"
"<line5>      logger.warn(""Cannot load resource "" + resource + "" from classpath"");"
"<line13>            log.error(""Could not find field type"");<line19>    log.error(""Could not determine field type"");"
"<line28>        logger.error(""Given target type for mounted files or folders was not extending AbstractFile."", ex);<line47>        logger.warn(""Cannot handle watch event, folder {} has no path"", folder.getUuid());"
"<line14>        log.error(""Failed to create Publisher for the topic [{}]."", topic, e);"
<line11>      log.error(e.getMessage(), e);
"<line3>      LOG.info(""Reconnected to "" + serverURI);<line9>          LOG.debug(""Resubscribing topic("" + topic + "") QoS:"" + qos);"
"<line13>              log.debug(""Client received:\n"" + frame);<line17>                  log.debug(""PUBLISH "" + publish);<line29>              log.debug(""Client sent:\n"" + frame);"
"<line5>      log.info(""Skipping graceful shutdown call, because web-console not up for {}"", this);"
"<line2>      log.info(""Importing portlet settings..."");<line12>      log.info(""Portlet settings successfully imported"");"
"<line15>        log.warn(""Optimizer did not work in its expected input. Exception name was ""+ exc.getClass().getName()+ "" Trying with the assumption of former versions of Input "");<line24>        log.error(""Error optimizing the initial deployment"");"
"<line1>    LOGGER.debug(""Do something thing by Sub Generic Controller"");"
"<line2>    log.info(""HANDLE: OpenAction for element: "" + action.getElementId());"
"<line4>      logger.trace(this + ""::Acking message "" + message);"
"<line8>        log.trace(""Resource ""+ name+ "" not found with classloader: ""+ delegate+ "". Trying other delegates"");"
"<line3>      log.info(""Will proxy requests to backend that are not getmap or getcapabilities."");<line4>      log.info(""Will NOT proxy non-getMap requests to backend."");"
"<line15>      log.error(""sizeof: Failed to compute function for "" + type + "": "", t);"
"<line1>    LOGGER.info(""Creating disambiguated org {}"", organization.name);"
"<line3>      logger.trace(""addLongField fieldName: {}; value: {}"", fieldName, value);"
<line5>        LOG.info()
"<line19>        LOG.debug(""Malformed URL from "" + pathName, e);"
"<line2>      logger.debug(""Stop periodic connection check"");"
"<line1>    log.debug(""getting stats for cache \'{}\'"", cacheName);<line6>        log.debug(""Statistics are not enabled for cache \'{}\'"", cacheName);<line9>      log.warn(""Error getting Stats object - are statistics enabled for cache \'{}\'?"", cacheName, e);"
"<line7>    logger.info(""taskAckCommand : {}"", taskAckCommand);"
"<line1>    LOG.debug(""Recovering from JMS Connection exception (attempt: {})"", task.getCurrentAttempts());<line4>      LOG.debug(""Successfully recovered JMS Connection (attempt: {})"", task.getCurrentAttempts());<line7>        LOG.debug(""Failed to recover JMS Connection. Will try again in ""+ task.getCurrentDelay()+ "" millis"",e);"
"<line2>    log.info(""Exporting personal user data for account {}"", account.getUid());<line11>      log.info(""Running search '{}'"", filter);<line12>      log.info(""User query successful: {}"", result);<line13>      log.error(""Error running {}"", filter, e);<line19>      log.error(""Error generating LDIF file"", e);<line22>    log.info(""Returning LDIF: {}"", ldifContents);"
"<line7>    logger.info(""load schema [%s]"", spec);<line14>                  logger.info(""Getting storage path for segment [%s]"", segment.getSegment().getId());<line15>                  logger.info(""Fetch segment files from [%s]"", path);<line17>                  logger.info(""Locally storing fetched segment at [%s]"", dir);<line18>                  logger.info(""finished fetching segment files"");"
<line10>      logger.error(msg, ex);
"<line6>      log.debug(""User agent "" + userAgent);<line15>          log.debug(""Invalid WebDAV path "" + httpServletRequest.getPathInfo());<line47>          log.error(webDAVException, webDAVException);<line48>          log.warn(webDAVException, webDAVException);<line52>      log.error(exception, exception);<line59>        log.info(StringBundler.concat(xLitmus,httpServletRequest.getMethod(),"" "",httpServletRequest.getRequestURI(),"" "",status));"
"<line10>    LOGGER.info(""Generating code for .NET Framework "" + this.targetFramework);"
"<line2>    LOGGER.info(""Connection ({}) closed."", this.connection, e);"
<line6>      log.error(e.getMessage(), e);
"<line27>      logger.error(""A Problem happened in the BalancerValve on response "" + response, e);"
"<line1>    logger.debug(""enforce(agreement={},since={})"", agreement.getAgreementId(), since);"
"<line16>        LOG.warn(""Error reaching aws-ec2 instance ""+ node.getId()+ ""@""+ node.getLocation()+ "" on port ""+ node.getLoginPort()+ ""; falling back to jclouds metadata for address"",e);<line20>        LOG.warn(""Cannot query aws-ec2 Windows instance ""+ node.getId()+ ""@""+ node.getLocation()+ "" over ssh for its hostname; falling back to jclouds metadata for address"");<line26>          LOG.warn(""Error querying aws-ec2 instance ""+ node.getId()+ ""@""+ node.getLocation()+ "" over ssh for its hostname; falling back to jclouds metadata for address"",e);"
"<line9>      LOGGER.debug(""Beginning tests"");<line10>      LOGGER.error(e);"
<line24>      log.error(systemException, systemException);
<line6>        logger.debug(msg);<line7>        logger.debug(msg, t);<line10>        logger.info(msg);<line11>        logger.info(msg, t);<line14>        logger.warn(msg);<line15>        logger.warn(msg, t);<line18>        logger.error(msg);<line19>        logger.error(msg, t);
"<line2>    LOG.debug(""Disposing Model on CamelContext"");"
"<line7>      log.warn(""Both legacy {} and R7 {} properties are specified. Using R7 property: {}."",PaxWebConstants.SERVICE_PROPERTY_HTTP_CONTEXT_ID,HttpWhiteboardConstants.HTTP_WHITEBOARD_CONTEXT_SELECT,selector);"
"<line5>      LOGGER.error(""process sync timeout request error"", e);"
"<line12>      logger.error(""error in buildTree"", t);"
"<line3>      LOG.error(""Scope can't be null."");"
"<line1>    LOG.info(""calling inoutdemo: {}, {}"", inout1[0], out1[0]);"
"<line2>    logger.debug(""executing test testInsertEmptyWhere"");"
<line21>    logger.trace(txIntrospector.toString());
"<line5>          log.info(""Port "" + x + "" was marked as healthy"");"
"<line10>      LOG.info(""Setting Row Filter for counter."");<line14>      LOG.info(""Setting TimeRange for counter."");<line16>    LOG.warn(""Got the Scan: "" + s);"
"<line21>      LOGGER.info(brother.getName()+ "" output: \n""+ Util.prettyPrintXml(brother.getOutput().getMessage()));"
"<line9>                logger.debug(""Exception while cleaning up Lucene Index Repository"", e);"
"<line3>    log.debug(""Loading experiment info"");"
<line12>        log.debug(sb.toString());
<line18>      log.error(systemException, systemException);
"<line1>    LOG.debug(""Allocating {} task executors for redundancy."", number);<line3>      LOG.warn(""Expect to allocate {} taskManagers. Actually allocate {} taskManagers."",number,allocatedNumber);"
"<line4>    LOGGER.info(""Bundle [{}] removed successfully"");"
"<line22>        log.error(""ClassNotFoundException exception occurred: "" + entry.getValue());"
"<line5>    logger.debug(""Created new freemarker template processor for DocUtils"");"
"<line30>      LOGGER.error(""MessagingException in recoverAttachment"", e);<line31>      LOGGER.error(""IOException in recoverAttachment"", e);"
"<line17>          logger.warn(""Cannot find VM in VcCache for node ""+ node.getVmName()+ "" whose mobid is ""+ node.getVmMobId()+ ""."");<line19>            logger.warn(""Cannot find VM by VM path ""+ node.getVmName()+ "" whose mobid is ""+ node.getVmMobId()+ ""."");<line30>            logger.warn(""FT secondary VM status is incorrect for node ""+ vm.getName()+ "". ""+ ""FT status ""+ vm.getFTState().toString()+ "" is unexpected."");"
"<line2>    log.info(""------  testSingleValueAndMultiFieldNoParens  ------"");"
"<line10>      log.warn(""Unable to update service context properties."");"
"<line6>        logger.debug(""Assuming that domain_id field doesn't exist in account_vlan_map table, no need to""+ "" upgrade"");"
"<line12>          logger.error(""TTransportException writing to internal frame buffer"", e);<line14>          logger.error(""Exception writing to internal frame buffer"", e);<line23>          logger.error(""TTransportException inside handler"", e);<line26>          logger.error(""TApplicationException inside handler"", e);<line29>          logger.error(""Exception inside handler"", e);<line37>          logger.error(""Exception writing to internal frame buffer"", ex);"
"<line2>    LOGGER.debug(""DistinguishedNames: ""+ ArrayConverter.bytesToHexString(msg.getDistinguishedNames().getValue()));"
"<line9>        log.error(""server path is not configured in database and not yet generated from Http request!!!.""+ "" Admin need to configure it in database"");"
"<line3>      logger.debug(""Initialize {}"", this.getClass().getSimpleName());"
"<line10>        LOG.error(""Error cancelling command selection key"", e);<line14>      LOG.debug(""Command {} is not executed because it is not in the whitelist."", cmd);<line20>    LOG.info(""Processing {} command from {}"", cmd, sock.socket().getRemoteSocketAddress());"
"<line2>    LOG.info(""Tearing down OmidTestBase..."");"
"<line6>    LOGGER.info(""Configuring JDBC connection using data source: {}"", dataSource.toString());"
"<line30>        log.warn(""Something had happened while validating a connection"", e);<line37>          log.warn(""Can't close a http client"", e);"
"<line2>      logger.debug(""Starting thread for "" + p2pReaderName());"
"<line47>            logger.debug(""Returning CREATED response for start case with content '{}'"", response);"
"<line5>      logger.debug(""Found custom marshaller builder {} that is going to be used instead of the default"",marshallerBuilder);"
"<line29>      log.error(""[{}] Error adding entry"", name, result.status);"
"<line14>      log.info(""Ignoring exception, likely coming from Hadoop 1"", e);"
"<line5>      log.error(""Failed to log timeseries delete"", e);"
<line14>      log.error(e.getMessage(), e);
"<line3>      log.debug(""Removed user {} from {} tracks"", user, result.getN());<line4>      log.error(""Error removing user {} from tracks: {}"", user, result);"
"<line6>      log.trace(""Creating new TAD..."");<line14>      log.trace(""Using TAD from cache..."");"
"<line14>          logger.debug(""Prefilled connection {} connection count is now {}"",connection,connectionAccounting.getCount());<line17>        logger.info(String.format(""Unable to prefill pool to minimum because: %s"", ex.getMessage()));<line22>            logger.debug(""Unable to prefill pool to minimum, connection count is now {}"",this::getConnectionCount);"
"<line1>    LOGGER.debug(String.format(""Trying to delete all models in index '%s'"", index));<line5>      LOGGER.debug(String.format(""Deleted %d models in the index '%s'"", bulkResponse.getTotal(), index));"
"<line12>      LOG.info(""KVDiskStorage create a new file, filePath = "" + newPath);"
"<line2>    LOGGER.info(""  testStringResult"");"
"<line5>        log.error(""Empty partitions for topic {}"", topic);"
"<line7>    logger.info(name.getMethodName());<line13>                  logger.info(name.getMethodName() + "" - callback - got broadcast"");<line14>                    logger.info(name.getMethodName() + "" - callback - invalid content"");<line16>                    logger.info(name.getMethodName() + "" - callback - content OK"");<line23>                  logger.info(name.getMethodName() + "" - callback - error"");<line30>      logger.info(name.getMethodName() + "" - subscription successful, subscriptionId = "" + subscriptionId);<line30>      logger.info(name.getMethodName() + "" - Invoking fire method"");<line32>      logger.info(name.getMethodName() + "" - fire method invoked"");<line35>      logger.info(name.getMethodName() + "" - results received"");<line43>        logger.info(name.getMethodName() + "" - unsubscribe successful"");"
"<line16>                LOGGER.info(""add listener"");<line21>                LOGGER.info(""add listener executor"");<line25>                LOGGER.info(""remove listener"");"
"<line16>          LOGGER.error(""Error validating preservation binary "" + binary.getStoragePath(), e);"
"<line17>          log.error(""Failed to locate bundle at "" + location);<line28>            log.info(""Deploying external component: "" + url);<line32>            log.error(""Failed to deploy: "" + file, e);"
<line10>      LOGGER.error(e);
"<line16>          LOG.info(""Could not load factory due to missing dependencies."");"
"<line2>    logger.info(""{} - {}"", DESCRIPTION, MongoDBHelper.getRiverVersion());<line7>      logger.error(""Cannot start. Current status is {}"", status);<line11>      logger.info(""River is currently disabled and will not be started"");<line14>      logger.info(""Startup pending"");"
"<line11>        LOG.trace(""Flushing entry {} ({})"", entry, message);"
<line12>        logger.warn(e.getLocalizedMessage());
"<line3>    logger.info(""Curator framework start operation invoked"");<line6>      logger.info(""Waiting to connect to zookeeper, startupTimeout : {}"", startupTimeOutMs);<line8>      logger.error(""Interrupted while waiting to connect zookeeper (connectString : {}) : {}"",ex,connectString);<line12>    logger.info(""CuratorFramework client started successfully"");"
"<line4>      log.debug(""Executing {} command..."", getName());<line8>        log.debug(""Getting deployment policy {}"", deploymentPolicyId);"
<line9>    logger.info(MESSAGE_PROCESSING_BEGIN);<line29>                    logger.info(MessageFormat.format(MESSAGE_PROCESSING_RECORD_BEGIN_TEMPLATE, recordName));<line30>                    logger.info(MessageFormat.format(MESSAGE_PROCESSING_RECORD_END_TEMPLATE, recordName));<line44>    logger.info(MESSAGE_PROCESSING_END);
"<line12>      logger.error(""failed to open eventhub receiver: "" + ex.getMessage());"
"<line3>      LOGGER.info(""Resizing node table from {} to {}"", oldsize, newsize);"
"<line4>      logger.debug(""extractId:uri="" + uri);<line8>      logger.debug(""id="" + id);"
"<line22>          log.debug(""Found conflicting container with uuid {""+ conflictingContainer.getUuid()+ ""} of node {""+ conflictingNode.getUuid()+ ""}"");"
"<line13>    LOG.debug(""Graph [{}] get rays paths from '{}' with ""+ ""direction '{}', edge label '{}', max depth '{}', ""+ ""max degree '{}', capacity '{}' and limit '{}'"",graph,sourceV,direction,edgeLabel,depth,maxDegree,capacity,limit);"
"<line15>    LOGGER.info(""Executing GeoServerGetStyleCommand..."");<line22>      LOGGER.error(""Exception encountered executing command"", e);"
<line32>      log.error(systemException, systemException);
<line27>            ActiveMQServerLogger.LOGGER.warn(e.getMessage());
"<line8>        log.debug(""Can not send message"", e);"
"<line5>      log.info(""...Locking the singleton world of the DB definition!"");"
"<line6>      LOG.debug(""{} search result(s) found for URL {}"", addresses.size(), url);"
"<line12>      logger.error(""Error creating ideainstance"", t);"
"<line6>      LOG.info(""TLS tests enabled so starting the TLS auth route"");<line9>      LOG.info(""TLS tests NOT enabled, so NOT starting the TLS auth route"");"
"<line4>      log.info("">>> Test with "" + threads + "" transactions."");<line20>                    log.info("">>> Performs put [node=""+ ((IgniteKernal) ignite).localNode()+ "", tx=""+ tx+ "", key=""+ key1+ ']');<line23>                      log.info("">>> Performs sleep. [node=""+ ((IgniteKernal) ignite).localNode()+ "", tx=""+ tx+ ']');<line26>                      log.info("">>> Performs put [node=""+ ((IgniteKernal) ignite).localNode()+ "", tx=""+ tx+ "", key2=""+ key2+ ']');"
"<line7>        log.trace(""Skipped post_logout_redirect_uri validation (because""+ "" allowPostLogoutRedirectWithoutValidation=true)"");<line20>        log.trace(""Failed to validate post_logout_redirect_uri."");<line29>      log.trace(""Unable to validate post_logout_redirect_uri."");<line36>        log.error(e.getMessage(), e);"
"<line3>      ActiveMQRALogger.LOGGER.trace(""getAcknowledgeMode()"");"
<line18>      log.error(systemException, systemException);
<line5>      LOGGER.debug(url.toString());<line8>      LOGGER.debug(data);<line24>            LOGGER.error(line);<line28>      LOGGER.error(e);
"<line2>    logger.debug(""A communication failure event arrived"", ex);<line6>      logger.debug(""Exception while closing client"", e);"
"<line6>    log.debug(""Running virt_limit post unbind."");"
"<line12>          logger.warn(""exception when close gtscanner"", e);"
<line6>    LOGGER.info(html);
"<line24>      LOGGER.error(""removeByFilter occur a exception"", e);"
"<line9>              LOGGER.debug(""Could not find a MetacardMapper for featureType {}."", featureType);"
"<line12>        LOG.debug(""Added {} into the enabled schema {}"", dn.getName(), schemaName);<line18>        LOG.info(msg);<line21>      LOG.debug(""The SyntaxChecker {} cannot be added in the disabled schema {}"",dn.getName(),schemaName);"
"<line10>    log.info(""All volumes are of the same size. No need for capacity calculations."");"
"<line10>                log.info(""Publishing Cluster terminating event for [application] ""+ appId+ "" [cluster] ""+ getClusterId()+ "" [instance] ""+ instanceId);"
"<line14>      log.error(Util.getMessage(""FailedSubscribingTo"") + destination + "": "", ex);"
"<line11>        LOGGER.warn(""Invalid access on table, iterator has been closed"");"
<line12>      log.error(exception, exception);
"<line3>      LOG.debug(""Got cached sources for internet radio {}!"", radio.getStreamUrl());<line5>      LOG.debug(""Retrieving sources for internet radio {}..."", radio.getStreamUrl());<line8>          LOG.warn(""No entries found for internet radio {}."", radio.getStreamUrl());<line9>          LOG.info(""Retrieved playlist for internet radio {}, got {} sources."",radio.getStreamUrl(),sources.size());<line11>        LOG.error(""Failed to retrieve sources for internet radio {}."", radio.getStreamUrl(), e);"
"<line22>        LOG.info(""url:"" + page.getUrl().toString());<line22>        LOG.info(""limit:"" + page.getContent().limit());"
"<line5>      LOG.warn(""Unexpected exception while handling framework message"", t);"
"<line2>    LOGGER.info(""  checkSubscribeToEntityUpdatePATCH"");<line8>              LOGGER.debug(""    {}"", entityType);"
"<line7>        LOGGER.error(""Can not parse port from substring {}"", portSubstring);"
"<line6>    logger.debug(""checkSystemIfUniqueValidationNeeded started..."");"
"<line14>        LOG.error(""Error removing node"", t);"
"<line8>        logger.debug(""Try to get Metadata for topic {} broker {}"", topic, broker);<line28>          logger.warn(""Broker {} is unavailable or in bad state!"", broker);"
"<line2>    logger.debug(""Get localized virtual asset for target locale"");<line13>        logger.debug(""Remove untranslated text unit"");<line14>        logger.debug(""Set translation for text unit with name: {}, translation: {}"",assetTextUnit.getName(),translation);"
"<line14>    LOG.debug(""Graph [{}] query vertices by label: {}, properties: {}, ""+ ""offset: {}, page: {}, limit: {}"",graph,label,properties,offset,page,limit);"
"<line12>      log.debug(""Created session for device authorization grant page, sessionId: {}"",deviceAuthzSession.getId());"
"<line13>        logger.error(""listPodIdsHavingVmsforAccount:Exception: "" + e.getMessage());<line19>      logger.error(""listPodIdsHavingVmsforAccount:Exception : "" + e.getMessage());<line27>        logger.error(""listPodIdsHavingVmsforAccount:Exception:"" + e.getMessage());"
"<line23>    logger.info(""Blogger: "" + blogger);<line58>    logger.info(""12->72: "" + sim);"
"<line16>      log.debug(""Failed to fetch the container component {}. "", componentUid);"
"<line6>        LOGGER.error(""Failed to initialise."", ex);"
"<line7>      LOGGER.error(""Error in ZigBeeConsole API execute command."", e);"
"<line2>      log.info(""Private key does not exist,filepath={}"", this.rsaKeyFile.getAbsolutePath());<line7>      log.error(""errors while reading private key"", e);"
"<line31>        log.info(""Store file of catalog [%s] to %s"", catalogName, filePath);<line49>      log.error(ex, ""Pull catalog files failed"");"
"<line1>    LOG.info(""Close connection {}"", s);"
"<line7>        logger.warn(""IOException while closing file being read"", ioe);<line17>      logger.warn(""Problem reading "" + thisFilename + "" in "" + ""SimpleFileCollection.getDocuent() : "", ioe);"
"<line6>    LOG.debug(""Mapping port: [{}] for address: [{}] with description: [{}]."",port,address,mappingDescription);<line10>    LOG.debug(""Port: [{}] for address: [{}] with description: [{}] has been mapped."",port,address,mappingDescription);"
"<line20>      LOG.info(""starting {} consumer threads"", config.getConsumerThreads());<line30>      LOG.error(""Consumer failed"", t);"
"<line8>      LOG.error(""entity {}"", this, ex);<line13>        LOG.error(""attribute {}/{}"", this, a, ex);<line20>        LOG.error(""entity {}/{}"", this, e, ex);"
"<line4>      LOGGER.debug(""unbind branch type {}"", unbindBranchType);"
"<line3>    log.info(""createShare()"");<line9>    log.info(""irodsAbsolutePath:{}"", irodsAbsolutePath);<line9>    log.info(""deciding whether a file or collection..."");<line10>    log.info(""seeing if share already present.."");<line29>    log.info(""adding share tag"");<line30>    log.info(""setting inheritance and ACL"");<line35>    log.info(""share created"");"
"<line1>    LOG.info(""NoopTask.stopTask() invoked."");"
"<line24>                LOGGER.debug(debugMessage.toString());<line26>              LOGGER.debug(""===DONE: {}"", link.id);"
"<line2>    logger.trace(""Executing query: {0}"", query);"
"<line9>      logger.debug(""Spring Security setup complete."");"
"<line8>    logger.debug(""-----installProject--- , project name: {}, branch name: {}"", projectName, branchName);"
"<line5>    log.trace(""Reported device connection duration [tenant : {}, noOfDeviceConnections: {},""+ "" connectionDurationInMs: {}]."",tenantId,noOfDeviceConnections.get(),deviceConnectionDuration);"
"<line4>      log.error(""!!!!!!!ProvenanceEventJmsWriter is NULL !!!!!!"");"
"<line27>      logger.warn(""Retrying "" + toRetry.size() + "" entries "" + toRetry);<line31>      logger.warn(""Canceling execution with ids "" + execIds);<line33>        logger.warn(""Requested cancel of execution id: "" + executionId + "", result: "" + result);"
"<line13>      LOGGER.debug(""VdbManifestArtifactBuilder:processing '{}' validation error for schema '{}'"",errors.getLength(),schemaArtifact.getName());<line36>          LOGGER.debug(""VdbManifestArtifactBuilder:model artifact '{}' has validation error with severity""+ "" '{}', path '{}', and message '{}'"",new Object[] {schemaArtifact.getName(),ArtificerModelUtils.getCustomProperty(errorArtifact, VdbValidationError.PropertyId.SEVERITY),errorArtifact.getName(),ArtificerModelUtils.getCustomProperty(errorArtifact, VdbValidationError.PropertyId.MESSAGE)});"
"<line22>        log.warn(""No workflow definitions found for "" + workflowDefinitionName);"
"<line2>    log.info(""start YieldingIterator.next: "" + getTopValue());<line8>      log.info(""end YieldingIterator.next: yielded at "" + getTopKey());<line11>      log.info(""end YieldingIterator.next: ""+ (hasTop() ? getTopKey() + "" "" + getTopValue() : ""no top""));"
"<line10>        logger.warn(""Populated SecurityContextHolder with pre-auth token: '""+ context.getAuthentication()+ ""'"");<line11>          logger.debug(""Populated SecurityContextHolder with pre-auth token: '""+ context.getAuthentication()+ ""'"");<line13>        logger.warn(""SecurityContextHolder not populated with pre-auth token"");<line14>          logger.debug(""SecurityContextHolder not populated with pre-auth token"");"
"<line7>        LOGGER.error(""Failed at stopping KIE Server extension {}"", EXTENSION_NAME);"
"<line4>    log.info(""Starting up the default async job executor [{}]."", getClass().getName());"
"<line13>        LOGGER.warn(""namespace add for xml:lang attribute in {}"", datasubtag.getName());"
<line13>        log.debug(sb.toString());
"<line6>      LOG.error(""Error retrieving console layout info for role {}"", roleKey, e);"
"<line18>              logger.warn(""Unexpected object type of variantSource ({}) in file attributes. Expected {} or""+ "" {}"",variantSourceObj.getClass(),VariantFileMetadata.class,Map.class);<line29>              logger.warn(""Unexpected object type of AlignmentHeader ({}) in file attributes. Expected {}"",alignmentHeaderObj.getClass(),Map.class);"
"<line9>    log.trace(""State transfer happens here"");<line12>    log.trace(""Checking the values from caches..."");"
"<line18>      log.error(""Error deleting element."", e);"
"<line11>      LOGGER.warn(""Could not generate Signature! Using empty one instead!"", E);"
"<line3>      LOG.debug(""Retriving classifications for entity={}"", guid);"
"<line9>        log.error("""", ex);"
"<line15>        log.info(""Waiting until connectors of address space: '{}' messages {} will be in ready state"",name,getConnectorStatuses(clientAddressSpace));<line25>    log.info(""Connectors of address space {} are ready for use"", name);"
"<line17>        LOGGER.info(MessageFormat.format(""Found multiple implementations for ScriptVersionTrace {0}. Returning first""+ "" implementation"",scriptVersionTraceKey.toString()));"
"<line24>      logger.error(errorMsg);<line26>    logger.info(""Verified reply message "" + message.getMsgId() + "" correlation:"" + correlationId);"
"<line16>      log.warn(""Got exception: "", e);"
"<line8>      Log.error("""", ex);"
<line11>        log.debug(portalException, portalException);
"<line8>    LOG.warn(""[{}] Metric {} does not support {}, it will always return 0"",context.getSessionName(),DefaultSessionMetric.THROTTLING_QUEUE_SIZE.getPath(),requestThrottler.getClass().getName());"
"<line2>    log.debug(""=========== test1() ================="");"
"<line7>    LOG.debug(""Earliest reference available for timestamp [{}]"", earliestRefAvailTime);<line13>    LOG.debug(""Starting sweep phase of the garbage collector"");<line13>    LOG.debug(""Sweeping blobs with modified time > than the configured max deleted time ({}). "",timestampToString(maxModifiedTime));<line48>      LOG.warn(""Deleted only [{}] blobs entries from the [{}] candidates identified. This may happen if""+ "" blob modified time is > than the max deleted time ({})"",deleted,count,timestampToString(maxModifiedTime));<line50>      LOG.info(""Estimated size recovered for {} deleted blobs is {} ({} bytes)"",numDeletedSizeAvailable,org.apache.jackrabbit.oak.commons.IOUtils.humanReadableByteCount(deletedSize),deletedSize);<line55>    LOG.debug(""Ending sweep phase of the garbage collector"");"
"<line7>      logger.error(""Errore loading categories"", t);"
"<line2>    logger.info("""" + Schema.getDefaultSchema().getEntityClass(""sample_entity""));<line2>    logger.info("""" + Schema.getDefaultSchema().getEntityType(SampleEntity.class));<line3>    logger.info(entity.getType());"
<line36>        log.debug(message, exception);<line37>        log.error(message);
"<line8>        LOG.debug(""Failed verifying signature of token {}"", token, e);"
"<line1>    logger.warn(""Checkout conflicts warning"");"
<line5>        log.debug(_NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey);
"<line3>      LOGGER.warn(""The from value of a reference link is false, the link was not added to the link table"");<line7>      LOGGER.warn(""The to value of a reference link is false, the link was not added to the link table"");<line11>      LOGGER.warn(""The type value of a reference link is false, the link was not added to the link table"");<line14>    LOGGER.debug(""Link in table {} add for {}<-->{} with {} and {}"", type, from, to, type, attr);<line17>      LOGGER.warn(""An error occured while adding a dataset from the reference link table, adding not""+ "" succesful."",e);"
"<line4>    logger.debug(""{} will wait for lock path {}"", client, lockPath);<line14>        logger.debug(""{} waited {} ms for lock path {}"",client,(System.currentTimeMillis() - waitStart),lockPath);"
"<line5>      logger.error(""Fail to get HDFS! "", e);"
"<line2>    log.info(""-----  testDateFolderDistributor  -----"");"
"<line13>      LOGGER.error(""Exception in vulnerabilitybyapplications "", e);"
<line29>      log.info(buffer.toString());
"<line2>      logger.error(""Null user"");"
"<line3>      LOG.debug(""Google Cloud Storage available: "" + gcsAvailable);"
"<line3>    log.debug(""Started to create crypto provider"");"
"<line24>    Log.debug(""NeoEMF Editor opened in {0}"", stopwatch.stop().elapsed());"
"<line2>      logger.debug(""Open: connection is already open"");<line20>        logger.debug(""Starting Websocket connection"");<line22>        logger.warn(""Websocket Start Exception: {}"", e.getMessage());<line24>        logger.debug(""Connecting Websocket connection"");<line26>        logger.warn(""Websocket Connect Exception: {}"", e.getMessage());<line27>        logger.warn(""Websocket URI Exception: {}"", e.getMessage());"
"<line18>      logger.error(""routing {} to 'failure' because of missing attributes: {}"",new Object[] {flowFile, missingKeys.toString()});<line30>      logger.info(""adding Hash Value {} to attributes for {} and routing to success"",new Object[] {hashValue, flowFile});"
"<line2>    LOGGER.warn(""Executing: "" + unit);"
"<line2>    logger.debug(""Disposing MAX! device {} {}."", getThing().getUID(), maxDeviceSerial);<line10>      logger.trace(""Clear MAX! device {} {} from bridge."", getThing().getUID(), maxDeviceSerial);<line14>    logger.debug(""Disposed MAX! device {} {}."", getThing().getUID(), maxDeviceSerial);"
"<line2>      logger.debug(""drug was null returning false"");<line5>      logger.debug(""provider was null or blank returning false"");<line8>      logger.debug(""demographic was null returning false"");<line11>      logger.debug(""rx date was null returning false"");<line14>      logger.debug(""drug endDate was null"");<line17>      logger.debug(""drug special instructions was null returning false"");"
"<line3>    log.error(""IGNORE_ME"", ""Ignored error"", null);<line4>    log.info(""IGNORE_ME"", ""Ignored info"");<line4>    log.debug(""IGNORE_ME"", ""Ignored debug"");<line4>    log.trace(""IGNORE_ME"", ""Ignored trace"");<line4>    log.error(""ACCEPT_ME"", ""Accepted error"", null);<line5>    log.info(""ACCEPT_ME"", ""Accepted info"");<line5>    log.debug(""ACCEPT_ME"", ""Accepted debug"");<line5>    log.trace(""ACCEPT_ME"", ""Accepted trace"");"
<line8>      log.error(exception, exception);
"<line4>      logger.warn(file.getPath() + "" getCanonicalPath() error. Error:"" + e.getMessage(), e);"
"<line2>    logger.debug(""Executing samples command line"");<line36>        logger.error(""Subcommand not valid"");"
"<line18>        LOG.warn(""Interrupted while sleeping"", e);"
"<line3>      logger.debug(""Getting old job request"");<line5>      logger.debug(""Creating new job request"");"
"<line7>    LOG.info(""Single Entity:  {}"", prettyPrint(entity));<line11>    LOG.info(""Single Entry:  {}"", prettyPrint(entity));<line16>    LOG.info(""Single People Entiry with expanded Trips relation:  {}"", prettyPrint(entityExpanded));"
"<line2>    logger.info(""BEGIN: Removing build aggregation group: {}"", buildContentId);<line2>    userLog.info(""Removing build aggregation group"");<line12>    logger.info(""END: Removing build aggregation group: {}, took: {} seconds"",buildContentId,stopWatch.getTime(TimeUnit.SECONDS));"
"<line2>      log.debug(""Ids message handling failed. [exception=({})]"", exception.getMessage(), exception);"
"<line11>          LOG.error(""ResultLog thread interrupted."", e);<line12>          LOG.error(""Failed to generate the JSON for a result."", e);<line13>          LOG.error(""Failed to write JSON to output stream for a result"", e);"
"<line12>    logger.debug(""Request to join tournament - response (status "" + response.getStatus() + ""): "" + status);"
"<line7>            log.warn(""Wrong result type: {}"", json.getNodeType());"
"<line7>      LOG.debug(""close result set error"", e);"
<line22>      log.error(portalException, portalException);
"<line7>        LOG.info(""Derby shutdown complete normally."");<line8>        LOG.info(""Derby shutdown complete abnormally. - message:"" + se.getMessage());<line12>    LOG.info(""Shutdown database ("" + catalogUri + "")"");"
"<line4>    LOGGER.debug(""{} is executing synchronously"", logId);<line5>      LOGGER.debug(""{} not done yet"", logId);"
<line5>      log.error(exception, exception);
"<line3>      logger.debug(""Adding PhysicalNetworkServiceProvider VirtualRouter"");"
"<line40>                LOGGER.warn(""Unbekannte Einstellung {}"", child.getName());<line45>      LOGGER.debug("""", e);<line47>      LOGGER.debug(""\""Verwende Zahlenformat '{}' aus Attribut NUMBERS.\"""", format);<line48>      LOGGER.debug(""Verwende ABDRUCK_NAME '{}'"", copyName);"
"<line2>    LOG.debug(""Getting Types: {}."", ManagementBusInvocationPluginRest.TYPES);"
"<line15>      logger.error(""exception"", e);"
"<line3>    Log.debug(""INI FILE:"" + commaSeparatedFilePaths);"
"<line6>      logger.warn(""Module[{}] Received command {} with illegal value {} on channel {}"",thing.getUID(),command,e.getMessage(),channelUID.getId());"
"<line1>    log.debug(""Copying local file: {} to remote file: {} (in host {})"", origFile, targetFile, host);"
<line13>        log.debug(exception, exception);
"<line12>      logger.error(""[closeAllConnections] {}"", e);"
"<line54>      logger.warn("""", ex);"
"<line5>          LOG.debug(""processing include ..."");<line11>            LOG.debug(resourceError.message, resourceError);"
"<line20>          LOG.debug(""Matrix param {} :: {}"", entry.getKey(), entry.getValue());"
"<line8>      logger.debug(""to be created, uoc common"");<line8>      logger.debug(objectAsXmlString(uocCommon, UocCommon.class));"
"<line9>      LOGGER.warn(""Could not cancel sync repl request"", e);"
"<line1>    logger.info(""Downloading genome info ..."");"
"<line11>    log.info(""User's info:\n"" + formatJson(contents));"
"<line2>    log.debug(""renewConnection()"");<line6>    log.debug(""evaluate conn for renewal:{}"", irodsAccount);<line10>      log.debug(""return a refreshed connection"");"
"<line5>        logger.error(""{}"", e.getLocalizedMessage(), e);"
"<line3>      LOG.info(""Log4j 1.2 jmx support is disabled by property."");<line7>        LOG.info(""Log4j 1.2 jmx support found and enabled."");<line8>        LOG.info(""Log4j 1.2 jmx support not found; jmx disabled."");"
"<line18>          logger.warn(""applyMCTS interrupted"");<line28>        logger.info(""Player: ""+ name+ "" Simulated ""+ simCount+ "" games in ""+ thinkTime+ "" seconds - nodes in tree: ""+ root.size());<line28>        logger.info(""Total: Simulated ""+ totalSimulations+ "" games in ""+ totalThinkTime+ "" seconds - Average: ""+ totalSimulations / totalThinkTime);<line52>        logger.info(""Simulated "" + simCount + "" games - nodes in tree: "" + root.size());"
"<line1>    logger.debug(""Enrich text units with usages"");<line8>    logger.debug(""Fetch the asset text unit information"");<line17>    logger.debug(""End Enrich text units with usages"");"
"<line9>                    log.trace(""Not able to resolve blob"");<line13>                    log.debug(""Not allowed to bulk download blob for document {}"", doc::getPathAsString);<line22>      log.debug(""No blob to be zipped"");"
<line6>      log.error(exception, exception);
<line3>      log.debug(portalException, portalException);<line33>      log.error(portalException.getMessage());
"<line2>    LOG.debug(""Begin --""+ "" PatientDiscoveryPolicyTransformHelper.transformPatientDiscoveryEntityToCheckPolicy()"");<line3>      LOG.error(""Request is null."");<line7>    LOG.debug(""End --""+ "" PatientDiscoveryPolicyTransformHelper.transformPatientDiscoveryEntityToCheckPolicy()"");"
"<line2>    logger.debug(""testRedefineTerms"");"
"<line12>        LOG.debug(""removing "" + partition + "" satisfied by "" + key.get(i));<line22>          LOG.debug(""removing "" + source + "" satisfied by "" + key.get(i));<line32>        LOG.debug(""original: "" + original + "", strict: "" + isSatisfiedBy);<line33>          LOG.debug(""removing "" + timeField + "" satisfied by "" + key);"
"<line2>    LOG.info(""Executing operation isItemModifiedByOther"");"
"<line2>    LOGGER.info(""Received request for number of active resources"");"
"<line23>          logger.error(""Unexpected interruption when waiting for next start up check"", e);"
"<line2>    logger.info(""Starting slave.. slaveId - {}, environmentIds - {}"",slaveId,apolloConfiguration.getSlave().getSlaveCsvEnvironments());"
"<line9>      log.debug(""Error generating export report {} for {}."", report, key.user.email, e);"
"<line6>      logger.warn(""Failed to convert Number {} because Smartthings returned a null value."", deviceType);<line26>        logger.warn(""Failed to convert Number {} with a value of {} from class {} to an appropriate type."",deviceType,deviceValue,deviceValue.getClass().getName());"
<line51>      LOG.error(new AuthenticationException(AuthenticationException.UNNOWN_EXCEPTION, ne.getMessage()));
"<line8>      logger.error(""Create topic:{} addr:{} failed"", addr, topic);"
"<line2>    log.info(""onJointAngles {}"", angleMap);"
"<line3>      log.error(""Buyer user email is required"");"
"<line22>      logger.debug(""Target ["" + title + ""] did not match the query SMARTS. Skipping constraints"");"
"<line5>    logger.debug(""Filtering groups with SQL: "" + where);<line8>      logger.debug(""complete sql: "" + completeSql + "", params: "" + where.getParams());<line17>      logger.debug(""Filter '"" + filter + ""' generated invalid SQL"", e);"
"<line6>      logger.info(""Using default value of "" + Arrays.toString(defaultValue) + "" for property "" + key);"
"<line7>    logger.debug(""Sending request %s: %s"", request.hashCode(), request.getRequestLine());"
"<line4>      LOG.debug(""Ensemble change is disabled. Retry sending to failed bookies {} for ledger {}."",failedBookies,ledgerId);<line15>        LOG.info(""[EnsembleChange-L{}-{}] : writing new ensemble info = {}, block add completions = {}"",new Object[] {getId(), curNumEnsembleChanges, ensembleInfo, curBlockAddCompletions});<line18>        LOG.error(""Could not get additional bookie to remake ensemble, closing ledger: {}"", ledgerId);"
<line16>        logger.debug(responseAsString);
"<line8>    LOGGER.error(""Failed to install style class : "" + cl.getName());"
"<line2>    logger.debug(""C --> P {}"", record);"
"<line7>      log.error(""Error while checking whether "" + _app + "" exists in backend: "" + e.getMessage());"
"<line29>            log.trace(""{},{} is not at the centre of {}!"",lr.getDecimalLatitude(),lr.getDecimalLongitude(),lr.getStateProvince());"
"<line2>    LOGGER.debug(""fuzzy_norm   : "" + Norm.NORMS[m_norm]);<line2>    LOGGER.debug(""shrink       : "" + Shrink.SHRINKS[m_shrink]);"
"<line4>      log.debug(""Subscribed to: "" + pattern);"
"<line3>      LOG.info(""===========================testTaskMatrixClock==============================="");<line32>      LOG.error(""run testTaskMatrixClock failed "", x);"
"<line1>    log.debug(""Starting testLoginFromConfig."");<line3>    log.debug(""testLoginFromConfig success."");"
"<line11>          LOG.warn(""No test results were found in the report file:"" + "" "" + path);"
"<line12>      log.error(""You've specified an unsupported baud rate"");"
"<line10>      logger.debug(""Could not get appliance command"", e);"
"<line3>    log.info(""updateUserInfo()"");<line9>    log.info(""userName:{}"", userName);<line9>    log.info(""userInfo:{}"", userInfo);<line10>    log.info(""looked up user:{}"", user);<line12>    log.info(""updated info"");"
"<line6>      logger.error(""Invalid module path"");<line11>      logger.error(""Invalid module name. Should contain only alphabets and underscore"");"
<line21>      log.error(systemException, systemException);
"<line5>      LOG.info(""Ledger ID: "" + lh.getId());<line15>          LOG.debug(""Entries counter = "" + sync.counter);<line20>      LOG.debug(""*** WRITE COMPLETE ***"");<line28>      LOG.error(""Test failed"", e);"
"<line40>          LOG.error(""Failed to terminate periodic progress reporting in 1 minute. ""+ ""Waiting for it to terminate indefinitely..."");<line41>          LOG.info(""Periodic progress reporting terminated."");"
"<line2>    logger.debug(""Calling AdminDistributedSystemImpl#onDisconnect"");<line3>    logger.debug(""Completed AdminDistributedSystemImpl#onDisconnect"");"
"<line3>      log.info(""Stopping connector {}"", connName);<line4>        log.warn(""Ignoring stop request for unowned connector {}"", connName);"
"<line9>      LOG.error(""Request failed: {}."", e.getMessage());"
"<line2>    LOGGER.info(""exec container commmand: "" + execCommand);<line12>      LOGGER.info(log);"
"<line5>    LOG.info(""Transaction created "" + t1);<line20>    LOG.info(""Transaction created "" + t2);<line27>      LOG.trace(""row: "" + Bytes.toString(r.getRow()) + "" count: "" + count);<line31>    LOG.info(""Transaction created "" + t3);"
"<line2>    log.info(""Server: {}"", getOpts().getUrl());<line2>    log.info(""Username: {}"", getOpts().getUsername());<line2>    log.info(""Source language: {}"", DEFAULT_SOURCE_LANG);<line2>    log.info(""Translation language: {}"", getOpts().getTransLang());<line3>      log.info(""Project: {}"", getOpts().getProject());<line4>    log.info(""Glossary file: {}"", getOpts().getFile());<line4>    log.info(""Batch size: {}"", getOpts().getBatchSize());<line25>        log.error(""Project {} not found"", project);<line31>    log.info(""Pushing glossary document [{}] to server"", glossaryFile.getName());<line37>      log.info(""Total entries: {}"", totalEntries);<line45>        log.info(""Pushed {} of {} entries"", totalDone, totalEntries);"
"<line14>      log.info(""objStat indicates collection type that does not support this operation:{}"", objStat);"
"<line12>    LOG.info(""Sent 0: "" + outbound[0]);<line12>    LOG.info(""Sent 1: "" + outbound[1]);"
"<line10>        log.error(""Exception reading property ""+ CHUNK_QUEUE_LIMIT+ "". Defaulting internal queue size to ""+ QUEUE_SIZE);<line12>    log.info(""Using NonBlockingMemLimitQueue limit of "" + MAX_MEM_USAGE);"
"<line4>      LOG.info(""GeolocationFileStore: reading location data from file '"" + dataFileName + ""'"");<line8>        LOG.info(""GeolocationFileStore: reading location data from resource '"" + dataFileName + ""'"");"
"<line6>      logger.error(""Update alert rule last check time, "" + validAlertRuleList, e);"
"<line56>            logger.error(e);<line60>        logger.info(""Unable to connect to "" + myhost + "" after "" + ConnectionRetries + "" tries"");<line71>          logger.info(e);<line76>      logger.info(e);"
"<line4>        LOG.info(""TestStep [""+ result.getTestStepName()+ ""] response: ""+ response.getStatus()+ "" - ""+ response.getContent().getText());<line5>        LOG.debug(""Missing HAR response for TestStep ["" + result.getTestStepName() + ""]"");"
"<line2>    logger.trace(""Firing Disconnected!"");"
"<line6>        logger.debug(""No '"" + paramOrCookieName + ""' request param or cookie found"");"
"<line1>    LOGGER.trace(""mainMenuPerformed: {}"", menu);"
"<line5>        logger.error(""Failed to initialize properties file. Verification failed."");<line9>      logger.error(""Properties file path is missing."");"
"<line3>      LOGGER.trace(format(""Create typed query for SELECT : %s"", regularStatement.getQueryString()));"
"<line20>          logger.debug(pi);<line22>        logger.warn(""Package namespace unknow: '"" + namespace + ""'"");"
"<line35>        logger.info(""Loading torrent from "" + f.getName());<line37>      logger.info(""Starting tracker with {} announced torrents..."", t.getTrackedTorrents().size());<line39>      logger.error(""{}"", e.getMessage(), e);"
"<line8>      logger.error(""Problem creating item from XML: "" + de.getLocalizedMessage());<line8>      logger.debug(""commonPartXML: "" + commonPartXML);"
"<line27>              LOGGER.info(""Specified tiling parameters are not valid. tileWidth = ""+ tileWidth+ "" tileHeight = ""+ tileHeight);<line40>                LOGGER.info(""Specified quality is not valid (it should be in the range [0,1]).""+ "" compressionQuality = ""+ compressionQuality);"
"<line1>    LOG.info(""changePassword({})"", user.toString());<line6>      LOG.error(e.getMessage(), e);"
"<line8>      logger.error(""[logTransaction]"" + type + "","" + name + "","" + task, th);"
"<line6>        log.warn(""The checksum for {} is invalid."", checksumFile);<line11>        log.error(""Checksum read error during validation on {}"", checksumFile);<line14>        log.error(""Digester failure during checksum validation on {}"", checksumFile);<line18>        log.error(""File not found during checksum validation: "", e);"
"<line10>      LOGGER.info(""Unsubscribe failed for mailbox {}"", mailboxName, e);"
"<line22>            logger.warn(""Couldn't delete "" + resource.getLocation() + "". "" + e.getMessage());<line37>            logger.warn(""Couldn't delete "" + iFile.getLocation() + "". "" + e.getMessage());"
"<line10>    logger.debug(""Inserting remote "" + remoteName + "" for site "" + siteId + "" into database."");<line17>      logger.debug(""Encrypt password before inserting to database"");<line23>      logger.debug(""Encrypt token before inserting to database"");<line29>      logger.debug(""Encrypt private key before inserting to database"");<line34>    logger.debug(""Insert site remote record into database"");"
"<line1>    log.debug("""");<line3>      log.warn(""no Federator Flows in conversionTable: {}::{}"", networkId, flow.getFlowId());<line15>        log.warn(""not found Original Flow: {}"", flowId);<line18>        log.debug(""not flow's status none."");<line21>    log.debug(""next federate stauts:: none"");"
"<line1>    logger.debug(""Waiting for finishing Homematic device discovery scan"");<line6>      logger.error(""Error waiting for device discovery scan: {}"", ex.getMessage(), ex);<line11>    logger.debug(""Finished Homematic device discovery scan on gateway '{}'"", gatewayId);"
"<line20>    log.info(""init data source success"");"
"<line2>    LOGGER.info(""testvoidInRPC() is called!"");"
<line23>      logger.warn(sb.toString());
"<line1>    logger.debug(""Creating user..."");"
"<line2>    LOG.trace(""access to save"");"
"<line4>      logger.error(""could not create platform mbean server: {}"", t.getMessage(), t);"
"<line4>        logger.debug(""Starting broadcast for {}"", broadcastAddress.toString());<line10>          logger.warn(""Error sending broadcast: {}"", e.toString());"
"<line1>    logger.debug(""MetadataDB executeUpdate:"" + s);"
"<line4>        Logger.debug(this, ""attempting to lock"");"
"<line4>      this.logger.warn(""New event store system is disabled"");<line10>      this.logger.error(""Failed to get the configured event store [{}]"", this.configuration.getEventStore(), e);<line26>          this.logger.info(""Synchronizing legacy events from index {} to {}"", offset, offset + events.size());"
"<line24>        LOGGER.info(""No model definition found in "" + submodel.getId() + ""."");"
"<line5>    LOGGER.debug(""Patch User {} with {}"", id, partialDto);"
"<line11>    LOG.info("">>> Model : "" + model.toString());"
"<line10>      LOG.debug(""Load from ODocument, field:{}, schemaType:{}, docField:{}, storeType:{}"",new Object[] {field.name(), fieldSchema.getType(), docf, storeType});"
"<line7>        LOG.error(""Unable to process "" + hostname + "" as a valid hostname"", e);"
"<line10>        LOGGER.warn(""Processing notifications for this run will be stopped, because the circuit breaker is""+ "" open."",exc);"
"<line1>    log.error(""poll invoked but consumer stopped for topic"" + topic, new RuntimeException(""stacktrace""));"
"<line6>      logger.fatal(""Connect: wrong server address"", ex);<line21>          logger.error(""Connect: unknown server error"", exep.getCause());<line29>        logger.fatal(""Connect: unknown error"", ex);<line32>      logger.fatal(""Connect: unknown IO error"", ex);<line38>      logger.warn(""Connect: wrong versions"");<line47>      logger.fatal(""Connect: FAIL"", t);"
"<line8>        logger.error(""Failed to parse the value to get the parameter id."", e);"
"<line2>    logger.debug(""LogWaitChecker: Trying to match '%s' [Pattern: %s] [thread: %d]"",txt, pattern.pattern(), Thread.currentThread().getId());<line10>      logger.debug(""Found log-wait pattern in log output"");"
"<line34>                    LOG.debug(""compress: '{}'"", pathInZipFile);<line49>            LOG.debug(""compress: '{}]"", pathInZipFile);<line60>      LOG.error(""compress:"" + e.getMessage(), e);"
"<line2>    LOG.debug(""Visit is field null expression: "" + ctx.getText());<line4>    LOG.debug(""End visit is field null expression: "" + ctx.getText());"
"<line4>    logger.debug(""Compiling Groovy code:\n{}"", code);"
"<line3>    log.info(""ClientObserver -> onRegistrationSuccess...  EndpointName [{}] [{}]"",request.getEndpointName(),registrationID);"
"<line3>      log.debug(""Node {"" + nodeName + ""} joined the cluster."");"
"<line30>          LOG.warn(""Failed to choose a bookie from {} : ""+ ""excluded {}, fallback to choose bookie randomly from the cluster."",bookieNodeToReplace.getNetworkLocation(),excludeBookies);"
"<line18>    logger.debug(""Storage path updated to {}"", path);"
"<line3>      LOG.debug(""GetVertexStatus via AM for app: "" + appId + "" dag: "" + dagId + "" vertex: "" + vertexName);"
"<line3>      LOGGER.trace(""->COLLECTED POINT X VALUE(LON): "" + myCurrentLon);"
<line1>    log.warn(StringUtils.safeFormat(message, formatArgs), t);
"<line3>    log.info(""Starting..."");<line17>        log.error(e.getMessage(), e);<line23>    log.info(""Took: "" + duration + "" ms"");<line23>    log.info(""Save: "" + saveCount.intValue() + "" files, "" + bytesWritten.get() + "" bytes"");<line23>    log.info(""Read: "" + (read1Count.get() + read2Count.get()) + "" files, "" + bytesRead.get() + "" bytes"");<line23>    log.info(""Average save time (ns): "" + (double) saveTime.get() / (double) saveCount.get());"
"<line1>    log.info(""stopService, stopping..."");<line5>      log.warn(""ServerSocket.close"", e);<line7>    log.info(""stopService, complete."");"
"<line9>    logger.info(""Attempting to start the connector with an INVALID configuration, so MULTIPLE error""+ "" messages and exceptions will appear in the log"");"
"<line2>    logger.info(""Started testStaleIndexCleanup()"");<line32>          logger.info(""Updated {} of {} times"", count, numEntities * numUpdates);"
"<line2>    LOG.info(""Catalog URI {}"", catalog.getUri());<line10>    LOG.debug(""Target database {}, table {}"", db, table);<line41>      LOG.debug(""partitionKeys {} and partitionValue {}"",partitionKeys.toString(),partitionValues.toString());"
<line4>      LOGGER.fatal(tcx.getMessageAndLocation());
"<line3>    log.debug(""Start uploading content"");<line5>    log.debug(""Waiting 10 seconds to auto-termination..."");<line7>    log.debug(""Start downloading file"");<line10>      log.debug(""The uploadad and downloaded files are equal"");<line11>      log.debug(""The uploadad and downloaded files are different"");"
"<line7>      logger.error(""Error updating configuration"", e);"
"<line2>    log.trace("">> update() accountAsset = {}"", accountAsset);<line8>      log.trace(""<< update() INSERT accountAsset = {}"", accountAsset);<line11>      log.trace(""<< update() DELETE, height={}, accountAsset = {}"", height, accountAsset);"
"<line12>    logger.info(""Creating entity in fabric {} at {}{}"",new Object[] {this,location,(creation != null && !creation.isEmpty() ? "", properties "" + creation : """")});"
"<line1>    log.info(""[I198] creating BytBuffer from {} chunks"", buffers.size());<line11>    log.info(""[I208] partData: size={}"", partData.capacity());"
"<line12>      LOG.info(new StringBuffer(""Encrypted "").append("" attributes of table "").append(tableName));<line13>      LOG.error(new StringBuffer(""Caught exception, while encrypting "").append("" attributes of table "").append(tableName),e);"
"<line3>    LOGGER.debug(""Updating Cache after content modification: {}"", update);<line6>      LOGGER.error(""Error processing Event"", ex);"
"<line2>      LOGGER.warn(""Switching the order manager used by the ElevationManager."");"
<line23>      log.error(systemException, systemException);
<line23>      log.error(systemException, systemException);
"<line4>      LOGGER.warn(""Hostname not found, this leads to inproper client-configuration."");<line6>      LOGGER.warn(""BaseDN not found, this leads to inproper client-configuration."");<line22>        LOGGER.info(""Replaced ${myip}-value 'localhost' with '{}'"", hostname);<line23>        LOGGER.error(""Cannot obtain the host ip-address, using default: "" + hostname);<line47>              LOGGER.error(""UTF-8 encoding not supported"", e);"
"<line42>            SHUTDOWNlogger.debug(""Disposing component [{}]..."", instance.getClass().getName());<line43>            SHUTDOWNlogger.debug(""Component [{}] has been disposed"", instance.getClass().getName());<line44>            this.logger.error(""Failed to dispose component with role type [{}] and role hint [{}]"",componentEntry.descriptor.getRoleType(),componentEntry.descriptor.getRoleHint(),e);"
"<line55>        log.info(""*** Starting test: "" + msg);"
"<line12>      logger.debug(""OvenData = {}"", response);<line15>      logger.debug(""Error processiong Get request {}"", urlStr);<line22>      logger.debug(""Unknwon Error: {}"", e.getMessage());<line29>      logger.debug(""Setting thing '{}' to OFFLINE: Error '{}': {}"", thingUID, error, errorDetail);"
"<line2>    LOG.info(""Creating Zookeeper Client connecting to {}"", zkConnection);<line9>    LOG.info(""Connecting to ZK cluster {}"", zkClient.getState());<line11>    LOG.info(""Connection to ZK cluster {}"", zkClient.getState());"
"<line22>      LOGGER.warn(""Transformer {}. Regulating control forced to off. Only one control is supported"", id);"
"<line2>    log.info(""Service Started"");"
"<line8>        log.error(""Some processors are still active"");"
"<line1>    log.debug(""failed deposit: {}"", message);"
"<line8>              logger.debug(""Ignoring self commit message."");<line9>              logger.debug(""Received remote commit message. serverId={}, tx={}"",remoteCommit.getServerId(),remoteCommit.getTxNumber());"
"<line9>        log.warn(Messages.getString(""TableViewerCreator.columnNoIBeanProperty"", column.getId(), column.getTitle()));"
"<line2>    logger.debug(""Getting product count: "", super.getProductCount());"
"<line6>          LOG.warn(""Ignoring unexpected runtime exception"", e);<line7>          LOG.warn(""Ignoring unexpected exception"", e);<line14>      LOG.info(""accept thread exitted run method"");"
"<line10>      LOGGER.error(""Unable to perform aggregation on non-geotools feature adapter '""+ adapter.getTypeName()+ ""'"");"
"<line8>          log.warn(""SQL driver deregistration failed"", e);<line36>                  log.debug(""Set field {} to null in class {}"", field.getName(), clazz.getName());<line38>                log.debug(""Could not set field {} to null in class {}"",field.getName(),clazz.getName(),t);<line42>          log.debug(""Could not clean fields for class {}"", clazz.getName(), t);"
"<line5>      logger.debug(""sendCommand getSignalStrength :: {}"",SimTechSim7000AtCommands.getSignalStrength.getCommand());<line27>          logger.trace(""getSignalStrength() :: +CSQ={}"", sCsq);<line37>            logger.trace(""getSignalStrength() :: signalStrength={}"", signalStrength);"
"<line1>    log.debug(""deleting StgMUmsetzStat instance"");<line3>      log.debug(""delete successful"");<line4>      log.error(""delete failed"", re);"
"<line9>      logger.info(""Installing {}"", plugin.getClass().getName());"
<line3>    LOG.debug(QUERY_CONVERTED_QUERY, queryJsonNode);
"<line10>    LOGGER.info(""Certificate created (SHA-256 fingerprint: {})"", fingerprint);"
"<line4>      log.error(""invalid parameters for validator "" + abstractParameter.getClass().getName());"
"<line3>    log.debug(""updateContents:: npanels IS null ? {}, client IS null ? {}"",npanel == null,getClient() == null);"
<line20>      log.error(errmsg, e);
"<line8>        log.warn(""Unable to calculate installation path. setting to 'unknown'"");"
"<line7>        LOGGER.info(""Found vertex: "" + vertex);"
"<line11>        LOGGER.warn(""Additional Header has wrong format {}"", nameValuePair);<line14>      LOGGER.info(""Set additional header: {}"", nameValuePair);"
"<line8>        LOGGER.debug(""Loading local.properties from env var [$LATKE_LOCAL_PROPS="" + localPropsEnv + ""]"");<line10>        LOGGER.debug(""Loading local.properties from classpath [/local.properties]"");<line14>        LOGGER.debug(""Loaded local.properties"");"
"<line4>    log.info(""Exporting the DwC Archive to Avro started {}"", inputPath);"
"<line12>      LOG.error(""Error occured while doing a classification"", e);<line13>      LOG.error(""I/O exception encountered while doing a classification"", e);"
"<line10>      log.error(""Unable to handle unknown extensionPoint "" + extensionPoint);"
"<line10>      logger.warn(""failed while to convert message. applicationName:{}, original:{}, message:{}."",applicationName,resultMap,e.getMessage(),e);"
"<line7>    LOG.debug(""Starting file uploading..."");<line15>              LOG.debug(""Reads image content from the field ; name: '{}'."", name);<line32>              LOG.debug(""File '{}' upload has been successfully processed."", name);"
"<line6>      LOG.trace(""no meters on device for node: {} -> SKIPPING"", nodeId.getValue());<line11>      LOG.trace(""removing meter {} - absent in config {}"", meter.getMeterId(), nodeId);"
"<line9>        log.info(""No configuration supplied for table "" + table);"
"<line2>      ActiveMQRALogger.LOGGER.trace(""setRetryInterval("" + retryInterval + "")"");"
"<line11>      logger.debug(""No root domains configured, UAA is catch-all domain for host:"" + hostname);<line13>    logger.debug(""Unable to determine subdomain for host:""+ hostname+ ""; root domains:""+ Arrays.toString(defaultZoneHostnames.toArray()));"
"<line19>      log.error(""Error while creating relation"", ce);"
"<line17>          LOGGER.warn(""Could not authenticate bearer / jwt token"", e);<line21>          LOGGER.warn(""Server Error."", e);"
"<line5>        logger.info(""properties loaded from {}"", propertiesFilePath);<line7>        logger.error(""Loading properties {} failed"", propertiesFilePath, e);"
<line5>        log.debug(_NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey);
"<line2>    logger.info(""Deleting host "" + fqdn + "" in cluster "" + clusterName);"
"<line20>    LOGGER.debug(""Creating new derivate with ID {}"", derivateID);"
"<line12>    log.info(""Pruned data for organisation unit: "" + organisationUnit);"
"<line20>        LOGGER.info(""End of file reached"");<line24>      LOGGER.error(""Could not read shape file."" + e, e);"
"<line2>      log.warn(""waitWarrantBlockChange invoked from invalid context"");<line4>      log.debug(""waitWarrantBlockChange {}"", warrant.getDisplayName());<line6>      log.debug(""waitWarrantBlockChange returns immediately"");<line24>                log.trace(""notify waitWarrantBlockChange of property change"");"
"<line6>      logger.debug(""Upserting a Permission. userLink [{}], permission id [{}]"",userLink,permission.getId());<line13>      logger.debug(""Failure in upserting a Permission due to [{}]"", e.getMessage(), e);<line13>      return Observable.error(e);"
"<line1>    log.info(""...Deleting already-released AlterDDL in unreleased directory: ""+ unreleasedSqlFileList.size());"
"<line12>      log.error(""Error occured, Caused by {}."", e);"
"<line11>      LOGGER.error(""UTF-8 not supported?"", e);"
<line10>      logger.debug(e.getMessage(), e);
"<line7>      log.error(""Redis protocol error"", e);"
"<line6>        LOGGER.error(""File not readable %s"", file, e);<line7>    } else LOGGER.error(""File not found %s"", file);"
<line22>        logger.error(e1.getMessage(), e1);
"<line6>      log.error(""Error parsing certificate of host '{}': {}"", hostName, ex.getMessage());<line6>      log.debug(""Exception"", ex);"
"<line5>    log.info(""[{}] [{}] {} {}"",jobId.getName(),containerId.substring(0, Math.min(7, containerId.length())),stream.id(),stringBuilder.toString());"
"<line4>      LOG.info(""The derived from fraction {} ({}) is greater than its max value {}, max value will be""+ "" used instead"",memoryDescription,relative.toHumanReadableString(),rangeFraction.getMaxSize().toHumanReadableString());<line6>      LOG.info(""The derived from fraction {} ({}) is less than its min value {}, min value will be used""+ "" instead"",memoryDescription,relative.toHumanReadableString(),rangeFraction.getMinSize().toHumanReadableString());"
<line13>      log.error(portalException, portalException);
"<line2>      log.debug(""{} moveTo {} {} {}"", getName(), topStom, midStom, lowStom);"
"<line8>          logger.error(""unhandled exception in plugin on tag style changed"", ex);<line11>      logger.error(""Unknown error in plug-in interface when tag style was added"", ex);"
"<line4>        logger.debug(""Request success. request={}, result={}"", logString(message), logString(response));<line7>        logger.info(""Request fail. request={}, result={}"", logString(message), logString(response));"
"<line30>      LOG.debug(""Request {} had {} retries in a row, not retrying again (num retries on failure: {})"",request.getId(),numRetriesInARow,request.getNumRetriesOnFailure());<line32>    LOG.debug(""Request {} had {} retries in a row - retrying again (num retries on failure: {})"",request.getId(),numRetriesInARow,request.getNumRetriesOnFailure());"
"<line3>    logger.warn(""{} for buffer with length {} was fragmented {} times (total fragmented operations: {})"",operation,buffer.limit(),iterations,total);"
"<line3>    log.debug(""Building command {} with arguments {}"", mc.getName(), argAssignmentList);"
<line2>    LOGGER.trace(ENTERING, methodName);<line2>    LOGGER.trace(EXITING, methodName);
"<line1>    logger.debug(""Schedule clock sync job"");<line10>                  logger.debug(""Error syncing clock: {}"", e.getMessage());"
"<line13>        logger.warn(""action=fetchWeather error date=""+ date+ "", lat=""+ latitude+ "", lon=""+ longitude+ "", city=""+ closestCity.geo_name);"
"<line10>        log.debug(String.format(""A cluster monitor is not found in autoscaler context "" + ""[cluster] %s"",clusterId));"
"<line3>      logger.debug(""{} channelActive()"", logPrefix());<line9>      logger.debug(""{} channelActive() done"", logPrefix());"
"<line1>    log.debug("""");"
"<line2>      log.error(""cannot update null element"");<line14>      log.error(""error on updating "" + element.getClass().getSimpleName() + "": "" + e.getMessage());"
"<line4>      Log.error(""Cannot create stable ID: No class specified! Will return generated ID: "" + id);"
"<line7>      LOGGER.debug(""Comparing '"" + lastActTZ + ""' with '"" + lastTimeZoneTZ + ""'."");"
"<line8>        LOG.warn(""An auxiliary connection for device {}, but no primary connection. Refusing""+ "" connection."",deviceInfo);<line11>          LOG.info(""An auxiliary connection was added to device: {}"", deviceInfo);<line13>          LOG.warn(""Not able to add auxiliary connection to the device {}"", deviceInfo);<line17>      LOG.info(""Device {} connected."", deviceInfo);<line20>        LOG.warn(""Device {} is already trying to connect, wait until succeeded or disconnected."",deviceInfo);<line24>          LOG.warn(""Device {} is already in termination state, closing all incoming connections."",deviceInfo);<line26>        LOG.warn(""Device {} already connected. Closing previous connection"", deviceInfo);<line27>        LOG.info(""Old connection dropped, creating new context chain for device {}"", deviceInfo);<line29>        LOG.info(""No context chain found for device: {}, creating new."", deviceInfo);"
"<line10>        logger.warn(""error while deleting remote repository"", e);"
"<line13>      log.error(""Unable to get asset entries"", exception);"
"<line2>    LOG.debug(""retrieving page "" + pageUrl + "" for group "" + groupId);"
"<line8>        logger.warn(String.format(""something goes wrong when getWithoutAck data from server:%s"",currentConnector != null ? currentConnector.getAddress() : ""null""),t);<line10>        logger.info(""restart the connector for next round retry."");"
"<line13>          LOG.info(""Failed to send audit message:"", e);"
"<line6>    log.info(""Inited ML service"");"
"<line3>    LOG.info(""onMode.string = "" + string);<line3>    LOG.info(""onMode.ircUser = "" + ircUser);<line3>    LOG.info(""onMode.ircModeParser = "" + ircModeParser);"
"<line13>        logger.debug(""Unable to get Cqs. Error: {}"", cqe.getMessage(), cqe);"
"<line6>      log.error(""session was null"");"
"<line32>          LOG.warn(String.format(""Can't deserialize matching rule order type for id '%s'. Falling back to default""+ "" (%s)"",orderTypeId, ArrangementMatchRule.DEFAULT_ORDER_TYPE.getId()));"
"<line4>      log.warn(""unable to request host state update: "", e);"
"<line3>        logger.debug(""abortTest: Disconnecting from distributed system and sending null chunk to abort"");"
"<line3>      LOG.warn(""Failed to locate region in '""+ tableName+ ""', row='""+ Bytes.toStringBinary(req.row)+ ""', locateType=""+ req.locateType,error);"
<line7>      log.error(exception, exception);
"<line8>        logger.warn(""Received AF_INCOMING_MSG but no listeners. ""+ ""Message was from {} and cluster {} to end point {}. Data: {}"",msg.getSrcAddr(),msg.getClusterId(),msg.getDstEndpoint(),msg);<line9>        logger.trace(""Received AF_INCOMING_MSG from {} and cluster {} to end point {}. Data: {}"",msg.getSrcAddr(),msg.getClusterId(),msg.getDstEndpoint(),msg);<line18>          logger.error(""Error AF message listener notify."", e);"
"<line19>      log.debug(""Failed to find radius clients"", e);"
"<line12>                    logger.warn(""Exception at getOAuthAccessTokenAsync"", e);"
"<line9>      LOG.error(""Error for model: {} and context: {}"", model, dataObject, e);"
"<line9>      logger.error(""Failed to obtain WifiInterfaceMode"", e);"
<line19>      LOG.error(message, e);
"<line1>    logger.trace(""setting passivator: {}"", p);"
"<line4>      log.debug(""Trying AK 2.2 SslFactory methods."");<line6>      log.debug(""Could not find AK 2.2 SslFactory methods. Trying AK 2.3+ methods for SslFactory."");<line9>        log.debug(""Using AK 2.2-2.5 SslFactory methods."");<line10>        log.debug(""Could not find AK 2.3-2.5 SslFactory methods. Trying AK 2.6+ methods for SslFactory."");<line12>          log.debug(""Using AK 2.6+ SslFactory methods."");"
"<line2>    LOG.debug(""Slot request with allocation id {} failed for slot {}."", allocationId, slotId, cause);<line11>      LOG.debug(""There was not pending slot request with allocation id {}. Probably the request has been""+ "" fulfilled or cancelled."",allocationId);"
"<line20>              log.debug(""Adapter ["" + name + ""] is stopping receivers"");<line28>                  log.debug(""Adapter [""+ getName()+ ""] waiting for receiver [""+ receiver.getName()+ ""] in state [""+ receiver.getRunState()+ ""] to stop"");<line31>                    log.warn(""Interrupted waiting for threads of receiver [""+ receiver.getName()+ ""] to end"",e);<line33>                log.info(""Adapter [""+ getName()+ ""] successfully stopped receiver [""+ receiver.getName()+ ""]"");<line45>              log.debug(""Adapter ["" + name + ""] is stopping pipeline"");"
"<line4>      LOG.debug(""<== HBaseResourceMgr.connectionTest() ServiceName: ""+ serviceName+ ""Configs""+ configs);<line8>      LOG.error(""<== HBaseResourceMgr.connectionTest() Error: "" + e);<line11>      LOG.debug(""<== HBaseResourceMgr.connectionTest() Result: "" + ret);"
"<line26>              logger.warn(""Found a structured scope, ignoring structure"");<line27>              logger.debug(""found unexpected entry"");<line31>            logger.debug(""Found unexpected entry"");<line39>    logger.info(""Done reading system scopes"");"
"<line1>    logger.trace(""Processor for thing {} writing command to device"", thingID());<line2>      logger.debug(""Error writing to device because output stream object is null"");"
"<line6>    logger.debug(""Entry point of generateCoordinates()"");<line6>    logger.debug(""We have a molecules with "" + numAtoms + "" atoms."");<line37>      logger.debug(""*** Start of handling the rest of the molecule. ***"");"
"<line3>      logger.trace(LogMarker.SERIALIZER_VERBOSE, ""Writing Boolean {}"", value);"
"<line1>    log.debug(""attaching dirty MbStatus instance"");<line3>      log.debug(""attach successful"");<line4>      log.error(""attach failed"", re);"
"<line6>      LOG.error(""Unable to get List of Trust Bundles: "", ex);"
"<line4>      logger.error(LoggingAnchor.THREE,MessageEnum.RA_GENERAL_EXCEPTION.toString(),ErrorCode.BusinessProcessError.getValue(),""Exception in encryptPassword "",e);"
"<line9>    LOGGER.info(""Logging EMR cluster definition being deleted. emrClusterDefinition={}"",xmlHelper.objectToXml(createEmrClusterDefinitionFromEntity(emrClusterDefinitionEntity), true));"
"<line3>        log.info(""{} - {} socket {}"", reset ? ""Resetting"" : ""Closing"", logMessage, socket);<line4>        log.debug(""{} socket {}"", reset ? ""Resetting"" : ""Closing"", socket);<line10>          log.trace(""doSocketClose(socket[{}], logMessage[{}], reset[{}] - ignoring exception raised by""+ "" Socket.shutdownInput()"",socket,logMessage,reset,ignoredEx);<line16>          log.trace(""doSocketClose(socket[{}], logMessage[{}], reset[{}] - ignoring exception raised by""+ "" Socket.shutdownOutput()"",socket,logMessage,reset,ignoredEx);<line24>          log.trace(""doSocketClose(socket[{}], logMessage[{}], reset[{}] - ignoring exception raised by""+ "" Socket.setSoLinger({}, {})"",socket,logMessage,reset,on,linger,ignoredEx);<line29>        log.trace(""doSocketClose(socket[{}], logMessage[{}], reset[{}] - ignoring exception raised by""+ "" Socket.close()"",socket,logMessage,reset,ignoredEx);"
"<line7>    log.info(""Sending order using xml payload: {}"", xml);<line9>    log.info(""Created new order with id "" + id);"
"<line13>          logger.error(""Failed to initialize JDBC driver class '"" + driverClassName + ""'!"", e);<line24>        logger.error(""Failed to get JDBC connection using URL: "" + url, e);"
"<line25>          LOGGER.warn(""No acceptable certificates found. Acceptable issuers are: ""+ myProviderFilter.getAcceptableIssuers());<line42>      LOGGER.warn(""Could not get certificate from provider: "" + e, e);"
"<line11>        log.debug(""Found constructor for Page of type '{}' and argument of type '{}'."",pageClass,argumentType);<line12>        log.debug(""Page of type '{}' has not visible constructor with an argument of type '{}'."",pageClass,argumentType);"
"<line7>    LOGGER.info(""handleResponse for MessageType: {}"", messageType);<line17>      LOGGER.error(""Device Response not ok."", exception);<line29>      LOGGER.info(""We used sensor status to keep 104 LMDs connected, ignore response: {}"", responseMessage);"
"<line5>      logger.error(""Test error"", e);"
"<line1>    log.debug(""merging MbB2mDel instance"");<line3>      log.debug(""merge successful"");<line5>      log.error(""merge failed"", re);"
<line38>      log.error(systemException, systemException);
"<line1>    logger.debug(""Deactivating ExampleComponent"");"
"<line8>      log.info(""calendar empty, returning empty set"");<line15>          log.trace(""processing event "" + event.getSummary().getValue());<line33>            log.trace(""added event "" + newevent);"
"<line17>        logger.debug(""The policy of region is {}"",(this.enablePersistence ? DataPolicy.PERSISTENT_REPLICATE : DataPolicy.REPLICATE));<line29>        logger.debug(""{}: Attempting to create queue region: {}"", this, this.regionName);<line45>          logger.debug(""{}: Created queue region: {}"", this, this.region);<line47>        logger.fatal(String.format(""%s: The queue region named %s could not be created"",new Object[] {this, this.regionName}),e);"
"<line12>            LOG.debug(""Asynchronous revalidation failed due to I/O error"", ex);<line14>            LOG.error(""HTTP protocol exception during asynchronous revalidation"", ex);<line16>            LOG.error(""Unexpected runtime exception thrown during asynchronous revalidation"", ex);"
"<line10>      logger.error(""invokeInFxThreadAndWait() failed"", e);"
"<line10>    LOG.info(""Copy job progress: {}/{}"", progress.getProgress(), progress.getProgressMax());"
"<line22>          logger.error(method + "": bad or missing specifier!"");"
"<line2>    LOG.info(""-- Loading Hetu Metastore --"");"
"<line16>                  logger.debug(""Background started."");<line20>                    logger.debug(""Background done."");<line27>              logger.debug(""Refreshing."");"
<line2>    logger.info(string, o);
"<line26>      logger.warn(String.format(""Failed to dispatch event %s : Input must have a pluginName or a valid pluginKey""+ "" specified json=%s"",commandType, metadata.getEventJson()));"
"<line9>    logger.debug(""About to unmarshal variable from payload: '{}'"", variablePayload);<line11>    logger.debug(""Setting variable '{}' on process instance with id {} with value {}"",varName,processInstanceId,variable);"
"<line2>    logger.debug(""KHD start of testDispatchingJobsHigherMaxLoad"");"
"<line3>      log.debug(String.format(""Cancel action %s"", action));"
"<line6>                log.info(""JMXFetch is closing"");"
<line7>      log.error(e, e);
"<line4>    log.info(""creating work directory {}"", location);"
"<line2>    log.info(""Console: "" + ((System.console() != null) ? ""available"" : ""not available""));<line2>    log.info(""Headless: "" + (GraphicsEnvironment.isHeadless() ? ""yes"" : ""no""));<line6>      log.info(""If you are running ""+ applicationName+ "" in a server environment, please use '-Djava.awt.headless=true'"");<line47>      log.info(""Running in server environment or from command line: disabling interactive shutdown""+ "" dialog."");"
<line17>      log.error(UNEXPECTED_ERROR_OCCURRED, exception);
"<line3>      logger.info(""SegmentAppendTrieDict needn't to copy"");"
"<line6>            logger.debug(""Connection attempt failed to start for device {}"", device.getAddress());<line9>            logger.debug(""Connection to device {} timed out"", device.getAddress());<line14>              logger.debug(""Service discovery for device {} timed out"", device.getAddress());<line18>          logger.debug(""Device information fetched from the device: {}"", device);<line25>          logger.warn(""Participant '{}' threw an exception"", participant.getClass().getName(), e);"
"<line9>      LOGGER.debug(""Error while loading internals: "" + e.getMessage());"
"<line5>        LOGGER.info(""PURGE ACTION - Purged nodeId {}"", nodeId);"
"<line2>    logger.info(""Downloading gene Ensembl data (gtf, pep, cdna, motifs) ..."");"
"<line3>    log.debug(""Closing ShardQueryLogic: "" + System.identityHashCode(this));<line4>      log.debug(""ScannerFactory was never initialized because, therefore there are no connections to""+ "" close: ""+ System.identityHashCode(this));<line5>      log.debug(""Closing ShardQueryLogic scannerFactory: "" + System.identityHashCode(this));<line13>          log.debug(""Cleaned up "" + nClosed + "" batch scanners associated with this query logic."");<line20>          log.debug(""Cleaned up "" + nClosed + "" scanner sessions."");<line22>        log.error(""Caught exception trying to close scannerFactory"", e);<line26>        log.debug(""Closing ShardQueryLogic planner: ""+ System.identityHashCode(this)+ '('+ (this.getSettings() == null ? ""empty"" : this.getSettings().getId())+ ')');<line28>        log.error(""Caught exception trying to close QueryPlanner"", e);<line32>        log.debug(""Closing ShardQueryLogic queries: "" + System.identityHashCode(this));<line34>        log.error(""Caught exception trying to close CloseableIterable of queries"", e);<line38>        log.debug(""Closing ShardQueryLogic scheduler: "" + System.identityHashCode(this));<line44>        log.error(""Caught exception trying to close Scheduler"", e);"
"<line2>    LOG.info(""Executing operation putTransformer"");"
"<line5>    LOG.debug(""Closing reader on path:{}"", path);"
<line53>      LOG.info(msgForLog);
"<line7>              log.error(""Failed to execute shutdownhook"", e);"
"<line50>      logger.error(""Start RocketMQ consumer error"", ex);"
"<line4>    LOGGER.debug(""getById {} / {}"",identifier,URLEncoder.encode(identifier, StandardCharsets.UTF_8.toString()));"
"<line1>    log.debug(""Adding file {} to work {}"", childResc, work.getPid());<line32>          log.warn(""Failed to ingest file {} due to a checksum mismatch, {} retries remaining: {}"",childPid.getQualifiedId(),CHECKSUM_RETRIES - retryCnt,e.getMessage());"
"<line5>      LOGGER.debug(""XEditor extension function calling {} {}"", clazzName, methodName);<line15>      LOGGER.warn(""Exception in call to external java method"", ex);"
"<line20>                LOGGER.debug(""Configure channel {} with method {} on bean {}"", channel, method, bean);<line28>              LOGGER.debug(""Configure again channel {} with method {} on bean {}"",channelName,method,bean);<line34>              LOGGER.debug(""Channel {} already initialized. Not called method {} on bean {}"",channelName,method,bean);"
"<line5>      logger.error(result);<line6>      logger.error(""Unexpected error."", e);"
"<line1>    LOG.trace(""removeTableFromGroup: "" + groupName + "": "" + schemaName + ""."" + tableName);"
"<line12>              LOGGER.warn(""KafkaTopic {} could not be deleted, since it doesn't seem to exist"",resourceName.toString());<line27>                        LOGGER.debug(""KafkaTopic {} deleted {}"", resourceName.toString(), notExists);"
<line16>      log.error(systemException, systemException);
"<line4>      LOGGER.trace(""Using search processor: SearchGet"");<line7>      LOGGER.trace(""Using search processor: SearchSet"");"
"<line3>    LOG.info(""Styx REST Interface incoming request uri={}, method={}"",request.getRequestURI(),request.getMethod());"
"<line4>        logger.debug(""String already represents JSON. Skipping conversion in favor of""+ "" 'getBytes(StandardCharsets.UTF_8'."");"
"<line2>      log.warn(""cast method received a null"");<line22>    log.error(""Received an object I don't know how to cast! The object is:""+ dataObj+ "", of ""+ dataObj.getClass());"
"<line13>      logger.warn(""failed to parse expiration date on SSR DOCS"");"
"<line32>      logger.error(""getByCriteriaWithAliasByOrder failed, criteria = "" + criterion.toString(), e);"
"<line2>      logger.debug(""Entered StatAlertsManager.updateAlertDefinition *****"");<line8>            logger.debug(""Removed StatAlertDefinition: {}"", defns[i].getName());<line20>      logger.debug(""Exiting StatAlertsManager.updateAlertDefinition *****"");"
"<line1>    logger.trace(""supported identifiers: {}, searching for objects {}"", supportedIdentifiers, cosemObjects);"
"<line7>    logger.debug(""Will look for network with name-label:""+ label+ "" on host ""+ citrixResourceBase.getHost().getIp());<line17>      logger.debug(""Network object:"" + nw.getNetwork().getUuid(conn));<line19>      logger.debug(""PIF object:"" + pifRec.uuid + ""("" + pifRec.device + "")"");<line43>      logger.error(""An error occurred while fetching the interface for ""+ label+ "" on host ""+ citrixResourceBase.getHost().getIp(),e);"
"<line3>    logger.info(""      Iniciando Teste de RegressÃ£o de Layout"");<line3>    logger.info(""========================================================"");"
"<line14>          logger.error(""TTransportException writing to internal frame buffer"", e);<line16>          logger.error(""Exception writing to internal frame buffer"", e);<line29>          logger.error(""TTransportException inside handler"", e);<line32>          logger.error(""TApplicationException inside handler"", e);<line35>          logger.error(""Exception inside handler"", e);<line43>          logger.error(""Exception writing to internal frame buffer"", ex);"
"<line6>        log.debug(""Fill "" + filler.fillerId + "": Creating "" + query.getLanguage() + "" query executer"");"
"<line2>    LOG.info(""Finished rollback on [{0}/{1}]"", _shard, _table);"
"<line7>        logger.trace(""JGroupsMessenger received {} headers: {}"", jgmsg, jgmsg.getHeaders());<line16>          logger.info(""Failed sending Pong response to "" + jgmsg.getSrc());<line35>          logger.trace(""JGroupsMessenger dispatching {} from {}"", msg, msg.getSender());"
"<line1>    logger.debug(""Attempting to build the Matcher Array from input query"");<line3>      logger.info(""Added query to matcher array: "" + query);"
"<line3>      logger.info(""stop() started"");<line8>      logger.info(""stop() completed"");"
"<line6>      LOG.debug(""While reading schema on connector {}"", connector, e);"
"<line5>    LOGGER.debug(""trying to execute:  "" + pb.command());<line14>      LOGGER.debug(""Process started. PID = "" + pid);"
"<line3>    LOGGER.debug(""clientNonce: ""+ ArrayConverter.bytesToHexString(clientEsniInne.getClientNonce().getValue()));"
<line6>      LOG.error(e.getLocalizedMessage());
"<line21>          logger.debug(""Issuing consumed until request to persistence manager for topic: {} with min-id: {}"",topic.toStringUtf8(),minConsumedMessage);"
"<line13>      logger.error(""Error while deleting attributeId: "" + id, e);"
"<line3>      LOG.info(""Skipping index range cleanup because the Elasticsearch cluster is unreachable or""+ "" unhealthy"");<line14>      LOG.info(""Removing index range information for unavailable indices: {}"", removedIndices);"
"<line3>    log.info(""Found: "" + context);"
"<line5>      logger.info(""Resolved env. variable DATACLEANER_HOME: {}"", path);<line9>        logger.info(""Resolved system property DATACLEANER_HOME: {}"", path, candidate);"
"<line31>      logger.debug(""Took {} Âµs to get stats from {}.{}"",timeGetStats.elapsed(TimeUnit.NANOSECONDS) / 1000,table.getDbName(),table.getTableName());"
"<line20>    LOG.debug(""Creating Process for worker ID {}"", workerId);<line30>          LOG.info(""Still waiting for startup of environment '{}' for worker id {}"",processPayload.getCommand(),workerId);"
"<line10>      LOGGER.error(""Could not invoke configuration listeners."", e);"
"<line13>      log.error(""Shutdown failed: {}"", e.getMessage(), e);"
"<line16>        LOG.debug(""bridge-domain was deleted from interface for endpoint {}"", rEp);<line17>        LOG.warn(""bridge-domain was not deleted from interface for endpoint {}"", rEp, e);<line19>      LOG.debug(""Forwarding is not removed - Location of renderer endpoint does not contain ""+ ""external-node therefore VPP renderer assumes that interface for endpoint is not ""+ ""assigned to bridge-domain representing external-node. {}"",rEp);"
"<line3>    LOGGER.info(""get paragraph list from remote interpreter noteId: {}, user = {}"", noteId, user);<line12>      LOGGER.error(""user or noteId is null!"");"
"<line13>          logger.debug(""Controller script found for page "" + pageUrl + "" at "" + scriptUrl);<line16>        logger.debug(""No controller script for page "" + pageUrl + "" at "" + scriptUrl);<line18>      logger.error(""Error while trying to retrieve controller script at "" + scriptUrl, e);"
"<line4>      log.warn(""Invalid HTTP header specified in ""+ ACCESS_CONTROL_REQUEST_HEADERS+ "" '""+ headerName+ ""'. ""+ ""It will be ignored and not attached to the ""+ ACCESS_CONTROL_ALLOW_HEADERS+ "" response header"");"
"<line10>      logger.trace(""WorkplaceSearchClient class not found for version {} in the classpath. Skipping..."",version);<line16>    logger.trace(""Found [{}] class as the elasticsearch client implementation."", clazz.getName());"
"<line1>    log.debug(""publishPinArray {}"", data);<line7>        log.error(""not a valid pin address {}"", address);"
"<line1>    LOG.info(""Disconnecting from broker {} on user {}"", config.url(), config.username());"
"<line11>      logger.error(""Study ID is null"");"
<line30>      LOG.error(e.getMessage(), e);
"<line2>      LOGGER.info(""Ignore, already stop or reusing exist instance, displayName={}."",deployDefinition.getDisplayName());<line7>    LOGGER.info(""stop complete, displayName={}."", deployDefinition.getDisplayName());"
<line16>      LOGGER.warn(PROBLEM_GETTING_COMMITTEE_PROPOSAL_FOR_ID_S_FROM_DATA_RIKSDAGEN_SE, id);
"<line5>        log.debug(""found Slave DataSource name="" + slaveName);"
<line8>      log.error(exception, exception);
"<line1>    logger.trace(""{}: ZclCluster.addAttributeListener adding {}"",zigbeeEndpoint.getEndpointAddress(),listener);"
"<line11>        logger.warn(""Unable to copy relationship property {} of relationship {} to {}: {}"",new Object[] {sourcePropertyKey, rel.getUuid(), destPropertyKey, fex.getMessage()});"
"<line6>        LOGGER.debug(""error when trying to instal artifacts"", e);"
"<line10>        log.info(""Logging out participant via SOAP: "" + participant);<line17>        log.warn(""Logging out the participant "" + participant + "" via SOAP failed"", e);"
"<line2>    logger.debug(""bridgeStatusChanged {} for thing {}"", bridgeStatusInfo, getThing().getUID());"
"<line9>      log.debug(""proceedElement '"" + obj.getClass() + ""'...."");<line12>        log.info(""proceedElement: field '""+ field.getDeclaringClass().getName()+ "".""+ field.getName()+ ""'."");<line34>      log.warn(""Field '"" + key + ""' not found."");"
"<line4>      logger.error(""No 'target' defined for index {} on {}.{}. This should not happen and should be reported""+ "" for investigation. That index will not be visible in Stargate schema view"",indexName(index),keyspaceName,tableName);<line11>      logger.error(""Could not find secondary index target column {} in the columns of table {}.{} ""+ ""({}). This should not happen and should be reported for investigation. That index ""+ ""will not be visible in Stargate schema view."",targetColumn,keyspaceName,tableName,baseTableColumns);"
"<line17>    LOG.info(""feeds before rebind are: "" + origFeeds);<line21>    LOG.info(""tasks before disabling HA, "" + tasksBefore.size() + "": "" + tasksBefore);<line36>                LOG.info(""tasks after disabling HA, "" + tasksAfter.size() + "": "" + tasksAfter);<line48>    LOG.info(""feeds after rebind are: "" + newFeeds);"
"<line1>    logger.info(LogBanner.start(""Deprecating entity""));<line1>    logger.info(""[Deprecate Entity] Deprecating "" + info.getEntityToDeprecate());<line13>    logger.info(LogBanner.end());"
<line22>      log.error(systemException, systemException);
"<line6>        log.info(""Delete existing log (lastIndex '{}') and replace with received snapshot (index '{}')"",lastIndex,index);"
"<line2>    logger.info(""[testTokenCleanup] vm1 tests recursion"");"
"<line5>      logger.error(""event=failed_to_get_number_of_not_visible_messages queue_url="" + queueUrl);"
"<line26>          LOG.error(""Unable to encode parameter name or value: "" + paramName + ""="" + paramValue, ex);"
"<line38>          LOG.error(""failed to build BigtableAsyncAdmin"", e);"
"<line3>    LOG.debug(""Runtime context is prepared: {}"", RuntimeContext.get());"
"<line21>      log.error(""{}"", e.getMessage(), e);"
"<line2>    LOGGER.debug(""Stopping lock service"");"
"<line3>    log.info(""computeLocalFileChecksum()"");<line6>    log.info(""localFile:{}"", localFile);<line18>    log.info(""using checksum algorithm:{}"", checksumEncoding);<line26>      log.error(""cannot find file for computing local checksum"", e);"
"<line26>        logger.info(String.format(""Excpetion  in bucket index creation : %s"", ignor.getLocalizedMessage()),ignor);"
"<line6>      LOG.debug(""Queue recently deleted, will retry in 30 seconds."");<line10>        LOG.warn(""failed to retry queue connection."", e);<line12>      LOG.warn(""Could not connect to queue in amazon."", e);"
"<line2>      log.error(""unrecoverable error in task feeder or one of its mapper threads. immediately halting""+ "" jvm"",t);"
<line12>        logger.warn(JSONObjectUtil.toJsonString(statistic));
"<line3>    LOG.info(""Executing operation receiveMeshInterfaceObjects"");"
"<line3>    logger.debug(""List to delete: "" + uuid);"
"<line12>                    logger.warn(""Exception at getGeoDetails"", e);"
"<line18>      logger.info(e.getKey() + "" = "" + e.getValue());"
"<line6>      log.error(""Exception thrown"", e);"
"<line16>        LOG.debug(""AtlasPAMAuthenticationProvider{groupsFromUGI= ""+ groupsFromUGI+ '\''+ "", options=""+ options+ '}');<line18>      LOG.error(""Exception while setLdapProperties"", e);"
"<line2>    logger.warn(""Reconfiguring Clamav current host ..."");<line5>        logger.warn(""Clamav current host reconfigured to "" + clamdHost);<line7>        logger.error(""Clamav reconfiguration failed ! "");"
"<line7>            LOG.debug(""{} cookie has been found and is being processed"", cookieName);"
"<line4>      LOG.error(""Unable to read {} from file {}: {}"",key,NhincConstants.MESSAGES_PROPERTY_FILE,ex.getLocalizedMessage(),ex);"
"<line3>      logger.debug(""File len: {}"", d.length);<line5>      logger.debug(""Exception occured during data conversion: {}"", e.getMessage());"
"<line5>        LOG.debug(""Trying to load store from file"");<line7>        LOG.debug(""Trying to load store from classpath"");"
"<line2>    LOGGER.debug(""Parsing HelloRetryRequestMessage"");"
"<line53>          LOG.warn(message);<line63>          LOG.error(""Unexpected error"", e);<line64>          LOG.error(getMessagesFromException(e));"
"<line9>      log.error(""Failed to start CloudStack"", e);"
"<line1>    log.info(""Truncating file [{0}] to {1}"", filepath, length);"
"<line3>      this.logger.error(""Invalid trace: {}"", invalidTrace);"
"<line6>        log.debug(""Cannot process event: {} as its class type: {} is not assignable with: {}"",new Object[] {event, event.getClass().getName(), eventClass.getName()});"
"<line5>        log.error(ee.getCause(), ""Error in %s"", this);<line6>        log.error(ce, ""Future for %s has already been cancelled"", this);"
"<line3>      LOG.debug(""Switch idle"");"
"<line2>    CorsAwareNegotiateSecurityFilter.LOGGER.info(""[waffle.servlet.CorsAwareNegotiateSecurityFilter] Starting"");<line3>    CorsAwareNegotiateSecurityFilter.LOGGER.info(""[waffle.servlet.CorsAwareNegotiateSecurityFilter] Started"");"
"<line3>      LOG.warn(""Configured connection limit {} is too high: Recommended is maximum {} (based on {})"",configuredLimit,recommendedLimit,strategy.getResourcesDescription());<line4>      LOG.debug(""Configured connection limit: {}"", configuredLimit);"
"<line2>    logger.info(""Shutdown of management agent will commence in: ""+ MANAGEMENT_AGENT_SHUTDOWN_INTERNAL_SECONDS+ "" seconds"");<line6>    logger.info(""Initiating shutdown of management agents"");<line7>      logger.info(""Shutting down agent: "" + getAgentDescription(agent));"
"<line7>        logger.warn(String.format(""something goes wrong when getting data from server:%s"",currentConnector != null ? currentConnector.getAddress() : ""null""),t);<line9>        logger.info(""restart the connector for next round retry."");"
"<line6>      LOG.trace(""Expanding envelope {} for offering {} to include {}"",offeringEnvelope,offering,envelope);"
"<line3>      logger.debug(""Failed to send a 413 Request Entity Too Large."", future.cause());"
"<line3>      log.info(""Found MagicAnnotation on field "" + field + "" of class "" + bean.getClass());"
<line10>      log.error(portalException, portalException);
<line18>      log.error(e.getMessage());
"<line9>        LOGGER.info(""Starting to receive messages."");<line13>            LOGGER.info(""Received empty message, count = "" + count);<line14>            LOGGER.info(""Received message, id = "" + message.getJMSMessageID());<line18>          LOGGER.warn(""Not reading more messages, already read "" + count + "" messages."");<line20>        LOGGER.info(""Stopping to receive messages."");<line23>      LOGGER.info(""caught exception: "" + ExceptionUtils.getStackTrace(e));"
<line15>      LOGGER.info(new String(IOUtils.toByteArray(p.getInputStream())));
"<line1>    LOGGER.info(""attempt to migrate navigation"");"
"<line7>      log.error(""Failed to delete file: {}"", file, e);"
"<line1>    logger.info(""Bundle {} is starting!"", this.getClass().getSimpleName());<line8>    logger.info(""Bundle {} has started!"", this.getClass().getSimpleName());"
"<line15>      LOG.error(""error during save patient: {}"", e.getLocalizedMessage(), e);"
"<line11>        logger.info(""Missing status on Status Object. "");<line15>        logger.info(""Status is not 'Component Done OK' or 'Component Done Error'"");<line26>      logger.debug(event);<line28>      logger.error(""Error in ASDCStatusCallback {}"", e.getMessage(), e);<line28>      logger.debug(""Error in ASDCStatusCallback {}"", e.getMessage());"
"<line3>    logger.trace(""bridgeDirectCommunicate(BCP: {},{}authenticated) called."",communication.name(),useAuthentication ? """" : ""un"");"
"<line16>    logger.info(""generated "" + target);"
"<line9>      LOG.trace(""New classifier created: {}\n{}"", classifierEntry.getKey(), classifierEntry.getValue());"
"<line3>      log.debug(""Loading properties files from '{}' with loader '{}'"", resourceStream, loader);<line21>      log.warn(""Unable to find resource "" + resourceStream, e);"
"<line3>      LOGGER.debug(""clients size: {}"", clients.size());"
<line2>    LOGGER.info(JSON.toJSONString(jobLogPo));
"<line28>      logger.warn(""Unable to create image reference for container {} due to {}"", id, e.getMessage(), e);"
"<line3>    logger.warn(""Invoking unimplemented method build"");"
"<line6>      log.warn(""Warning"", e);"
"<line16>    LOGGER.info(""FindExperiments Response : "" + response.getExperimentsList());<line39>    LOGGER.info(""FindExperiments by attributes test start................................"");"
"<line2>    logger.debug(""{} stopped"", this);"
"<line1>    LOGGER.info(""Unblocked messages to "" + address);"
"<line11>          log.warn(""Unable to parse date "" + value, exception);"
"<line4>      log.error(""Cannot get home path"");"
"<line19>            Log.info(""Successfully executed flush operation ':"" + flushOp + ""'"");"
"<line3>    logger.debug(""Absorbance (Cys Reduced): {}"", PeptideProperties.getAbsorbance(sequence, true));<line3>    logger.debug(""Absorbance (Cys Not Reduced): {}"", PeptideProperties.getAbsorbance(sequence, false));<line3>    logger.debug(""Extinction Coefficient (Cys Reduced): {}"",PeptideProperties.getExtinctionCoefficient(sequence, true));<line3>    logger.debug(""Extinction Coefficient (Cys Not Reduced): {}"",PeptideProperties.getExtinctionCoefficient(sequence, false));<line3>    logger.debug(""Instability Index: {}"", PeptideProperties.getInstabilityIndex(sequence));<line3>    logger.debug(""Apliphatic Index: {}"", PeptideProperties.getApliphaticIndex(sequence));<line3>    logger.debug(""Average Hydropathy Value: {}"", PeptideProperties.getAvgHydropathy(sequence));<line3>    logger.debug(""Isoelectric Point: {}"", PeptideProperties.getIsoelectricPoint(sequence));<line3>    logger.debug(""Net Charge at pH 7: {}"", PeptideProperties.getNetCharge(sequence));"
"<line44>            getSession().error(getString(""PageAccountActivation.account.activation.failed""));<line74>      LOGGER.error(""No accounts to validate for user {}"", userModel.getObject());<line74>      getSession().warn(getString(""PageAccountActivation.nothing.to.activate""));"
"<line22>      logger.warn(""Unable to extract the version of Restcomm Load Balancer currently running"", e);"
<line7>      LOGGER.error(t.getLocalizedMessage(), t);
"<line3>      logger.trace(""No value found for property {}, returning default {}"",configurationKey.getName(),configurationKey.getDefaultValue());<line5>      logger.trace(""Loaded property {} with value {}"",configurationKey.getName(),configurationKey.getDefaultValue());"
"<line2>    log.info(""Process PipelinesIndexedMessage - {}"", message);<line13>    log.info(""The message has been sent - {}"", outputMessage);"
"<line6>      LOG.debug(""Workflow {} does not exist possibly due to stale workflow cache"",state.workflowInstance().workflowId().toKey());"
"<line12>          LOG.debug(""Found an existing revision number:{0}"", revisionNo);<line14>        LOG.debug(""Created a new revision number: 1"");<line22>      LOG.error(null, ex);"
"<line7>      LOGGER.warn(""Caught exception while closing result set '"" + resultSet + ""', ignoring."", e);"
"<line3>    LOG.debug(""Starting testKerbFileAccess() ..."");"
"<line2>    LOG.info(""\n===RunDaoTest.get===\n"");"
<line19>    LOG.error(message);
"<line2>    logger.trace(""refining "" + concept);"
"<line7>        LOG.debug(""Handling notification {} with repair handler {}"",notification,repairStatusHandlers.get(repairNo));<line12>      LOG.error(""Error while processing JMX notification"", e);"
"<line4>    LOGGER.trace(""QUERY getFeature(identifier): {}"", HibernateHelper.getSqlString(criteria));"
"<line2>    log.info(""Publish {}: Prepare target store for uploaded datasets"", jobId);"
"<line7>              LOGGER.info(""Processing message {}"", exchange.getIn().getBody());"
"<line7>      logger.info(""This contact has not been found by obm-sync"", e);"
"<line3>      logger.trace(LogMarker.SERIALIZER_VERBOSE, ""Writing Long {}"", value);"
"<line8>    log.debug(""Creating server in port: "" + port);"
"<line12>      logger.debug(""Adding AWS {} mapping to database: {} points to {}, object version {}"",getStoreType(),storagePath,objectName,objectVersion);"
<line12>        log.debug(sb.toString());
"<line6>        log.info(""Initializing Certificate management repository database schema"");<line8>        log.info(""Certificate management repository database already exists. Not creating a new""+ "" database."");<line14>      log.debug(""Certificate management metadata repository schema has been successfully initialized"");"
"<line4>    log.debug(""Authenticating user '{}' through LDAP"", principal);"
"<line3>    LOG.info(""Fixed: "" + orcid + "" "" + correctedEmailHash);"
<line9>          log.debug(noSuchFileShortcutException, noSuchFileShortcutException);
"<line6>      log.debug(""Missing requested artifact. [artifact=({}), contract=({}), ""+ ""issuer=({}), messageId=({})]"",requestedArtifact,transferContract,issuerConnector,messageId);"
"<line1>    log.debug(""Failure for batch with transaction id "" + transactionId + "" for "" + name);<line4>          log.trace(""Failed message "" + msg.getJMSMessageID());<line5>          log.warn(""Could not identify failed message "", e);<line8>      log.warn(""Failed batch has no messages with transaction id "" + transactionId);"
<line10>        LOGGER.error(t.toString(), t);
"<line22>      logger.info(""Imported ontology document IRI {} is malformed."", ontologyIRI);<line23>      logger.info(""Imported ontology document {} does not exist on the Web (File Not Found)."", ontologyIRI);<line25>      logger.info(""Imported ontology document {} could not be retrieved. Cannot connect to {} (Unknown""+ "" Host)."",ontologyIRI,host);<line26>      logger.info(""Imported ontology document {} could not be retrieved: {}"", ontologyIRI, e.getMessage());"
"<line12>      log.debug(""{} was redirected to {}"", redirectTarget, destinationUrl);"
"<line9>      LOG.fatal(""Error starting TajoMaster"", t);"
<line7>      log.error(exception, exception);
"<line6>      LOGGER.debug(""Received a [{}] operation, but no mapped metacardIds were available for product [{}]."",catalogOperation,referenceKey);"
"<line4>      LOG.info(""Stopping Vertx {}"", vertx);"
"<line11>    log.debug(""User "" + personDao.getLoggedPerson().getEmail() + "" retrieved list of education levels."");"
"<line3>        LOGGER.info(""Using Fedora Client with valid user, valid pass"");<line8>        LOGGER.info(""Using Fedora Client with valid user, bogus pass"");<line14>      LOGGER.info(""Using Fedora Client with bogus user"");"
"<line3>      logger.error(""Usage: <node>"");"
"<line14>    logger.info(""Configuration updated"");"
"<line18>    logger.debug(""Start fetching pom!"");"
"<line9>      LOGGER.debug(""did store last authenticated timestamp for user [{}]"", user.getNickname());"
"<line29>      logger.error(""Error creating url"", e);"
"<line11>        log.debug(""Discarding service-provider={}, consumer={}"",entry.getServiceProperties(),serviceConsumer);"
"<line3>      LOG.info(""Not reporting data points to graphite."");<line5>    LOG.info(""Reporting data points to graphite server {}:{} every {} seconds with prefix '{}' and""+ "" predicates '{}'."",graphiteConfiguration.getHostname(),graphiteConfiguration.getPort(),graphiteConfiguration.getPeriodSeconds(),graphiteConfiguration.getPrefix(),JavaUtils.COMMA_JOINER.join(graphiteConfiguration.getPredicates()));"
"<line2>    LOG.debug(""fetching all pages for course "" + courseId);"
"<line3>    logger.trace("">> listing node with ids(%s)"", ids);<line4>    logger.trace(""<< list(%d)"", set.size());"
<line8>      logger.error(request.getRequestContext(), ex.getMessage(), ex);
"<line17>              log.debug(""Timed out waiting for rebalancing status from coordinator, trying again"");"
"<line11>      logger.error(""Unable to get user by its name"", fex);"
"<line9>        LOG.debug(""Preemptively sending default basic credentials"");<line15>        LOG.error(e.getMessage(), e);"
"<line5>        logger.debug(""The name of the queue region is {} and the size is {}. keyset size is {}"",prQ.getName(),prQ.size(),prQ.keys().size());"
"<line19>      log.debug(""Change detected: {}"", entry);"
<line7>        log.warn(exception, exception);
"<line2>    LOG.error(""Only self managed allowed!"", exception);"
"<line12>      logger.debug(""Unable to resolve port [{}] to an identifier"", portName);<line13>      logger.debug(""Resolved port [{}] to identifier [{}]"", portName, portId);"
"<line4>    LOG.info(""RCT start to generate redis rdb file.{}:{}"", host, port);<line17>    LOG.info(""RCT generate redis rdb file success. cost time:{} ms"",(System.currentTimeMillis() - currentTime));"
"<line3>      logger.error(""CertificateSigningRequest cannot be null"");<line6>      logger.error(""CertificateSigningRequest cannot be empty"");<line9>      logger.error(""CertificateSigningRequest requester common name cannot be empty"");<line13>      logger.error(""CaProperties cannot be empty"");<line42>        logger.error(msg);"
"<line13>    LOG.info(""serial took: {}ms"", serial);<line13>    LOG.info(""parallel took: {}ms"", parallel);"
"<line2>    log.info(""Closing MiniTrogdorCluster."");"
"<line5>      log.info(""adjusting paths in entry to reflect linked collection info"");"
"<line6>      LOGGER.error(""Failed to transform JSON: "" + e, e);"
"<line7>      log.trace(""testFolderishCollection2():\n""+ ""Create a folder with the Collection facet (\""collectionFolder\"") inside a folder""+ "" (\""folder1\"");\n""+ ""Add \""folder1\"" to the \""collectionFolder\"" collection;\n""+ ""Register \""folder1\"" as a synchronization root;\n""+ ""Create a collection \""collectionSyncRoot\"" and register it as a synchronization""+ "" root;\n""+ ""Create a document \""testDoc\"" and add it to both collections \""collectionFolder\""""+ "" and \""collectionSyncRoot\"".\n"");"
"<line8>    log.trace(""Number of results of polling: "" + pollResults.size());<line30>            log.trace(""Connection error"", r.connectError);<line32>            log.trace(""Polling error"", r.pollError);"
"<line2>    LOGGER.info(""FindExperimentRuns Negative test start................................"");<line20>      LOGGER.warn(""Error Code : "" + status.getCode() + "" Description : "" + status.getDescription());<line22>    LOGGER.info(""FindExperimentRuns Negative test stop................................"");"
"<line9>      log.error(""Failed to discover installed apps: Could not get app folder path, external directory not""+ "" set"");<line13>      log.info(""Old apps folder does not exist, stopping discovery"");<line16>      log.error(""Failed to discover installed apps: Path is not a directory '"" + path + ""'"");<line19>        log.error(""Failed to discover installed apps: Could not list contents of directory '""+ path+ ""'"");<line22>            log.warn(""Failed to discover app '""+ folder.getName()+ ""': Path is not a directory '""+ folder.getPath()+ ""'"");<line25>              log.warn(""Failed to discover app '""+ folder.getName()+ ""': Missing 'manifest.webapp' in app directory"");<line32>                log.error(ex.getLocalizedMessage(), ex);<line46>          log.info(""Discovered app '"" + app.getName() + ""' from local storage "");<line48>      log.info(""No apps found during local discovery."");"
"<line12>    log.debug(""User info={}"", json);"
"<line39>            logger.debug(""Begin write to HTML"");<line40>            logger.debug(""End write to HTML"");<line50>            logger.error(""ExecutionException occurred while getting the result of the HTML rendering"", e);<line53>            logger.warn(""Unexpected interrupt in done() method!"");"
"<line40>      logger.error(""NotificationController - viewNotificationList() - ERROR "", e);"
<line2>    logger.debug(Messages.GETTING_INSTANCES_OF_APPLICATION_0, app.getName());
"<line7>      LOGGER.debug(""Registering module state with id ["" + id + ""]"");"
"<line3>      LOG.trace(""Channel open: {}"", ctx.channel());"
"<line13>          LOGGER.error(""Could not encrypt private key: "" + e, e);<line17>        LOGGER.error(""Could not decrypt private key: "" + e, e);<line21>          LOGGER.debug(""Could not decrypt private key: "" + e, e);"
"<line8>      ActivityHelper.logger.debug(e, e);<line8>      ActivityHelper.logger.error(""getVariable: "" + e);"
"<line7>        logger.debug(""Online check completed with success: {} - status code: {}"",content,contentResponse.getStatus());<line9>        logger.debug(""Online check failed with status code: {}"", contentResponse.getStatus());<line12>      logger.debug(""Online check failed because of {}!"", e.getMessage());"
<line19>        LOG.error(err);<line62>      LOG.error(error, e);
"<line10>      logger.info(""Trying to cancel job {} with savepoint, but no savepoint directory configured."", jobId);"
"<line52>    LOG.debug(""TaskManager start command: "" + startCommand);"
"<line2>    Log.debug(""Test"");<line20>        Log.debug(""\t""+ veff+ ""\n\t\ttranscript: ""+ veff.getTranscriptId()+ ""\n\t\tHgvs (DNA): ""+ veff.getHgvsDna());"
"<line8>          logger.info(format(""Job %s is already running or it has been stop"", job.getName()));<line13>      logger.debug(format(""Starting execution of job %s"", job.getName()));<line26>        logger.debug(format(""Job %s execution failed"", job.getName()), failure);<line38>          logger.debug(format(""Job close %s failure"", job.getName()), exception);"
"<line1>    log.debug(""persisting StgMsCmState instance"");<line3>      log.debug(""persist successful"");<line4>      log.error(""persist failed"", re);"
<line1>    logger.debug(message);
"<line6>      logger.error(""unable to translate "" + netexSourceType + "" as PTNetworkSourceType"");"
"<line26>                        Log.error(""Error while retrieving org unit ""+ userJS.getMainOrgUnit()+ "" for user ""+ userJS.getId());"
"<line4>      log.error(""Got IOException while reading delimited file: "" + lineProducer);"
"<line20>        Log.debug(""Adds a monitored point '""+ point.getLabel()+ ""' to container with id #""+ containerId+ ""."");"
"<line17>      LOGGER.error(""Error in getAppsBySeverity from ES"", e);"
"<line16>        LOG.warn(""Incorrect {} configuration - missing {} selector"", pid, KEY_BUNDLE_SN);"
"<line2>    log.error(""getModulesConfigurationPropertiesFallback: "" + serviceName);"
"<line4>          log.debug(""Setting autocommit to ""+ desiredAutoCommit+ "" on JDBC Connection [""+ connection+ ""]"");"
"<line36>          LOG.error(""create connection Thread Interrupted, url: "" + jdbcUrl, e);<line45>        LOG.error(""create connection SQLException, url: ""+ jdbcUrl+ "", errorCode ""+ e.getErrorCode()+ "", state ""+ e.getSQLState(),e);<line66>        LOG.error(""create connection RuntimeException"", e);<line69>        LOG.error(""create connection Error"", e);<line78>        LOG.info(""put physical connection to pool failed."");"
"<line18>      logger.error(""Error while extracting Allowed Nested Types"", t);"
"<line15>        log.trace(has);<line52>      log.trace(""\n"" + sb.toString());"
"<line28>          LOG.warn(""Server: "" + server);<line28>          LOG.warn(""Error reading byte #"" + byteNum, e);"
"<line3>      log.trace(""already closed"");<line5>    log.debug(""closing {}"", this);"
"<line18>        LOGGER.debug(""Unable to build query. Unknown sort order of [{}]."",sortOrder == null ? null : sortOrder.identifier());"
<line14>      LOGGER.error(e, e);
"<line28>      logger.info(""SpringSocialSecurity sign in details not found in session"");"
"<line2>    log.debug(""Starting up test server"");"
"<line17>      logger.debug(""More than one element builder plugin claims responsibilty for ""+ flavor+ "": ""+ buf.toString());"
"<line2>    log.info(""-------------------------Started autoscaling policy test case-------------------------"");<line79>    log.info(""-------------------------Ended autoscaling policy test case---------------------------"");"
"<line15>      logger.debug(""Playback started"");<line18>      logger.warn(""Cannot start"");"
"<line41>    log.info(""PageMemory tracker started, ""+ U.readableSize(maxMemorySize, false)+ "" offheap memory allocated."");"
"<line2>    LOGGER.info(""MQTT Message received {}"", new String(message.getPayload()));"
"<line8>      LOG.debug(""Received direct ack for message [{}], associated with null tuple"", msgId);<line12>      LOG.debug(""Received ack for message [{}], associated with tuple emitted for a ConsumerRecord that""+ "" came from a topic-partition that this consumer group instance is no longer""+ "" tracking due to rebalance/partition reassignment. No action taken."",msgId);"
"<line5>        LOG.error(""Failed to transform String to URI: {} "", string);"
"<line2>    log.debug("""");"
<line20>              LOG.error(STD_ERR_MSG, e);
"<line2>    log.debug(""Monitoring condition with specified checkInterval of {}, timeout of {}, timeUnit {}"",checkInterval,timeout,timeUnit);"
"<line7>      logger.error(""Unable to move file '"" + file.getName() + ""' to directory: "" + destinationDir, ex);"
"<line2>    LOGGER.debug(""HeartbeatMode: "" + ArrayConverter.bytesToHexString(msg.getHeartbeatMode().getValue()));"
"<line2>    LOGGER.debug(""Get levels with criteria={}"", criteria);"
"<line16>        LOGGER.warn(""Unable to add file '{}' because cannot find corresponding group  with @USE='{}'. ""+ ""Ignore file and continue."",ref.toFileId(),use);"
"<line22>              LOG.error(""While updating console layout for role {}"", wrapper.getKey(), e);"
"<line5>      log.debug(""file descriptor from new file create: {}"", fileDescriptor);<line11>      log.error(msg, e);"
"<line11>        LOGGER.error(""Failed to parse state interval: "" + e, e);"
"<line15>        log.error(""Error cloning Managed block disk '{}': {}"", managedBlockDisk.getDiskAlias());"
"<line9>      logger.warn(""Unable to collect a list of wiki objects components: %s"", e);"
"<line6>      logger.error(""fail to unMarshal json {}"", e.getMessage());"
"<line16>          LOGGER.info(""Removing Cruise Control to the classic Kafka."");<line21>    LOGGER.info(""Verifying that in {} is not present in the Kafka cluster"", Constants.CRUISE_CONTROL_NAME);<line29>    LOGGER.info(""Verifying that {} pod is not present"", clusterName + ""-cruise-control-"");<line30>    LOGGER.info(""Verifying that in Kafka config map there is no configuration to cruise control metric""+ "" reporter"");<line37>    LOGGER.info(""Cruise Control topics will not be deleted and will stay in the Kafka cluster"");<line44>          LOGGER.info(""Adding Cruise Control to the classic Kafka."");<line49>    LOGGER.info(""Verifying that in Kafka config map there is configuration to cruise control metric""+ "" reporter"");<line52>    LOGGER.info(""Verifying that {} topics are created after CC is instantiated."",Constants.CRUISE_CONTROL_NAME);"
"<line5>    logger.debug(""About to marshal case file data (name = {}) for case with id '{}' {}"",name,caseId,caseFileData);"
"<line21>                    LOG.warn(""Unable to fetch system jobs from node {}:"", entry.getKey(), e);"
"<line9>      LOG.debug(""Reading checkpoint for {} to {}"", entry.getName(), checkpointPath);"
"<line4>      log.trace(""deleteIfExists({}): {}"", path, r);"
"<line5>        log.debug(""Executing query {}."", cqlQuery);<line30>      log.error(""Error while executing native CQL query Caused by {}."", e);"
<line5>      log.warn(missingMessage(key));
"<line7>      LOGGER.warn(""Could not resolve Inbox mailbox"", e);"
"<line39>        LOG.debug(""No pod for: "" + podName + "" port: "" + podPort + "" path: "" + podPath);<line45>        LOG.debug(""Invoking: ""+ url+ "" from pod: ""+ podName+ "" port: ""+ podPort+ "" path: ""+ podPath);"
"<line4>      logger.info(""Starting monitoring thread with reference date {}"", lastSync);<line18>              logger.info(""changes detected : {}"", changedCollections.toString());<line26>          logger.error(e1.getMessage(), e1);<line27>          logger.error(e.getMessage(), e);<line30>      logger.error(e1.getMessage(), e1);<line31>      logger.error(e.getMessage(), e);"
"<line3>    LOG.debug(""Begin retrieveDocument"");<line38>    LOG.debug(""End retrieveDocument"");"
"<line2>    LOGGER.error(""Exception:"", exception);"
"<line3>    logger.debug(""Committing XA TX. {}, One Face: {}"", xid, b);"
"<line14>          LOG.debug(""Free space for block expansion: freeing {} bytes on {}. "",options.getSize(),options.getLocation());<line23>            LOG.debug(""Allocation after freeing space for block expansion, {}"",dirView == null? ""no available dir."": ""available bytes in dir: "" + dirView.getAvailableBytes());<line25>            LOG.error(""Target tier: {} has no evictable space to store {} bytes for session: {}"",options.getLocation(),options.getSize(),sessionId);<line28>          LOG.warn(""Target tier: {} has no available space to store {} bytes for session: {}"",options.getLocation(),options.getSize(),sessionId);<line37>        LOG.debug(""Allocate to anyTier for {} bytes on {}"", options.getSize(), options.getLocation());<line45>          LOG.debug(""Allocation on anyTier failed. Free space for {} bytes on anyTier"", toFreeBytes);<line54>            LOG.debug(""Allocation after freeing space for block creation, {} "",dirView == null? ""no available dir."": ""available bytes in dir: "" + dirView.getAvailableBytes());"
"<line8>    log.info(methodName + "" Sending events"");<line12>    log.info(methodName + "" Done sending events"");<line13>    log.info(methodName + "" delta="" + (endTime - startTime));"
"<line1>    logger.info(""test-auth: Auth accept OK"");"
"<line10>    LOGGER.debug(String.format(""Computing minimal bounds geometry on layer '%s' (tree '%s') for change %s...%s "",tileLayerName, layerTreeName, oldCommit, newCommit));<line22>        LOGGER.debug(String.format(""Feature tree '%s' not affected by change %s...%s (took %s)"",layerTreeName, oldCommit, newCommit, sw));<line25>        LOGGER.debug(String.format(""Minimal bounds on layer '%s' computed in %s: %s"",tileLayerName, sw, formattedWKT(minimalBounds)));<line28>      LOGGER.error(String.format(""Error computing minimal bounds for %s...%s on layer '%s' after %s"",oldCommit, newCommit, tileLayerName, sw));<line45>      LOGGER.debug(""Reprojecting geometry mask to gridset {}"", gridsetId);<line47>        LOGGER.debug(""geometry mask reprojected to gridset {}: {}"",gridsetId,formattedWKT(geomInGridsetCrs));"
<line11>      LOGGER.warn(ex);
"<line11>    logger.info(""query list queue return result:{}"", mvcResult.getResponse().getContentAsString());"
"<line3>      LOGGER.debug(""Nothing to commit. Transaction branch is null"");"
"<line9>        LOG.warn(""Some operators are behind for more than {} windows! Trimming the end window stats map"",this.vars.maxWindowsBehindForStats);<line10>          LOG.debug(""Removing incomplete end window stats for window id {}. Collected operator set: {}.""+ "" Complete set: {}"",endWindowStatsOperatorMap.firstKey(),endWindowStatsOperatorMap.get(endWindowStatsOperatorMap.firstKey()).keySet(),allOperators.keySet());<line23>              LOG.debug(""Disregarding stale end window stats for window {}"", windowId);<line32>          LOG.debug(""Stats for non-existent operators detected. Disregarding end window stats for window""+ "" {}"",windowId);"
"<line29>      LOGGER.debug(format(""Injecting read/write/serial consistency levels %s/%s/%s into entity meta of %s"",readConsistencyLevel.name(),writeConsistencyLevel.name(),serialConsistencyLevel.name(),entityClass.getCanonicalName()));"
"<line12>      LOG.info(""No service configuration given and no ConfigurationProviders set."");"
"<line2>    log.info(t + """");"
"<line5>      log.error(""RemoteException"", e);"
"<line18>      LOG.error(""Error stopping plugin \"""" + getName() + ""\"": "" + e.getLocalizedMessage(), e);"
"<line12>    LOG.info(""Running the pipeline"");<line13>    LOG.info(""Pipeline has been finished!"");"
"<line2>      log.debug(""kurentoTest."" + jsFunction + ""('"" + peerConnectionId + ""');"");<line5>      log.warn(""Client does not support RTC statistics (function kurentoTest.{}() not defined)"");"
"<line9>        LOG.info(""Copying jarFile = "" + jarPath);"
"<line1>    logger.debug(""Product service returning list of products"");"
"<line3>        log.trace(""Session updated successfully. Session: "" + session);<line8>          log.trace(""Session persisted successfully. Session: "" + session);<line11>        log.error(""Failed to persist session, id: "" + session.getId(), ex);<line13>      log.error(""Failed to persist session, id: "" + session.getId(), e);"
<line21>      log.error(systemException, systemException);
"<line2>    logger.debug(""Device item {} state changed to {}"", item, newState);"
<line6>      Log.error(e.getMessage());
"<line8>      logger.info(String.format(""Destroying failed nodes with ids: %s"", lostIds.toString()));<line14>      logger.info(""Failed nodes destroyed ;)"");"
"<line11>      LOG.info(""Adaptive Scheduler configured, but Batch job detected. Changing scheduler type to NG /""+ "" DefaultScheduler."");"
"<line2>      log.warn(""Bad checksum at "" + getPosition() + "". Skipping entries."");"
"<line14>        logger.error(message);<line21>      logger.warn(""Ontology created from template with errors"");"
"<line5>    LOGGER.info(""Autowired IntegrationStepHandlers found: {}"", integrationStepHandlers.size());<line11>      LOGGER.info(""ServiceLoader loaded IntegrationStepHandlers: {}"",handlers.size() - integrationStepHandlers.size());"
"<line8>                    LOG.error(""Failed to stop process"", t);"
<line9>      logger.error(e);
"<line3>      log.info(sql);<line5>      log.info(""*Failed to select sequence meta: "" + continued.getMessage());"
"<line4>        logger.info(""begin to create h2 database tcp server..."");<line12>        logger.info(""begin to start h2 database tcp server..."");<line13>        logger.info(String.format(""h2 database tcp server is started on port %s"", TCP_PORT));<line14>        logger.info(""h2 database tcp server is already running"");<line16>        logger.info(""begin to create h2 database web server..."");<line29>        logger.info(""begin to start h2 database web server..."");<line30>        logger.info(String.format(""h2 database web server is started on port %s"", WEB_PORT));<line31>        logger.info(""h2 database web server is already running."");<line33>      logger.error(""start h2 dabase server error"", e);"
"<line9>      log.debug(""Deleting not activated user {}"", user.getLogin());"
"<line21>          LOGGER.info(""Ignoring unregistered zero byte S3 file. s3Key=\""{}\"" storageName=\""{}\""""+ "" businessObjectDataKey={}"",s3ObjectSummary.getKey(),storageName,businessObjectDataKeyAsJson);"
"<line31>      logger.error(""Unable to get user settings:"", e);"
"<line12>          log.info(""Initialization completed after {} ms"",ManagementFactory.getRuntimeMXBean().getUptime());"
"<line2>      logger.warn(""error accessing mbean: {}"", mbeanObjectName, e);"
"<line3>      LOG.debug(""==> HiveMetastoreHook.onCreateTable()"");<line11>      LOG.debug(""<== HiveMetastoreHook.onCreateTable()"");"
"<line29>    LOGGER.info(""Rolling to new images has finished!"");"
"<line4>    logger.debug(""{} ready: initialized"", this.getClass().getName());"
"<line25>        logger.debug(""Thing {}: unexpected command {} from channel {}"",getThing().getUID(),command,channelUID.getId());"
"<line2>      logger.info(""Destroy registry:"" + getUrl());<line10>              logger.info(""Destroy unregister url "" + url);<line12>            logger.warn(""Failed to unregister url ""+ url+ "" to registry ""+ getUrl()+ "" on destroy, cause: ""+ t.getMessage(),t);<line25>              logger.info(""Destroy unsubscribe url "" + url);<line27>            logger.warn(""Failed to unsubscribe url ""+ url+ "" to registry ""+ getUrl()+ "" on destroy, cause: ""+ t.getMessage(),t);"
"<line22>        LOGGER.debug(""Group {} doesn't exist"", groupDN);"
"<line7>      logger.error(""Error updating jpavatar config"", t);"
"<line7>    logger.debug(""span:{}"", span);<line11>    logger.debug(""spanEvent:{}"", spanEvent);"
"<line2>    logger.debug(""Setting valency: "", valency);"
"<line5>      log.debug(""Receiving message from camel endpoint: '"" + endpointUri + ""'"");<line10>    log.info(""Received message from camel endpoint: '"" + endpointUri + ""'"");"
<line21>      log.error(systemException, systemException);
<line19>      log.error(systemException, systemException);
"<line4>    logger.debug(""registerUuid result: {}"", response);"
"<line1>    logger.info(""Obtained session"");"
"<line2>    logger.info(context, ""FeedServiceImpl:getFeedsByUserId method called : "");<line24>                logger.error(context,""FeedServiceImpl:getRecordsByUserId :Exception occurred while mapping feed""+ "" data."",ex);"
"<line4>    log.debug(""env for channel {}: {} = {}"", new Object[] {id, name, value});"
<line17>      LOG.warn(exception.toString());
"<line41>              LOG.warn(""Failed to build sorted map from state, this may result in wrong result. ""+ ""The sort key is {}, partition key is {}, ""+ ""treeMap is {}. The expected inner rank is {}, but current size is {}."",sortKey,partitionKey,treeMap,innerRank,size);"
<line7>        log.debug(exception, exception);
"<line5>        LOGGER.warn(""cannot find event handler by class: "" + context.getClass());"
"<line3>      LOG.info(""*** server shut down"");"
"<line2>    log.info(""Getting current bundle list from TDM..."");<line31>    log.info(""Found "" + output.size() + "" bundle(s) available from the TDM."");"
"<line10>      LOG.error(""Exception when cancelling BigQuery job '{}' for stage '{}': {}"",bqDataset.getJobId(),datasetName,e.getMessage());<line17>      LOG.error(""Exception when deleting BigQuery table '{}' for stage '{}': {}"",bqDataset.getBigQueryTableName(),datasetName,e.getMessage());<line28>      LOG.error(""Failed to delete temporary directory '{}' for stage '{}': {}"",bqDataset.getGCSPath(),datasetName,e.getMessage());"
"<line7>      logger.error(""Error when setting the database driver "" + driver + ""{}"", ex.getMessage());"
"<line5>      logger.warn(""Exception during getCPUStateIdle"", e);<line6>    logger.trace(""getCPUStateIdle: {}"", result);"
"<line3>    LOG.info(""Killing "" + ensemble + "" from ensemble="" + firstEnsemble);"
"<line5>      logger.error(""save command history failed"", e);"
"<line4>    LOG.debug(""getRootFolder: "" + result);"
"<line5>    logger.info(""--- Stopping DX OSGi bundle {} --"", getDisplayName(bundle));<line35>    logger.info(""--- Finished stopping DX OSGi bundle {} in {}ms --"", getDisplayName(bundle), totalTime);"
"<line5>      logger.warn(format(""JDBC URL parameters user: %s passowrd: %s are not matching with commandline options""+ "" --source.username %s --source.password %s."",jdbcUsername, jdbcPassword, optionUsername, optionPasssword));<line5>      logger.warn(format(""JDBC URL parameters user: %s password: %s are used for database connection"",jdbcUsername, jdbcPassword));"
"<line23>      log.error(""Unable to render journal articles list"", exception);"
"<line2>    log.info(""...Checking previous resources by replacing"");"
"<line3>      LOG.debug(""==> AbstractServiceStore.updateTagServiceDefForDeletingRowFilterDef(""+ serviceDefName+ "")"");<line17>      LOG.debug(""<== AbstractServiceStore.updateTagServiceDefForDeletingRowFilterDef(""+ serviceDefName+ "")"");"
"<line7>          LOG.info(""Notification spooling is enabled: spool directory={}"", spoolDir);<line10>          LOG.info(""Notification spooling is not enabled"");"
"<line5>      log.error(""eval expression {} error"", expression, e);"
"<line19>          log.info(""Thread {} ops {} current TPS {}"",Thread.currentThread().getName(),submitOps,currentTPS);<line34>    log.info(""Thread {} final ops {} in {} seconds, TPS {} "",Thread.currentThread().getName(),totalOps,duration.elapsed(SECONDS),currentTPS);"
"<line9>          log.warn(""Cannot close: "" + name + "". Reason: "" + e.getMessage(), e);<line10>          log.warn(""Cannot close. Reason: {}"", e.getMessage(), e);"
"<line17>    logger.trace(format(""Register QueuedBlockEvent listener %s"", handle));"
"<line12>                  logger.info(""Incremental build request being processed: ""+ module.getRootPath()+ "" (updated)."");<line15>                  logger.error(e.getMessage(), e);"
<line18>      logger.info(e.getMessage(), e);
"<line13>    logger.info(""CoachShuttleGathering {} has {} road locations, {} coaches, {} shuttles and {} bus stops""+ "" with a search space of {}."",getInputId(),solution.getLocationList().size(),solution.getCoachList().size(),solution.getShuttleList().size(),solution.getStopList().size(),getFlooredPossibleSolutionSize(possibleSolutionSize));"
"<line17>        LOG.debug(""Unresolved reference"", ignore);"
<line14>    logger.info(mvcResult.getResponse().getContentAsString());
"<line1>    log.debug(""find() - id: {}"", id);"
"<line24>        log.info(""Failed to copy from blob store. Downloading from BLOB server instead."", e);<line42>        log.warn(""Could not delete the staging file {} for blob key {} and job {}."",incomingFile,blobKey,jobId);"
"<line3>    log.info(""loading from table properties: {}"", storageTableName);"
"<line6>      LOGGER.debug(""Calling after() for '{}' in processor '{}'..."",interceptor,component.getLocation().getLocation());"
<line5>      LOG.warn(e.getMessage(), e);
"<line12>                      Logger.debug(PentahoSystem.class,""System Listener Start: "" + systemListener.getClass().getName());<line21>                      Logger.debug(PentahoSystem.class,""System Listener Complete: "" + systemListener.getClass().getName());"
"<line4>      log.error(""failed to convert {} to a JCR path"", path);"
"<line13>    logger.info(""Created column '""+ col.REGISTRATION_DATE+ ""' and set the current timestamp, ""+ currentTimestamp+ "", to all ""+ updatedRows+ "" rows"");"
"<line3>      LOG.error(""createBalanceObject unexpected record size={}, record={}"",balance.size(),balance.toString());"
<line26>      LOGGER.error(e.getMessage(), e);
"<line12>        LOGGER.info(""No queries were found"");"
"<line7>        log.warn(""Missing com.apple.security.application-groups in sandbox entitlements"");<line15>          log.warn(String.format(""Migrate application support folder from %s to %s"", previous, folder));<line18>            log.warn(String.format(""Move application support folder %s to Trash"", previous));<line24>              log.warn(String.format(""Failure cleaning up previous application support directory. %s"",e.getMessage()));<line26>            log.warn(String.format(""Failure migrating %s to security application group directory %s. %s"",previous, folder, e.getMessage()));<line28>          log.debug(String.format(""No previous application support folder found in %s"", previous));<line32>    log.warn(""Missing support for security application groups. Default to application support""+ "" directory"");"
<line21>      log.error(systemException, systemException);
"<line2>    LOGGER.debug(""testGetPaginatedGroup"");"
"<line17>    log.debug(""Trasformazione esito caricamento pendenza in formato JSON -> CSV tramite template""+ "" freemarker ..."");<line36>      log.debug(""Trasformazione esito caricamento pendenza JSON -> CSV tramite template freemarker""+ "" completata con successo."");<line44>      log.error(""Trasformazione esito caricamento pendenza JSON -> CSV tramite template freemarker""+ "" completata con errore: ""+ e.getMessage(),e);"
"<line10>          LOG.info(""Output File: "" + status.getPath());<line10>          LOG.info(""key: '"" + key + ""' value: '"" + value + ""' expected: '"" + expectedResult + ""'"");"
"<line3>      LOG.info(""Received completed container:"" + cont);<line5>        LOG.error(""Container complete event for unknown container id "" + cont.getContainerId());"
"<line5>      log.error(""Error performing heartbeat: "" + e.getMessage());"
"<line8>        log.error(""Failed to delete secret from storage [{}]"", secret.getId(), e);"
"<line5>    log.info(""looking to see if iRODS tag still desired:{}"", currentTag);<line13>      log.info(""removing tag from iRODS, no longer desired:{}"", currentTag);"
"<line7>        logger.warn(""Exception at updateProfileImage"", e);"
<line11>    logger.debug(url);
"<line5>        logger.warn(""Playlist failed to remove entries from start, check all cameras in groups use the same""+ "" HLS settings."");"
"<line15>    LOGGER.info(""Executing RemoveStoreCommand..."");<line22>      LOGGER.error(""Exception encountered executing command"", e);"
"<line8>        LOGGER.warn(""Project {1} does not exist."", project.getName());"
"<line1>    log.debug(""getting MGsiegel instance with id: "" + id);<line5>      log.error(""get failed"", re);"
"<line4>    logger.info(String.format(""Wrote value: %s to element with id %s"", keyword, index));<line14>    logger.info(""Clicked save configuration button in APP settings"");"
"<line11>      log.error(""Failed to report active user."", e);"
"<line7>          LOGGER.debug(""Can't parse the matched number."", ex);"
<line5>    log.info(json);
"<line10>      LOGGER.error(""Exception:"", ex);"
<line22>      log.error(systemException, systemException);
"<line13>      LOG.debug(String.format(""Preparing to start process %s"", commandLine.toString()));<line21>      LOG.debug(String.format(""Successfully start process %s"", commandLine.toString()));<line22>      LOG.trace(String.format(""Problem during starting process %s"", commandLine.toString()), ex);<line25>    LOG.debug(String.format(""Waiting for the proces %s finish"", commandLine.toString()));<line31>      LOG.error(String.format(""Problem during process execution %s"", commandLine.toString()), ex);<line32>    LOG.debug(String.format(""Process %s has finished"", commandLine.toString()));"
"<line9>        LOG.warn(""Encountered error {} while recovering transaction {}. ""+ ""Presumably this transaction has been already committed before"",ex,transaction);"
"<line10>        LOG.error(""Unknown role "" + role);"
"<line6>          log.error(""Location {} for 2nd level cache should be a directory."", location);<line12>            log.warn(""Directory {} for 2nd level cache could not be created."", location);"
"<line2>    logger.trace(""{}: Received a heartbeat response"", memberName);<line9>          logger.info(""{}: Losing leadership because current term {} is smaller than {}"",memberName,currTerm,followerTerm);"
"<line8>    LOG.debug(""Entering AdapterPatientDiscoveryProxyNoOpImpl.respondingGatewayPRPAIN201305UV02"");"
"<line2>    LOG.info(""executing "" + getClass().getSimpleName());<line3>      LOG.info(""sending newsletter emails"");"
"<line21>        LOG.info(""exchangeVertexPartitions: Nothing to exchange, "" + ""exiting early"");<line34>          LOG.info(""exchangeVertexPartitions: Waiting for workers "" + workerIdSet);<line44>      LOG.info(""exchangeVertexPartitions: Done with exchange."");"
"<line25>    LOGGER.info(""WFS filter GetFeature response:\n"" + prettyString(doc));"
"<line46>              log.warn(""HtmlPreviewEntryPersistenceImpl.fetchByG_C_C(long, long, long, boolean) with""+ "" parameters (""+ StringUtil.merge(finderArgs)+ "") yields a result set with more than 1 result. This violates the logical""+ "" unique restriction. There is no order guarantee on which result is""+ "" returned by this finder."");"
<line22>      log.debug(q.toString());
<line3>    context.getStepLogger().debug(Messages.CHECKING_UPLOAD_APP_STATUS, application.getName());<line6>    LOGGER.info(format(Messages.UPLOAD_STATUS_0, upload));<line12>            .error(Messages.ERROR_UPLOADING_APP_0_STATUS_1_DESCRIPTION_2,application.getName(),upload.getStatus(),errorDetails.getDescription());<line14>        context.getStepLogger().debug(Messages.APP_UPLOADED, application.getName());
"<line1>    LOGGER.info(""============æ§è¡cancel ä»æ¬¾æ¥å£==============="");"
"<line7>            log.debug(""Ignoring put(). New entry is not from a newer reader. ""+ ""existing: ""+ e.creationTick+ "", new: ""+ reader.getCreationTick());"
"<line16>        log.warn(""Invalid schema version for release: "" + release, illegalArgumentException);"
"<line15>    logger.debug(""About to marshal process variable with name '{}' {}"", varName, variable);"
"<line2>    LOGGER.debug(""create context={}"", contextDto);"
"<line18>        log.error(""The requested state change should be either - 'ON' or 'OFF'"");<line33>      log.error(e.getErrorMessage(), e);"
"<line5>      logger.error(""Encountered an error: "", e);"
"<line2>      logger.info(""the java mongo replica already start"");<line10>      logger.error(""start replicator:{} error"", replicaSetConfig, e);"
"<line25>        logger.debug(""Definita dimensione di resize: {}"", dimension.getIdDim());"
"<line12>      LOG.info(""Not starting scheduler because autoStartScheduler is set to false."");<line15>          LOG.warn(""The scheduler has already started. Cannot apply the 'startDelayedSeconds'""+ "" configuration!"");<line16>          LOG.info(""Starting scheduler with startDelayedSeconds={}"", startDelayedSeconds);<line20>          LOG.info(""The scheduler has already been started."");<line21>          LOG.info(""Starting scheduler."");"
"<line1>    log.info(""Started event send for read"");<line11>      log.error(""Exception encountered: "" + ex.getMessage(), ex);<line13>    log.info(""Completed event send for read"");"
"<line2>    log.info(""Starting getLatestBundle."");<line11>        log.error(""Error reading latest bundle: "" + e);"
"<line2>    logger.debug(""Opening serial connection on port {}"", serialPortName);<line23>          logger.debug(""Caught IOException at dataIn.reset(): {}"", e.getMessage());<line32>      logger.debug(""Serial connection opened"");"
<line8>    LOGGER.error(String.format(Messages.Log.ERROR_WHILE_ATTACHING_VOLUME_GENERAL_S, errorText));
"<line2>      LOG.error(""All schedules must have a start time!"");<line6>      LOG.error(""Recurrence interval and unit must either both be present or both be absent"");<line9>      LOG.error(""Recurrence interval must be positive"");<line14>        LOG.error(""Recurrence must be at least {} ms"", MIN_RECURRENCE_MILLIS);"
"<line8>      logger.error(""Document creation failed."");"
"<line5>    LOGGER.debug(""DEBUG: start selectObjects {}"", jsonQuery);<line7>      LOGGER.debug(""{}"", jsonNode);<line13>      LOGGER.debug(""DEBUG {}"", jsonNode);<line14>      LOGGER.error(PARSING_ERROR, e);<line16>      LOGGER.error(ILLEGAL_ARGUMENT, e);<line18>      LOGGER.error(""exeption thrown"", e);"
"<line2>    log.trace(""Read {}  byte, val is {}"", context, ByteUtils.formatByte(b));"
"<line5>      LOG.error(""Could not perform reverse DNS lookup for [{}]. Cause [{}]"",key,ExceptionUtils.getRootCauseOrMessage(e));<line27>    LOG.debug(""Could not perform reverse lookup on IP address [{}]. No PTR record was found."", key);"
"<line7>        log.error(""Invalid setting for "" + prop + "" value="" + sPoolSize + "" using defaults."");"
"<line2>    logger.info(""Retrieving "" + this.url);"
"<line3>      log.trace(""Creating a new custom container builder"");"
"<line3>    logger.debug(""{} ready. Initialized"", this.getClass().getName());"
"<line2>    log.debug(""1"");<line2>    log.info(""2"");<line2>    log.warn(""3"");<line2>    log.error(""4"");"
"<line2>      logger.debug(""Cancelling query {}"", QueryIdHelper.getQueryId(id));"
"<line1>    log.trace(""Entered init"");<line2>    log.trace(""Exit init"");"
"<line20>        LOGGER.warn(""Node instance with url "" + ref.getUrl() + "" not found"");"
"<line1>    LOGGER.info(""Get the Object Group with Identifier {}"", id);"
"<line2>      LOG.debug(""==> HiveHook.initialize()"");<line13>      LOG.error(""Error instantiating Atlas hook implementation"", excp);<line17>      LOG.debug(""<== HiveHook.initialize()"");"
"<line8>      logger.trace(""Value '{}' for {} hasn't changed, ignoring update"", value, variable);"
"<line17>        log.error(""Exception executing request "" + request + "": "" + e.getCause().getLocalizedMessage(),e.getCause());<line20>      log.error(""Exception processing request "" + request, e);"
"<line41>      LOG.info(""Published [%d] segments"", newSegments.size());"
"<line2>      LOG.info(""Validation of Timestamp: The message was created in the future!"");"
"<line18>        logger.debug(debugger,"">>>>>>>>>>create a new schedule setting for unit [""+ topUnitName+ ""][""+ unitName+ ""]......"");"
"<line5>      LOGGER.info(""Got app ID: {}"", appId);"
"<line4>      LOG.info(""Found scheduled, but not in database {}"", configId);"
"<line2>    LOG.trace(""Starting {}"", this);"
"<line27>        log.warn(""Failed to disconnect transport"", e);"
"<line20>      log.error(""InterruptedException in receiver "" + i, e);"
"<line8>        logger.info(""Join check from ""+ getCallerAddress()+ "" failed validation due to incompatible version,""+ ""remote cluster version is ""+ request.getClusterVersion()+ "", this cluster is ""+ service.getClusterVersion());"
"<line9>                  log.error(""[Sentinel Starter] DataSource ""+ dataSourceName+ "" multi datasource active and won't loaded: ""+ dataSourceProperties.getValidField());<line19>                log.error(""[Sentinel Starter] DataSource ""+ dataSourceName+ "" build error: ""+ e.getMessage(),e);"
"<line23>        LOGGER.trace(""Removing existing geonames data with filter: {}"", filter);<line32>        LOGGER.trace(""Deleting {} GeoNames metacards"", metacardsToDelete.size());"
<line11>      LOG.error(myDiff.toString());
"<line13>    logger.info(""[testDeleted] {}"", needDeletedHello);"
"<line6>      log.info(""Test cache [mode=""+ ccfg.getCacheMode()+ "", atomicity=""+ ccfg.getAtomicityMode()+ "", backups=""+ ccfg.getBackups()+ "", near=""+ near+ ""]"");"
"<line2>    LOGGER.info(""TestJDBCInterface setup"");"
"<line3>      logger.error(""Resource not found: "" + path);"
"<line10>      LOG.info(""Failed to resolve the network location"", exc);"
"<line6>    log.info(""CAMP created '{}'"", instance);"
"<line2>      log.warn(""Detected that shutdown was requested. ""+ ""All clients in this app will now begin to shutdown"");"
<line10>        LOG.error(SshdText.get().sessionCloseFailed, e);
<line4>      LOG.info(json);
"<line5>        log.debug(""Unable to get file entry "" + fileEntryId, portalException);"
"<line5>      LOGGER.trace(""Parent null, skipping virtual items"");<line9>      LOGGER.trace(""No object wrapper found. Skipping virtual items."");<line16>          LOGGER.debug(""No wrapper found for {}"", virtualItemPath);<line23>        LOGGER.error(""Cannot find wrapper with path {}, error occurred {}"", virtualItem, e.getMessage(), e);"
"<line15>    LOG.info(""initializeUI took "" + watch.taken());"
"<line1>    log.debug(""finding FilterResZob instance by example"");<line8>      log.debug(""find by example successful, result size: "" + results.size());<line10>      log.error(""find by example failed"", re);"
"<line11>        log.warn(""Cannot find host ip"", e);"
"<line2>    log.debug(""onEndRequest"");"
"<line2>    Log.debug(""Test"");"
"<line6>      logger.info(""Bluetooth adapter interface => {}"", this.name);<line6>      logger.info(""Bluetooth adapter address => {}"", this.bluetoothAdapter.getAddress());<line6>      logger.info(""Bluetooth adapter le enabled => {}"", this.bluetoothAdapter.isLeReady());<line7>        logger.info(""Enabling bluetooth adapter..."");<line8>        logger.info(""Bluetooth adapter address => {}"", this.bluetoothAdapter.getAddress());<line11>      logger.warn(""No Bluetooth adapter found ..."");<line12>    logger.debug(""Updating Beacon Example... Done."");"
"<line29>      logger.error(""move processDefinition error: {}"", e.getMessage(), e);"
"<line4>    LOGGER.info(""Checking url: "" + url);"
"<line2>    LOG.debug(""Destroying scout server scoutSessionId={}"", session.getId());"
"<line13>    LOGGER.warn(""Could not cast preference value for key [""+ key+ ""] from [""+ pref.getValueType()+ ""] to [""+ type+ ""]"");"
"<line3>    log.debug(""Output from command: [{}]"", baos.toString().replaceAll(""\n"", ""\\\\n""));"
"<line6>    log.info(""started ""+ app+ "" containing ""+ brooklynNode+ "" for ""+ JavaClassNames.niceClassAndMethod());"
"<line4>    LOG.error(String.format(""A channel exception set on %s"", toString()));"
<line17>      log.error(systemException, systemException);
"<line7>        logger.warn(StringMessageUtils.getBoilerPlate(newArrayList(""WARNING:"","" "",""Class: '""+ name+ ""' is NOT exposed by the plugin but it will be visible ""+ ""due to it was manually forced to be exported for testing purposes."","" "",""Check if this is really necessary, this class won't be visible in standalone""+ "" mode.""),'*',DEFAULT_MESSAGE_WIDTH));"
"<line3>      logger.debug(String.format(""found %d domains"", domains.length));<line13>            logger.debug(""active disk found by cleanup thread"" + disk.getDiskPath());<line17>      logger.warn(""[ignored] Error trying to cleanup "", e);"
"<line2>    LOG.debug(""Vertices of {}"", graph);<line3>      LOG.debug(vertexString(vertex));<line4>    LOG.debug(""Edges of {}"", graph);<line5>      LOG.debug(edgeString(edge));<line6>    LOG.debug(""*******************Graph Dump****************************"");"
"<line7>      LOG.debug(""Received body: {}"", in);<line36>        LOG.debug(""Writing body: {}"", response);<line38>        LOG.debug(""Writing no response"");<line56>        LOG.debug(""Closing session when complete at address: {}"", address);"
"<line11>      LOG.error(""Unknown error during processing of [mutations={0}]"", e, mutations);"
"<line6>      LOGGER.warn(""Error upon {} mapping resolution for {}. You might want to audit mapping content for""+ "" this mapping entry. "",fromUser.asString(),connectedUser.asString());"
"<line8>    LOG.debug(""Begin provideAndRegisterDocumentSetBDeferredResponse"");<line37>    LOG.debug(""End provideAndRegisterDocumentSetBDeferredResponse"");"
"<line4>    log.debug(""Reading template {} from database"", orderXmlName);<line31>      log.warn(message, e);"
"<line4>      LOGGER.warn(MessageFormat.format(this.getActionExecution().getAction().getType()+ "" does not accept {0} as type for check name"",checkName.getClass()));"
"<line2>    LOG.info(Arrays.toString(args));<line39>    LOG.info(""JCT "" + (System.currentTimeMillis() - start));"
"<line5>      log.info(""[{}] Successfully removed topic {}"", clientAppId(), topicName);<line7>      log.error(""[{}] Failed to delete topic {}"", clientAppId(), topicName, t);"
"<line2>      log.warn(""Global FacesMessage to user (wid: {}): {})"",windowContext.getCurrentWindowId(),msg.getSummary());"
<line18>      logger.info(s);
"<line43>      logger.error(""Error occurred while executing 'describe disk-store': {}!"", e.getMessage(), e);"
"<line3>    log.trace(""Amqp reactor thread {} has started"", THREAD_NAME);<line6>      log.error(""Encountered an exception while running the AMQP reactor"", e);<line8>    log.trace(""Amqp reactor thread {} has finished"", THREAD_NAME);"
"<line37>      logger.trace(String.format(""FCM Notification Response status=%d, response=%s"",conn.getResponseCode(), response));"
"<line1>    log.debug(""validRequestorForSendType_returnType:"" + sendType + "","" + returnType);"
"<line47>      logger.debug(""SEARCHING FOR NOTES WITH CRITERIA: "" + criteria);"
<line16>      log.error(systemException, systemException);
"<line7>    LOG.info(""Total number of created elements: "" + uuidList.size());"
"<line10>        logger.error(""Failed to read response from remote service: "", e);"
"<line5>      this.logger.info(""Root context already created (using as parent)."");"
"<line3>      logger.info(""TimeSeries list to be deleted is empty."");"
<line12>      LOGGER.error(e);
"<line19>      LOG.info(String.format(""Padding ORC by %d bytes while merging.."", availBlockSpace));"
"<line10>      LOG.warn(""ExternalDataset '{}' does not have method '{}'. ""+ ""Can't register {} lineage for this dataset"",referenceName,methodName,accessType);<line11>      LOG.warn(""Unable to register {} access for dataset {}"", accessType, referenceName);"
"<line26>      log.error(""Error while storing: "" + e.toString());"
"<line24>      LOG.debug(""Error occurred: "", e);"
"<line44>        LOGGER.debug(""Cannot match record: merged record has a too low confidence value (""+ normalizedConfidence+ "" < ""+ minConfidenceValue+ "")"");"
<line10>        LOG.warn(e1.getMessage(), e1);
"<line6>          log.info(""[task.stop] removing from queue {} kick={} key={}"", kick.getJobKey(), kick, key);<line11>              log.warn(""---> send fail {} {} {}"", uuid, key, kick, ex);"
"<line6>        LOG.debug(""renewing delegation tokens for principal={}"", hiveMetaStorePrincipal);<line8>        LOG.info(""Renewed delegation token. new expiryTime={}"", expiryTime);<line16>            LOG.error("" Exception"", e);"
"<line6>      log.debug(count+ "" ""+ uids.size()+ "" ""+ uidsToRemove.size()+ "" ""+ uidsCopy.size()+ "" removing ""+ (count == 0 && uidsCopy.isEmpty()));"
<line56>      log.error(e.getMessage(), e);
<line7>      log.debug(e.getMessage(), e);
"<line7>    log.debug(""usersPage.getTotalElements(): {}"", usersPage.getTotalElements());"
"<line2>    Log.debug(""Test"");"
"<line2>    logger.error(""Filter get error"", t);"
<line3>      logger.error(new EmployeeNamesEmptyException());<line13>      logger.error(new CycleYearEmptyException());<line17>      logger.error(new CycleMonthEmptyException());
"<line4>      logger.trace(""Service already unregistered."");"
"<line6>          LOG.warn(""Could not find key {} in message"", mConfig.getMessageTimestampName());"
"<line15>    LOGGER.debug(""Projects size is {}"", projectPaginationDTO.getProjects().size());"
"<line3>    logger.info(""Qanary Message: {}"", myQanaryMessage);<line6>    logger.info(""Question: {}"", myQuestion);<line9>    logger.info(""Language: {}"", lang);<line9>    logger.info(""store data in graph {}"", myQanaryMessage.getEndpoint());"
<line9>          LOG.error(ex.getLocalizedMessage());
"<line22>      log.error(""Error destroying prepared queries"", e);"
"<line13>      logger.error("""", fex);"
"<line25>      log.debug(""Monitoring is adding a Suggestion entry"");<line27>      log.error(""Suggestion monitoring error: {}"", e.getMessage(), e);"
"<line3>      logger.trace(""The value {} ({}) is considered a byte because only the 8 least significant bits are set""+ "", but its value is outside signed byte that is between -128 and 127"",b,Integer.toHexString(b));"
<line13>        log.info(msg);
"<line7>    log.info(""-- start process, key: {}"", key.toString());<line19>        log.info(""---- key "" + key.toString() + "", part "" + partNb + "", documents {} "" + counter);<line32>    log.info(""-- end process, key: {}"", key);"
"<line3>      logger.debug(""PublishAuthUpdateTask.run started..."");<line4>        logger.trace(""Thread {} is interrupted..."", Thread.currentThread().getName());<line8>      logger.debug(""Exception:"", ex.getMessage());"
"<line1>    log.debug(""merging StgSysExportZos instance"");<line4>      log.debug(""merge successful"");<line6>      log.error(""merge failed"", re);"
"<line7>      log.debug(""Open file: {}"", fileChooser.getSelectedFile().getPath());"
"<line4>    LOGGER.debug(""Patch {} with {}"", id, partialDto);"
"<line2>    logger.trace(""Adding new MAX! {} with id '{}' to inbox"", device.getType(), device.getSerialNumber());<line56>      logger.debug(""Discovered MAX! device is unsupported: type '{}' with id '{}'"",device.getType(),device.getSerialNumber());"
"<line13>            log.trace(""ignored duplicate packet: apid={} time={} seq={}"",apid,TimeEncoding.toOrdinalDateTime(instant),seq);<line26>          log.trace(""ignored duplicate packet: apid={} time={} seq={}"",apid,TimeEncoding.toOrdinalDateTime(instant),seq);"
<line13>      LOG.error(e.toString(), e);
<line5>        log.debug(exception, exception);
"<line6>    logger.debug(""Processing DependencySet (output="" + dependencySet.getOutputDirectory() + "")"");<line7>      logger.warn(""DependencySet has nonsensical configuration: useTransitiveDependencies == false ""+ ""AND useTransitiveFiltering == true. Transitive filtering flag will be ignored."");<line12>    logger.debug(""Adding "" + dependencyArtifacts.size() + "" dependency artifacts."");<line26>        logger.debug(""Error retrieving POM of module-dependency: ""+ depArtifact.getId()+ ""; Reason: ""+ e.getMessage()+ ""\n\nBuilding stub project instance."");"
"<line17>      logger.error(""exception throws for receiver:"" + queryReceiver + "" retry another receiver"");<line28>        logger.error(""exception throws for receiver:"" + receiver + "" retry another receiver"");"
<line3>    log.error(msg);
"<line3>      log.info(""No virtual pool replication settings changes have been made"");<line9>      log.info(""Protection parameters cannot be modified to a vpool with provisioned filessystems "",from.getId());<line12>      log.info(""RPO parameters cannot be modified to a vpool with provisioned filessystems "",from.getId());<line14>    log.info(""No protection changes"");"
"<line24>        logger.error(""Error validating content {}"", this.getContentId(), t);<line32>        logger.error(""error creating new widget"", t);"
"<line22>                    LOGGER.debug(""Using the configured proxy username and password"");"
"<line1>    logger.debug(""Discovery job is running"");<line7>      logger.debug(""Discovery job got Socket exception creating multicast socket: {}"", se.getMessage());<line9>      logger.debug(""Discovery job got IO exception creating multicast socket: {}"", ioe.getMessage());<line16>        logger.debug(""Discovery job got exception waiting for beacon: {}"", ioe.getMessage());<line23>        logger.trace(""Projector with UID {} discovered at IP: {}"",uid,epsonMulticastListener.getIPAddress());<line24>        logger.trace(""Creating epson projector discovery result for: {}, IP={}"",uid,epsonMulticastListener.getIPAddress());<line34>    logger.debug(""Discovery job is exiting"");"
"<line5>    log.debug(""unwrapping package {}"", pack == null ? ""(unknown)"" : pack.getId());<line8>      log.debug(""unwrapping package {} completed in {}ms"", getId(), System.currentTimeMillis() - now);"
"<line7>        logger.warn(""Unable to publish updated App Certificate"");"
"<line12>        logger.info(""Tagger returned "" + taggerAllCrisesResponse.getCrisises().size() + "" crisis for user"");<line15>      logger.error(""Exception while fetching crisis by userId: "" + userId, e);"
"<line11>        logger.info(""Failed to add index for "" + value + "" due to "" + e.getMessage());"
"<line6>          LOG.warn(""Rack {} not present, discarding {}"", expiringObject.getMachineId(), expiringObject);<line10>            LOG.error(""Could not return rack {} to state {}"",rack.get().getName(),expiringObject.getRevertToState());"
"<line2>      LOG.trace(""Resource attempted to schedule a null task."");"
"<line4>    LOGGER.warn(""*      FINISHED CustomCRSLandsatIT               *"");<line4>    LOGGER.warn(""*         ""+ ((System.currentTimeMillis() - startMillis) / 1000)+ ""s elapsed.                 *"");<line4>    LOGGER.warn(""*                                       *"");<line4>    LOGGER.warn(""-----------------------------------------"");"
"<line19>        logger.info(""Transaction execution failed (service={}, txId={}, txMessageHash={})"",service.getName(),txId,context.getTransactionMessageHash(),e);"
"<line6>      LOGGER.warn(""Objects in source {} not found for ids: {}"", originalSource, differences);<line12>            LOGGER.warn(""Objects in source {} not found for ids: {}"", masterSource, differences);<line14>          LOGGER.debug(""Source not configured: {}"", masterSource, e);"
"<line32>      logger.warn(""unexpected table: {}"", tableName);"
"<line9>      logger.error(""meet error when create upgrade log, file path:{}"", upgradeLogPath, e);"
"<line3>      logger.trace(""Adding pageTransaction "" + pageTransaction.getTransactionID());"
"<line2>    LOG.info(""Executing operation getTransformerPluginV2Configuration"");"
"<line2>      LOG.debug(""==> RangerDefaultAuditHandler.getAuthzEvents("" + results + "")"");<line17>      LOG.debug(""<== RangerDefaultAuditHandler.getAuthzEvents("" + results + ""): "" + ret);"
"<line35>        log.error(""Cannot create new id token"", e);"
"<line9>      log.warn(""Ignore unknown class or property "" + targetType + "" "" + ex.getMessage());<line16>      log.error(""Failed to write "" + result + "" ex="" + ex, ex);"
"<line7>          log.warn(""{} was deprecated and will be removed in a future release; assuming user meant""+ "" its replacement {} and will remove that instead"",property,replacement);"
<line3>      logger.warn(marker, format, arguments);
"<line8>      logger.error(""getBrokerByRetryType error request {},response {}"", request, response);"
"<line3>    LOG.debug(""listing a migration issue for a content migration for the course"");"
"<line5>      LOGGER.error(""No source uri given to load model, return empty model."");<line9>      LOGGER.error(""Connection to modelserver has not been initialized, return empty model"");<line27>      LOGGER.warn(""No workflow index given to create model, use workflow with index: "" + workflowIndex);<line32>      LOGGER.error(""No workflow with index "" + workflowIndex + "" in "" + machine + "", return empty model."");"
"<line8>        log.error(""Error writing to network port"");"
"<line4>      LOGGER.warn(""Language classification {} not found"", classification.getRootID());"
"<line2>      log.debug(""Wrong push body."");<line6>      log.debug(""User has no access token provided for eventor push."");"
"<line12>      LOGGER.warn(""Could not find language resources \""""+ fileName+ ""\"" for SiteReportRater. Using default (english)."");"
"<line14>      LOG.warn(""Exception occurred during done UnitOfWork for Exchange: {}. This exception will be""+ "" ignored."",exchange,e);"
"<line3>      logger.debug(""loading DataMapper Libraries"");<line6>        logger.debug(""Found data mapper library:""+ dataMapperLibrary.getClass().getCanonicalName()+ "":""+ dataMapperLibrary.getLibraryName());<line13>        logger.trace(""Java runtime provided duplicates for ""+ dups.entrySet().stream().map(e -> e.getKey() + "":"" + e.getValue()).collect(Collectors.joining("","")));<line15>    logger.info(""Loaded DataMapper Libraries:"" + libraries.keySet());"
"<line1>    log.debug(""Authenticating client '""+ credentials.getClientName()+ ""' with sheme: ""+ credentials.getAuthenticationScheme());<line13>    log.debug(""Authenticated as: "" + this.user);"
"<line8>        LOG.debug(""Deleted {} open keys out of {} submitted keys."",numDeletedOpenKeys,numSubmittedOpenKeys);<line11>        LOG.error(""Failure occurred while trying to delete {} submitted open "" + ""keys."",numSubmittedOpenKeys);<line13>        LOG.error(""Unrecognized result for OMOpenKeysDeleteRequest: {}"", request);"
"<line67>      LOG.error(""Category Code cannot be found from the category list during recalculation of account""+ "" object code for Contracts & Grants Invoice Document."");"
"<line5>        log.trace(""no store found for "" + o.getUID() + "" for disposal"");"
"<line1>    LOG.info(""Interrupting ongoing backup."");<line2>      LOG.info(""Attempt to cancel backup task."");<line6>      LOG.info(""Attempt to cancel backup timeout task."");<line10>        LOG.info(""Attempt to resume journal application."");<line12>        LOG.warn(""Failed to resume journal application: {}"", e.toString());<line14>    LOG.warn(""Backup interrupted successfully."");"
"<line11>        logger.trace(""Assertion from where roles will be sought = "" + AssertionUtil.asString(assertion));"
"<line59>      LOG.error("""", e);"
<line27>      LOGGER.debug(e);
"<line31>      logger.trace(""workflow definition id is not specified"");<line37>    logger.debug(""Schedule with workflow definition '{}'"", wfConfig.get(WORKFLOW_DEFINITION_ID_PARAM));<line39>      logger.debug(""Rejected schedule without media package"");<line48>      logger.debug(""Rejected ingest with invalid media package {}"", mp);<line53>      logger.debug(""There can be only one (and exactly one) episode dublin core catalog:""+ "" https://youtu.be/_J3VeogFUOs"");"
"<line3>    log.info(""Finding document by employee name {} ."", employeeName);"
<line13>      log.info(message);
"<line5>      LOGGER.debug(""found systemvm patch iso "" + systemVmIsoPath);<line9>      LOGGER.debug(""last resort for systemvm patch iso "" + svm);<line13>      LOGGER.error(""Unable to locate "" + iso + "" in your setup at "" + isoFile.toString());"
"<line12>              LOGGER.debug(""Loading roles which the current user has right to assign"");"
"<line2>      LOGGER.error(""Expected {}, but got {}"", targetClass, entity);"
"<line7>    LOG.debug(""Creating security event with targetEdOrgList determined from entities of type ""+ entityType);"
"<line9>        logger.debug(""Received null bridge while updating Area status!"");<line14>      logger.debug(""Received exception while refreshing Area status: {}"", e.getMessage());"
"<line13>      LOGGER.error(""Streaming for replica set {} failed"", replicaSet.replicaSetName(), t);"
"<line4>        LOG.info(""Writing person: "" + person);"
<line13>        log.debug(exception, exception);
"<line1>    log.info(""buildOuputZip called with id="" + id);"
"<line3>    LOGGER.info(""Loaded segment metadata for segment : "" + segmentMetadata.getName());"
"<line7>      log.trace(""%s: rejected connection from %s %s"",local_addr, peer_addr, explanation(conn_exists, replace));"
"<line4>      LOG.debug(""Running beforeSuite ..."");<line11>      LOG.debug(""HTTP server started."");<line12>      LOG.debug(""Existing RPs are removed."");<line15>      LOG.debug(""SETUP_CLIENT is set in Tester."");<line16>      LOG.debug(""Tester's authorization is set."");<line17>      LOG.debug(""Finished beforeSuite!"");<line18>      LOG.error(""Failed to start suite."", e);"
"<line12>        LOGGER.warn(""Plugin provider for ingest type '""+ formatPlugin.getIngestFormatName()+ ""' does not support hdfs ingest"",e);"
"<line10>          log.info(""Performing keytab-based Kerberos re-login"");<line12>          log.info(""Performing ticket-cache-based Kerberos re-login"");<line21>        log.debug(""Not attempting Kerberos re-login: loginUser={}, currentUser={}, realUser={}"",loginUser,currentUser,realUser);<line23>      log.warn(""Failed to check (and/or perform) Kerberos client re-login"", e);"
"<line2>    LOG.info(""Executing operation putTransformerV2"");"
"<line10>      logger.debug(""At ""+ methodName+ "" method, ""+ description+ "" [invoker address=""+ localAddress+ "", ServerEndPoint=""+ getConnectionURL()+ ""]"");<line12>      logger.trace(""At ""+ methodName+ "", thread stack:""+ StringUtils.NEW_LINE+ StringUtils.getCurrentStackTrace());"
"<line3>    LOG.info(""Error in pingAsyncAsync() {}"", t.getMessage());"
"<line2>      logger.debug(""{} Stopping leadership election..."", logPrefix());"
"<line18>            LOG.error(""Failed while processing event"", ex);"
"<line6>    log.debug(""[PERF] Saving {} translations in batches"", textFlows.size());<line11>      log.debug(""[PERF] Batch save transaction {} - {}"", batchStart, batchEnd);<line24>              log.debug(""[PERF] Commit translations to database ({}ms)"", storeTranslations);<line26>        log.error(""Error filling target with machine translation"", e);<line29>      log.debug(""[PERF] Transaction complete {} - {} ({}ms)"", batchStart, batchEnd, transactionTime);<line30>    log.debug(""[PERF] Batches complete ({}ms)"", saveAllTimer);"
<line9>      LOG.error(ERROR_TRANSLATING_POSTGRES_EXC_MSG, pSqlException);
"<line6>      logger.warn(""Operation will be retried after {} milliseconds. Current attempt {}, Cumulative delay""+ "" {}"",retryDelay.toMillis(),this.currentAttemptCount,this.cumulativeRetryDelay,exception);<line8>      logger.debug(""Operation will NOT be retried. Current attempt {}"", this.currentAttemptCount, exception);"
"<line11>          logger.info(""Access point reachable? {}"", ret);"
"<line26>              log.debug(""{} Res<- {}"", label, jsonMessage);<line32>              log.trace(""{} Res<- {}"", label, jsonMessage);<line40>                  log.error(""Trying to send a message to a closed session"");<line46>      log.error(""{} Exception processing request {}."", label, message.getPayload(), t);"
<line5>            logger.info(dialog.getTitle());
"<line2>    log.info(""{0}: Cache file root directory: {1}"", logCacheName, rafDir);"
"<line2>    LOG.info(""Deleting topic {}"", topic);"
"<line7>    log.info(""Has sampling from previous runs {}"", hasSampling);<line9>      log.info(""Loading sampling from {}"", samplingPath);"
"<line49>      logger.error(""Ip Assoc failure on applying one ip due to exception:  "", e);"
"<line19>      logger.debug(""findAcceptApplicationName result:{}"", resultSet);"
"<line3>      logger.debug(""Note: Empty AI platform is being mapped."");"
"<line5>    logger.info(""assert equal positions for '"" + javaCodeSnippet + ""' -> '"" + tsCodeSnippet + ""'"");<line8>    logger.info(""org: "" + javaPosition + "" --> "" + tsPosition);"
"<line5>    logger.info(String.format(""Processing input index=%s, item=%s, in (%s)"", index, input[index], this));<line9>          logger.info(String.format(""Throwing exception index=%s, item=%s, in (%s)"", index, input[index], this));"
"<line2>    logger.warn(String.format(""Communication channel lost for AspFactroy=%s Association=%s"",this.name, association.getName()));"
"<line5>      log.error(""RemoteException"", e);"
"<line10>        LOG.error(""Exception in flush thread"", e);"
"<line55>        logger.error(""Unsupported feature type in Describe Platform M1_0: ""+ this.getDatasetFeatureType().toString());"
"<line13>            this.logger.warn(LOG_DESCRIPTOR_FAILREAD,""Failed to read XAR descriptor from entry [{}]: {}"",entry.getName(),ExceptionUtils.getRootCauseMessage(e));<line20>            this.logger.info(LOG_DOCUMENT_SKIPPED, ""Skipped document [{}]"", skip.getEntityReference());<line23>            this.logger.warn(LOG_DOCUMENT_FAILREAD,""Failed to read XAR XML document from entry [{}]: {}"",entry.getName(),ExceptionUtils.getRootCauseMessage(e),e);"
"<line5>      log.debug(""Result in "" + this.getClass().getCanonicalName() + "" injected from cache"");"
<line7>        LOG.warn(ex.toString());
"<line20>              log.info(""Following pods are in ready state {}"", current);<line25>        log.warn(""Selenium application was not redeployed correctly, try it again {}/{}"", i, attempts);"
"<line9>          LOGGER.warn(""UNKNOWN COMMAND RECEIVED TRHOUGH PIPE ""+ pipe.getInboundPipe()+ "": ""+ uce.getMessage());"
"<line13>      LOG.debug(""Dereferencing {} for {}"", contextReference, bundleContext);"
"<line11>      LOGGER.trace(StringUtilities.formatTimingMessage(""Added/Remove Geometries to registry[""+ newVisibleGeomSet.size()+ ""/""+ newHiddenGeomSet.size()+ ""] in "",System.nanoTime() - start));"
"<line30>      LOGGER.error(""IO Exception"", e);<line32>      LOGGER.error(e.getMessage(), e);"
"<line4>      LOGGER.info(""Obtained custom configured Event Hubs client for namespace '{}'"",customProducer.get().getFullyQualifiedNamespace());<line20>    LOGGER.info(""Using default Event Hubs client for namespace '{}'"",producer.getFullyQualifiedNamespace());"
"<line2>    logger.debug(""{}: Subscribe Event Channel (terminalID={}, {}:{}"",thingId,config.getTerminalID(),config.getIpAddress(),config.getPort());<line26>          logger.debug(""{}: SUBSCRIBE returned SID {}"", thingId, sid);"
"<line3>      logger.trace(""Failed to notify success ({}) to a promise: {}"", result, promise);"
<line5>      LOG.debug(message);
"<line2>    logger.info(""Filter get the value: "" + appResponse.getValue());"
"<line10>        LOG.error(""Unrecognized search sql: "" + searchSql);"
"<line13>                LOG.warn(""Deleting stale registration request {} : {}"",requestedHost.getHostname(),requestedHost.getAddress());"
<line6>      log.error(exception, exception);
"<line5>      logger.trace(LogMarker.VERSIONED_OBJECT_LIST_VERBOSE,""reading object {} of type {}"",index,objectTypeArray[index]);"
"<line4>    logger.debug(""pre-load class start"");<line9>    logger.debug(""pre-load class end"");"
"<line1>    LOGGER.info(""Computing the diff with diff commnd line"");<line1>    LOGGER.info(""The diff will be computed between:"");<line1>    LOGGER.info(directoryVersionOne.getAbsolutePath() + "" and "");<line1>    LOGGER.info(directoryVersionTwo.getAbsolutePath());"
"<line2>    LOG.trace(""PersonnameDAO.create() - Begin"");"
"<line15>      logger.warn(""BBSConfigSetting update/ got a error!"");"
"<line6>    log.info(""unmarshalling fonts.substitutions"");"
<line10>        log.debug(configurationException, configurationException);
"<line19>        logger.debug(""user=""+ userName+ "" password=""+ passwd+ "" password length=""+ passwd.getBytes().length);"
<line3>      log.info(bom.toString());
"<line6>            LOG.info(""{} initializing."", getName());<line7>            LOG.info(""{} initialization complete."", getName());"
"<line5>      log.warn(""Exception closing webSocket session"", e);"
"<line8>      LOG.info(""Error adding metadata record. metadataName={} length={}"", name, recordLength, e);"
"<line53>      LOG.debug(""Second connection with same container Id failed as expected."");"
"<line27>      LOG.debug(""Configuring HTTP client to use HTTP proxy {}:{}"", proxyAuthHost, proxyAuthPort);"
"<line10>    LOG.info(""Session "" + sessionId + "" is created."");"
"<line4>        logger.trace(""QueueRemovalMessage: removing dispatched events on queue {} for {}"", regionName, id);<line7>      logger.info(""Queue found destroyed while processing the last dispatched sequence ID for a""+ "" HARegionQueue's DACE. The event ID is {} for HARegion with name={}"",new Object[] {id, regionName});<line10>      logger.error(String.format(""QueueRemovalMessage::process:Exception in processing the last dispatched sequence ID""+ "" for a HARegionQueue's DACE. The problem is with event ID, %s for HARegion""+ "" with name=%s"",regionName, id),e);"
"<line10>        log.warn(""[{}][{}] Failed to find session by internal id"", externalId, internalId);<line12>      log.warn(""[{}] Failed to find session by external id"", externalId);"
"<line4>      log.debug(""Stopping %s"", obj.getClass().getName());<line6>        log.debug(""\t%s()"", preDestroy.getName());<line9>          log.error(e, ""Stopping %s.%s() failed:"", obj.getClass().getName(), preDestroy.getName());"
"<line7>      LOGGER.error(""Error on sending registry entry."", e);"
"<line5>        log.info(""Starting to launch query {} on driver {}"",query.getQueryHandle(),query.getSelectedDriver().getFullyQualifiedName());<line11>          log.error(""Error launching query: {}"", query.getQueryHandle(), e);<line15>            log.error(""Error in setting failed status"", e1);<line22>          log.error(""Error releasing session"", e);"
"<line5>      LOG.debug(""Finished split source: %s (%s)"", splitSource.getCatalogName(), splitSource.toString());"
"<line17>        logger.debug(testName + "": returned payload:"");<line17>        logger.debug(objectAsXmlString(contact, ContactsCommon.class));"
"<line3>          LOGGER.info(""[{}][doShutdown] Shutting down"", id);<line8>              .doOnSuccess(s -> LOGGER.info(""[{}][doShutdown] Shutdown"", id));"
<line8>        logger.error(error);<line8>        logger.debug(exception);
"<line13>      logger.info(""Moves: guestId="" + updateInfo.getGuestId() + "", maxDateInDB="" + ret);<line14>      logger.info(""Moves: guestId="" + updateInfo.getGuestId() + "", maxDateInDB=null"");"
"<line59>        LOG.warn(""Unable to read {}"", docf);"
"<line11>      LOG.warn(""Failed to take over file access count for all tables, ""+ ""which may make the measurement for data temperature inaccurate!"",e.getMessage());"
"<line2>    log.info(""Removing user {}"", user);"
"<line1>    LOGGER.info(String.format(""Executing: %s from %s"",command,pathToWorkingDirectory != null? pathToWorkingDirectory.getAbsolutePath(): System.getProperty(""user.dir"")));"
"<line6>      log.info(""Retrieving and copying persisted state to ""+ destinationDir+ "" with orphaned state deleted"");<line28>      log.error(""Error retrieving persisted state: "" + Exceptions.collapseText(e), e);<line33>        log.warn(""Subsequent error during termination: "" + e2);<line33>        log.debug(""Details of subsequent error during termination: "" + e2, e2);"
"<line2>    logger.info(""Starting {}..."", this);<line16>              logger.debug(""Marking time to rotate file {}"", pathController.getCurrentFile());<line23>      logger.info(""RollInterval is not valid, file rolling will not happen."");<line24>    logger.info(""RollingFileSink {} started."", getName());"
"<line6>            LOGGER.warn(""Ignoring failing MappingSource to MailAddress conversion for user {}"", user, e);"
"<line3>      LOGGER.debug(""Requesting certificate verification."");<line13>      LOGGER.debug(""Certificate verification failed."");<line15>      LOGGER.debug(""Certificate verification passed."");"
"<line2>      LOG.warn(""it can't be updated because the critical path provided is empty"");"
<line23>      log.error(systemException, systemException);
"<line3>    LOGGER.debug(""get One Ingest id={}"", id);"
"<line6>      LOG.error(""Op {} threw an exception during processing watermark"", this.getClass().getName(), e);"
"<line3>    log.debug(""TEST 1 ENTERING"");<line5>    log.debug(""TEST 1 EXITING"");"
"<line8>        LOG.warn(""Error while closing writer to "" + entry.getKey() + "". Exception follows."", ex);<line21>        LOG.warn(""shutdown interrupted on "" + execService, ex);<line28>    LOG.info(""Hive Bolt stopped"");"
"<line9>      log.warn(""Compaction failed for {} on {}"", compactable.getExtent(), getJob(), e);"
"<line1>    log.info(""Configuration resource: "" + resource);"
"<line2>      LOG.info(""Getting Hive Connection to "" + config.hiveJDBCUrl);"
"<line26>    log.debug(""Challenge:"" + new String(message, StandardCharsets.ISO_8859_1));<line27>    log.debug(""Client response:"" + new String(message, StandardCharsets.ISO_8859_1));"
"<line2>    logger.debug(""Getting index of single electron: "", electron);"
"<line6>      LOGGER.error(""Error writing uploaded File in temp file"", e);"
"<line2>    LOG.debug(""Creating a wired context to local LDAP server on port {}"", ldapServer.getPort());"
"<line2>    logger.info(""Starting simpleReadUUID()"");<line16>        logger.debug(""simpleReadUUID() Created {} entities"", i);<line19>    logger.info(""simpleReadUUID() Created {} entities"", i);<line27>    logger.info(""Finished simpleReadUUID()"");"
<line16>      log.error(systemException, systemException);
"<line6>      LOG.trace(""At index {}: {}"", i, currentAuthorizable.getID());<line9>        LOG.trace(""  Is member of at index {}: {}"",groupOfAuthorizableIndex,groupOfAuthorizable.getID());<line10>          LOG.trace(""    Swap at index {} (groupOfAuthorizableIndex {}):"", i, groupOfAuthorizableIndex);"
"<line8>    LOG.debug(""Graph [{}] get edges by ids: {}"", graph, stringIds);"
"<line3>        LOGGER.debug(""Stopping periodic Server Refreshes."");"
<line14>      log.error(jsonProcessingException, jsonProcessingException);
"<line11>        logger.debug(""Received an exception getting pdx type from pool {}, {}"", pool, e.getMessage(), e);"
"<line15>      log.error(""Failure to validate path "" + path, e);"
"<line4>      LOGGER.info(""Environment variable DART_POST_PROCESS_FILE not defined so the Dart code may not be""+ "" properly formatted. To define it, try `export""+ "" DART_POST_PROCESS_FILE=\""/usr/local/bin/dartfmt -w\""` (Linux/Mac)"");<line4>      LOGGER.info(""NOTE: To enable file post-processing, 'enablePostProcessFile' must be set to `true`""+ "" (--enable-post-process-file for CLI)."");<line8>      LOGGER.debug(""Serialization library not set, using default {}"", SERIALIZATION_LIBRARY_DEFAULT);<line12>      LOGGER.debug(""Date library not set, using default {}"", DATE_LIBRARY_DEFAULT);<line17>      LOGGER.debug(""Client name not set, using default {}"", DATE_LIBRARY_DEFAULT);"
"<line22>                logger.warn(""Could not delete from index series {}: {}"", id, e.getMessage());"
"<line1>    log.debug("""");<line31>      log.error(ex.getMessage(), ex);"
"<line7>        logger.error(""map entry wrong "" + e + "" to "" + c + "" old "" + b);"
"<line3>          logger.debug(""refreshLocationPrivateAsync() refreshing locations"");<line8>            logger.debug(""shouldRefreshEndpoints: true"");<line9>              logger.debug(""shouldRefreshEndpoints: can't be done in background"");<line35>            logger.debug(""shouldRefreshEndpoints: false, nothing to do."");"
"<line14>      LOG.info(""Result: "" + counter + "" = "" + row);"
"<line49>        logger.debug(""deleting temp file : "" + tempFile.getName());<line53>        logger.error(""can not delete temp file : "" + e.getMessage());"
"<line1>    log.debug(""Setting scanner factory on ShardQueryLogic: ""+ System.identityHashCode(this)+ "".setScannerFactory(""+ System.identityHashCode(scannerFactory)+ ')');"
"<line7>      log.info(""setting documents similarity exporter threshold to: "" + similarityThreshold);"
"<line5>      logger.debug(""Initialized device state for shade {} {}"",ThingStatus.OFFLINE,ThingStatusDetail.CONFIGURATION_ERROR);<line7>      logger.debug(""Initialized device state for shade {}"", ThingStatus.ONLINE);<line9>      logger.debug(""Initialized device state for shade {} {}"",ThingStatus.OFFLINE,ThingStatusDetail.BRIDGE_OFFLINE);"
"<line1>    logger.info(""==> Starting: LambdaApplication"");<line2>      logger.info(""==>  args: "" + Arrays.asList(args));"
"<line10>      LOGGER.info(SecurityMarkers.SECURITY_FAILURE,""Received token that did not pass signature verification"");<line11>      LOGGER.debug(SecurityMarkers.SECURITY_FAILURE, ""Received expired token"");<line12>      LOGGER.debug(SecurityMarkers.SECURITY_FAILURE, ""Received malformed token"");<line12>      LOGGER.debug(SecurityMarkers.SECURITY_FAILURE, e.getMessage());<line13>      LOGGER.error(SecurityMarkers.SECURITY_FAILURE, e.getMessage());"
"<line8>      LOG.debug(String.format(""table : %s, UUID: %s, commit-ts: %d, seq-no: %d, micro-ts: %d"",ar.getTableName(),ar.getTransactionUUID(),commitTimestamp,transactionSequenceNumber,microsOverride));"
<line3>      this.logger.info(fmt, i, i * 7L, i / 16.0);
"<line1>    logger.info(""Finished loading AnnotationKeys:"");<line14>      logger.info(annotationKeyPairToString(annotationKey));"
"<line1>    log.debug(""deleting StgMUmsetzStatTxt instance"");<line3>      log.debug(""delete successful"");<line4>      log.error(""delete failed"", re);"
"<line21>                    log.warn(""[{}] Failed to mark delete while trimming data ledgers: {}"",name,exception.getMessage());"
"<line14>    logger.info(""HTTP server stoped : "" + node);"
"<line8>          LOGGER.debug(Thread.currentThread().getName() + ""::uploading submitPoints "");<line14>        LOGGER.error(e.getMessage(), e);<line15>          LOGGER.debug(Thread.currentThread().getName()+ "" points:""+ uploadCounter.get()+ "" uploaded before error, now  release the lock."");<line20>      LOGGER.debug(Thread.currentThread().getName()+ "" no more points, total points:""+ uploadCounter.get()+ "" uploaded, now release the lock."");"
"<line1>    log.info(""released {}"", serviceName);"
"<line10>        log.debug(""Retrying work due to concurrent update ("" + i + ""): "" + this);<line10>        log.trace(""Concurrent update"", suppressed);"
"<line3>      log.info(""Iteration: "" + i);"
"<line3>      LOGGER.debug(""Admin supplied distance tolerance is too low. Defaulting to the minimum of {} meter"",MIN_DISTANCE_TOLERANCE_IN_METERS);"
"<line7>      logger.warn(""Warning! duplication nominal label"");"
"<line20>        log.warn(""Error updating JoalAudioSource ({})"", this.getSystemName());"
"<line18>      LOGGER.error(""StudyMetaDataService - termsPolicy() :: ERROR"", e);"
"<line6>      LOG.debug(""extract {} to {}"", file, dest);<line9>        LOG.error(ex.getMessage(), ex);"
<line25>        Log.fatal(ex);
"<line14>      logger.info(""Correctly failed to send event"", ex);"
"<line5>    LOG.debug(""Connecting..."");<line7>    LOG.debug(""Authenticating..."");<line9>    LOG.debug(""Authenticated"");"
"<line4>      ActiveMQRALogger.LOGGER.trace(""createTextMessage"" + session);"
"<line2>    log.info(""Watch closed"");"
"<line1>    log.debug("""");"
"<line12>      LOGGER.debug(""Capturing structure of table {}"", tableId);"
"<line8>      LOG.error("""", e);"
"<line2>    log.info(""-----  testProcessFlagsOrder  -----"");"
"<line13>    log.debug(""Creating new tag {}."", newTag);"
"<line7>          log.debug(""Complete topology event received"");<line12>          log.error(""Error processing complete topology event"", e);"
"<line2>    LOGGER.debug(""Inserting collection reference '"" + ref + ""' in cache"");<line8>    LOGGER.debug(""Clearing field types of collection '"" + ref + ""' from cache"");"
"<line3>      logger.trace(LogMarker.SERIALIZER_VERBOSE, ""Writing Class {}"", c);"
"<line12>        logger.error(""Error while writing to "" + urlFile, e);<line16>        logger.debug(""Test rip, found URL: "" + url);"
"<line4>    log.info(""getListInDirWithFilter(final IRODSFile irodsFile,final FilenameFilter fileNameFilter) "");<line12>    log.debug(""path for query:{}"", path);<line51>        log.debug(""more results to get for listing files, requerying"");<line60>      log.error(""query exception for  query"", e);"
<line22>      logger.error(ex);
<line5>      log.error(exception, exception);
"<line9>      LOGGER.warn(""Cannot read GeoWave build properties to show version information"", e);"
"<line1>    log.info(""...Extracting procedures"");<line5>    log.info(""...Processing procedures: "" + procedureMap.size());"
"<line5>    log.info(""Before restarting"");<line8>    log.info(""When bundles are stopped"");<line11>    log1.info(""When bundles are stopped (log1)"");<line13>    log.info(""After restarting bundles"");<line13>    log1.info(""After restarting bundles (log1)"");<line20>    log3.info(""After restarting bundles (log3)"");"
"<line30>        log.error(e.getMessage(), e);<line41>                log.warn(""Dependency for library ""+ nested_lib+ "" already exists, will not be duplicated for the nested library with""+ "" path [""+ paa.getArchivePath()+ ""]"");<line56>              log.error(e.getMessage(), e);<line58>            log.warn(""Nested analyzer of unexpected type [""+ nested_fa.getClass().getSimpleName()+ ""]"");"
"<line12>      LOGGER.error(""Error creating transfer thread"", ex);"
"<line6>          LOG.warn(""å­ç¬¦ä¸²ä¸­åç°éæ³å­ç¬¦ 0x00ï¼å·²ç»å°å"
"<line3>    LOGGER.info(""Dumping the content of the caches.\nCurrent counters:\n{}\n"", cacheInformation);"
"<line6>      log.debug(""entering 'save' method..."");"
"<line2>    LOG.info(""foo() method called with: "" + bar);"
"<line7>        logger.error(String.format(Locale.ROOT,""Convert %s to immutable for node %s failed."",cubeName,node.toNormalizeString()),ioe);"
"<line2>    log.debug(""start"");"
<line3>      LOG.error(e.getMessage());
"<line5>        logger.error(""Error stopping socket "" + socketDescription, e);"
"<line3>      logger.info(""kylin.source.hive.warehouse-dir is {}"", dbDir);<line4>      logger.warn(""kylin.source.hive.warehouse-dir is null"");"
<line3>      LOGGER.info(task);
"<line2>    log.warn(""disconnected"");"
"<line26>              Log.error(""Authentication error."", caught);<line28>              Log.error(""Invalid Token error ("" + sessionId + "")"", caught);<line30>              Log.info(""RCP Authorization Error calling ""+ action.getClass()+ "": ""+ caught.getMessage());"
"<line3>    log.info(""Start of deleteStaticRoute call"");<line8>    log.info(""End of deleteStaticRoute call"");"
"<line3>    logger.trace(""Read {}  byte, val is {}"", context, ByteUtils.formatByte(b));"
"<line15>          logger.error(String.format(""Cannot delete authority item CSID='%s' because it still has records in the""+ "" system that are referencing it."",docModel.getName()));"
"<line7>        log.info(""Score at iteration {} is {}"", iteration, score);"
"<line4>    logger.info(""totalThreadCount: "" + state.totalThreadCount);<line4>    logger.info(""threadId: "" + state.threadId);"
"<line45>          logger.debug(""Returning CREATED response for reopen case {}"", caseId);"
"<line16>        LOGGER.info(""Generated MQTT endpoint list: {}"", endpoints);<line16>        LOGGER.info(""Please set ""+ PREFIX_MQTT+ TAG_EXPOSED_MQTT_ENDPOINTS+ "" to set the correct MQTT end points."");<line17>        LOGGER.error(""Failed to create MQTT urls."", ex);"
<line17>      log.error(exception, exception);
"<line4>      log.error(""Local trip update buffer full!  Clearing!  Dropping "" + tripUpdate.getId() + "" record"");"
"<line3>      logger.trace(""addObjectField fieldName: {}"", fieldName);"
"<line4>      log.info(""Iteration: "" + i);"
"<line10>      log.error(""Exception in startHandshake"", e);"
"<line10>        logger.warn(""{}: No session available to remvoe session attribute! (this can happen in""+ "" onStructrLogin/onStructrLogout)"",getReplacement());"
"<line14>        logger.warn(""Failed to create account servlet"", e);"
"<line2>    logger.trace(""Submitting schema refresh"");"
"<line8>        logger.info(""Add crisis successful: "" + dto.getName() + "":"" + dto.getCrisisTypeId());<line13>      logger.error(""Error in /crisisType/addde."", e);"
"<line17>      LOGGER.error(""Unable to find OSLP device: {}"", deviceIdentification);"
"<line5>    logger.debug(""Session '{}' added to Web Socket manager"", session.getId());"
"<line29>        log.error(""failed to retrieve commentDocModel from relations"");<line31>        log.warn(""Could not adapt comment relation subject to a document ""+ ""model; check the service relation adapters configuration"");"
"<line18>      log.info(""No eligible text flows in document {}"", doc.getQualifiedDocId());<line24>      log.debug(""[PERF] Starting batch {} - {}"", startBatch, batchEnd);<line34>      log.debug(""[PERF] Sending batch {} - {}"", startBatch, batchEnd);<line36>      log.debug(""[PERF] Received response [{} contents] ({}ms)"",result.getContents().size(),mtProviderStopwatch);<line42>      log.warn(""Error getting confirmation backend ID for {}"", doc.getDocId());"
"<line4>      LOGGER.info(""Async Event [{}] has been created."", aEventId);<line5>      LOGGER.info(""Async Event [{}] has been started."", aEventId);<line6>      LOGGER.info(""Sync Event [{}] has been created."", sEventId);<line7>      LOGGER.info(""Sync Event [{}] has been started."", sEventId);<line10>      LOGGER.info(""Async Event [{}] has been stopped."", aEventId);<line11>      LOGGER.info(""Sync Event [{}] has been stopped."", sEventId);<line15>      LOGGER.error(e.getMessage());"
"<line5>      LOG.error(""Error loading config from storm conf {}"", topoConf);"
"<line8>        log.warn(""Found unexpected file '""+ kvp.getKey()+ ""' while asking replica '""+ Replica.getReplicaFromId(replicaId)+ ""' for file '""+ filename+ ""'"");<line9>        log.warn(""Unexpected error '""+ e+ ""' while asking ""+ ""replica '""+ Replica.getReplicaFromId(replicaId)+ ""' for file '""+ filename+ ""'"");"
"<line11>      logger.error(""Failed to set DEBUG log level due to ISolutionEngine not being available in""+ "" MicroPlatform"");"
"<line4>      log.warn(this.getClass().getName() + ""."" + methodName + ""() test took "" + runtime + ""ms"");"
"<line2>      log.debug(""fetching calendar configurations for "" + subscribeId);"
"<line11>      LOG.error(""Unknown error while trying to fetch index readers."", e);"
"<line1>    log.info(""Enter project description {}"", projectDescription);"
"<line5>      logger.warn(""Parent or child not found"");<line5>      logger.warn(""parentSysmlId: "" + parentSysmlId);<line5>      logger.warn(""childSysmlId: "" + childSysmlId);<line20>        logger.info(String.format(""%s"", LogUtil.getStackTrace(e)));<line21>        logger.warn(String.format(""%s"", LogUtil.getStackTrace(e)));"
<line8>        LOG.info(dataExample.getAbsolutePath());
"<line2>    log.info(""=== TEST for SOLUTION GENERATION of BLIND optimizer STARTED (syntax July 2015)==="");<line8>        log.error(""There was an error in the check of correctness. Solution was: "" + arrayDam[damnum]);<line14>    log.info(""=== TEST for SOLUTION GENERATION of BLIND optimizer FINISEHD ==="");"
"<line11>      logger.error(""Error closing tag"", t);"
"<line7>          logger.error(""Object validation error "" + reader.GetObjectPID() + "": "" + e.getMessage());"
"<line9>        LOGGER.error(""Only HLS 5, LLS 1 and public (LLS 0) connections are currently supported"");"
"<line7>      LOG.warn(""Exception checking for exists on: "" + key);"
"<line25>          Throwable err = fut.error();<line34>              log.info(indexStatStr(resStat));<line35>              log.error(""Error when trying to print index build/rebuild statistics [cacheName=""+ cctx.cache().name()+ "", grpName=""+ cctx.group().name()+ ""]"",e);"
"<line3>      LOG.warn(""{}: Ignoring {}"", server.getId(), reply);"
"<line1>    LOG.debug(""{}: {}"", FileUtils.fileName(getFile()), freeList.toString());"
"<line3>    LOG.debug(""SuggestComponent prepare with : "" + params);"
"<line6>      LOGGER.error(""Unable to deserialize '{}'"", message, e);"
"<line4>      LOGGER.error(getChosenSQLDriver() + "" class not found."", e);"
"<line4>    log.info(""startFolderEvent({})[{}] {}"", session, op, file);"
"<line63>      log.debug(""Failed to launch process used to create '"" + CLASSES_LIST_FILE_NAME + ""'."", e);"
"<line4>        log.debug(""unmapSession(id={}): {}"", sessionId, ioSession);"
"<line39>    logger.trace(""add zipped media package with workflow definition id: {} and workflow instance id: {}"",wdID,wiID);<line42>      logger.warn(""Delaying ingest because we have exceeded the maximum number of ingests this server is""+ "" setup to do concurrently."");"
"<line1>    logger.debug(""Groups"");"
"<line1>    logger.info(""deactivate {}..."",componentContext.getProperties().get(ConfigurationService.KURA_SERVICE_PID));<line5>        logger.warn(""Cannot publish disconnect certificate"");"
"<line2>    LOG.info(""Clone existing project {} to a new project."", idProject);"
"<line7>          LOG.info(""Loading includes from resource: {}"", include.getFile());<line9>          LOG.info(""Loading includes from file: {}"", include.getFile());<line23>                LOG.warn(""Ignoring attempt to set topology config property '{}' with override == false"",key);"
"<line53>      logger.info(""Forwarded the prediction model of ""+ numForwarded+ "" rows. [totalErrors=""+ cvState.getTotalErrors()+ "", lastLosses=""+ cvState.getCumulativeLoss()+ "", #trainingExamples=""+ count+ ""]"");"
"<line2>    log.info(""Starting JDBC Sink task"");"
<line10>      LOG.info(entityInfo.toString());
"<line16>        log.info(""Waiting until Address space configuration will be applied. Current: {}"",addressSpace.getAnnotation(AnnotationKeys.APPLIED_CONFIGURATION));<line25>    log.info(""Address space configuration for {}/{} successfully applied"",addressSpace.getMetadata().getNamespace(),addressSpace.getMetadata().getName());"
"<line1>    logger.trace(""evaluateState() called."");<line8>    logger.debug(""evaluateState() finished {}."", (success ? ""successfully"" : ""with failure""));"
"<line38>      LOG.debug(""certificate validation failed"", e);"
"<line2>    LOGGER.debug(""Start"");"
"<line8>                LOGGER.debug(""Canceling of the job {} done with status code {} "",id,res.result().statusCode());<line9>                LOGGER.error(""Canceling of job {} failed with response code {}"",id,res.result().statusCode(),res.cause());"
"<line3>      LOGGER.debug(""Writing: {}"", outputPath);"
"<line15>        log.info(""detach {}"", controllerList.getSelectedItem());"
"<line2>    LOG.info(""Worker "" + name + "" was interrupted..."");"
"<line11>      LOG.error("""", e);"
"<line3>        logger.warn(""Could not properly delete element file: {}"", elementFile);<line5>      logger.warn(""Tried to delete non-existent element file. Perhaps was already deleted?: {}"",elementFile);<line10>          logger.warn(""Could not properly delete element directory: {}"", elementDir);<line12>        logger.warn(""Element directory was not empty after deleting element. Skipping deletion: {}"",elementDir);<line14>      logger.warn(""Element directory did not exist when trying to delete it: {}"", elementDir);<line19>          logger.warn(""Could not properly delete mediapackage directory: {}"", mediapackageDir);<line21>        logger.debug(""Mediapackage directory was not empty after deleting element. Skipping deletion: {}"",mediapackageDir);<line23>      logger.warn(""Mediapackage directory did not exist when trying to delete it: {}"", mediapackageDir);"
<line5>    logger.info(Messages.STAGING_APP, app.getName());
"<line3>      LOG.debug(""Extra value is null, throw away it."");"
"<line44>        logger.warn(""No message integrity check"");"
"<line1>    log.info(""{} detaching {}"", getName(), device.getName());<line2>      log.info(""device {} not attached"", device.getName());<line7>    log.info(""detaching device {}"", device.getName());"
"<line4>    LOG.debug(""[{}] Creating new schedule for the control connection"", logPrefix);"
<line8>      logger.warn(null, t);<line10>    logger.warn(message.text(getLoggingLocale(), params), t);
"<line16>      log.debug(""Determined compilation unit "" + def_ctx + "" for qname ["" + qname + ""]"");<line43>          log.error(""Cannot delete file ["" + file + ""]: "" + e.getMessage());<line46>      log.error(""Error: "" + iae.getMessage(), iae);<line54>      log.error(""Error: "" + e.getMessage(), e);"
"<line3>      logger.error(""getListProperty(): argument 'name' must be non-null"");"
"<line1>    logger.debug(""vcenterId: "" + vcenterId);<line16>      logger.error(""Failed to retrieve value from sequence "" + seqName, e);"
"<line19>        LOG.info(""Source is ""+ srcContentSummary.getLength()+ "" bytes. (MAX: ""+ conf.getLongVar(HiveConf.ConfVars.HIVE_EXEC_COPYFILE_MAXSIZE)+ "")"");<line19>        LOG.info(""Source is ""+ srcContentSummary.getFileCount()+ "" files. (MAX: ""+ conf.getLongVar(HiveConf.ConfVars.HIVE_EXEC_COPYFILE_MAXNUMFILES)+ "")"");<line19>        LOG.info(""Launch distributed copy (distcp) job."");"
"<line9>      logger.error(""Error on creating column family, ignoring"", e);"
"<line42>                LOG.debug(""Successfully wrote request to fence ledger and read entry: ""+ entryId+ "" ledger-id: ""+ ledgerId+ "" bookie: ""+ channel.getRemoteAddress());"
<line9>      Logger.error(this.getClass().getName(), ex.getMessage(), ex);
"<line7>        LOG.error(""The passwords entered do not match"");<line10>        LOG.error(""The primary password entered is empty"");<line12>        LOG.error(""The password has been reset"");"
"<line29>          logger.warn(""***** "" + e.getMessage() + "" *****"");"
"<line4>      this.logger.warn(""level.dat for world '{}' could not be saved: "", this.levelName, cause);"
"<line17>        LOGGER.error(""Enclosing trace does not match split point. Found: {} expected: {}"",enclosingTrace.getTraceId(),enclosingTrace.getTraceId());"
"<line5>    log.debug("""");"
"<line36>        LOGGER.debug(""Unable to get X500 Name"", e);"
"<line11>      LOG.warn(""Aux path not found. Skipping: "" + metaClient.getMetaAuxiliaryPath());<line23>      LOG.info(""Deleting instant ""+ deleteInstant+ "" in auxiliary meta path ""+ metaClient.getMetaAuxiliaryPath());<line26>        LOG.info(""Deleted instant file in auxiliary metapath : "" + metaFile);"
"<line2>    logger.debug(""readSync request: {}"", attribute);<line6>      logger.debug(""readSync interrupted"");<line8>      logger.debug(""readSync exception "", e);"
"<line13>    LOG.debug(""Processing {}"", cmd);"
"<line3>    log.info(""Subscribing to notifications: {}"", nfConsumer.getTopic());"
"<line4>      logger.error(""Received publication for not existing subscription callback with subscriptionId {}"",subscriptionId);"
"<line5>      LOG.error(""add SERVER_FAILED event for request failed, "", e);"
"<line27>    LOGGER.trace(""updateTextSections fertig nach {} ms. Entfernte/Neue TextSections: {} / {}"",Integer.valueOf((int) (System.currentTimeMillis() - startTime)),invalidTextSections.size(),newTextSections.size());"
"<line70>      log.error(""Error while communicating with the ForgeAPI"", t);"
"<line1>    LOGGER.debug(""Retriving Feature table info: "" + serviceUrl.toString());<line11>      LOGGER.debug(""HttpGet "" + fullUrl.toString() + "" number of params: "" + params.size());<line12>      LOGGER.debug(""Response code: "" + response.getResponseCode() + ""\n\t"" + response.getBody());<line15>        LOGGER.debug(""    tokenJSON: "" + responseJSON);<line25>        LOGGER.error(errorMsg);"
"<line9>    LOGGER.info(""Set Device Verification Key Request received from organisation: {} for device: {} with""+ "" message priority: {}."",organisationIdentification,request.getDeviceIdentification(),messagePriority);"
"<line3>    log.debug(""Build scroll with query: {}"", query);"
"<line40>    LOG.debug(""{}"", sink.collectedTuples);"
"<line1>    log.error(""{}: could not find readable resource {} for the Menubars-Layout."",WebAppContextPath.class.getName(),menubarsLayoutXmlResource,cause);"
"<line16>      logger.warn(""Ignored child URL: {} in {}"", attrValue, url);<line19>      logger.debug(""{} -> {}"", attrValue, u);<line22>        logger.debug(""Add Child: {}"", u);<line25>      logger.debug(""Skip Child: {}"", u);"
<line13>      logger.debug(e.getMessage());
<line10>        log.debug(portalException, portalException);
"<line3>    logger.debug(""Send CEA message"");"
"<line45>      logger.debug(""The command '{}' is not supported by this handler."", command.getClass());"
"<line7>        LOG.warn(""Dataset homepageURL was invalid: {}"", Strings.nullToEmpty(homepageUrl));"
<line33>      log.error(systemException, systemException);
"<line5>      logger.trace(LogMarker.SERIALIZER_VERBOSE, ""basicWriteObject: {}"", o);<line25>        logger.trace(LogMarker.SERIALIZER_VERBOSE, ""Writing DataSerializable: {}"", o);<line45>        logger.trace(LogMarker.SERIALIZER_ANNOUNCE_TYPE_WRITTEN_VERBOSE,""DataSerializer Serializing an instance of {}"",o.getClass().getName());"
"<line13>                    LOGGER.info(""Verifying that consumer name is created with 'kafka-bridge-consumer-' plus""+ "" random hashcode"");<line15>                    LOGGER.info(""Response code from the Bridge is "" + response.statusCode());<line18>                    LOGGER.info(""Consumer instance of the consumer is "" + consumerInstanceId);"
"<line13>          log.trace(""Condition succeeded"");<line18>          log.trace(""Condition test failed"");"
"<line5>      log.error(""Could no de-serialize the following json "" + json, e);"
"<line3>      logger.error(""Found configuration for Notification Service with no 'id' element; this service cannot""+ "" be referenced so it will not be loaded"");<line6>    logger.debug(""Loading Notification Service with ID {}"", serviceId);<line8>      logger.error(""Found configuration for Notification Service with no 'class' element; Service ID is""+ "" '{}'. This service annot be loaded"",serviceId);<line15>      logger.error(""Found configuration for Notification Service with ID '{}' and Class '{}' but could not""+ "" load class."",serviceId,className);<line19>      logger.error(""Found configuration for Notification Service with ID '{}' and Class '{}' but class is""+ "" not a Notification Service."",serviceId,className);<line25>      logger.error(""Found configuration for Notification Service with ID '{}' and Class '{}' but could not""+ "" instantiate Notification Service."",serviceId,className);<line33>        logger.warn(""Found configuration for Notification Service with ID '{}' that has property value""+ "" configured but no name for the property."",serviceId);<line66>      logger.error(""Failed to load Notification Service with ID '{}'"", serviceId);<line66>      logger.error("""", e);"
"<line1>    LOG.debug(""reading tenant info [id: {}]"", tenantId);"
"<line6>        LOG.error(""Unable to determine host name for IP: "" + host + "", setting to default pool"", e1);"
"<line16>        logger.debug(""Command added to Queue {} -> {} (Device: {} token: {} Queue: {}).{}{}"",fullCommand.toString(),ip,Utils.getHex(deviceId),tokenText,concurrentLinkedQueue.size(),cloudServer.isBlank() ? """" : "" Send via cloudserver: "",cloudServer);<line22>      logger.warn(""Send command '{}' with parameters {} -> {} (Device: {}) gave error {}"",command,params,ip,Utils.getHex(deviceId),e.getMessage());"
<line56>      log.error(e.getMessage(), e);
<line33>      log.error(e.getMessage());
"<line2>    LOG.info(""Getting all"");"
"<line16>    logger.debug(String.format(""vm[uuid:%s] is started .."", self.getUuid()));"
"<line5>    log.info(Color.GREEN + ""Stop_2 : empty column stop_id"" + Color.NORMAL);"
"<line2>      logger.debug(""Failed to cleanup the input "" + origInput);"
"<line4>        log.error(""@Type does not match. Expected {} but found {}"", typeVal, doc.getType());<line21>        log.error(""Unable to instantiate class {}"", clazz.getSimpleName(), e);<line34>            log.error(""Annotated field {} can not be abstract"", f.getName());<line40>              log.error(""Unable to instantiate collection field {} of type [{}]"",f.getName(),f.getType(),e);<line57>      log.error(""Unable to access pojo field"", e);"
"<line6>      logger.debug(""Failed to parse path '{}', returning null"", path, e);"
"<line8>      logger.debug(""Channel {} unable to process command {}"", CHANNEL_BRIGHTNESS, command);"
"<line12>          LOG.warn(""Seems like the LDAP service ({}) has already been unbound."", getPort());<line16>          LOG.info(""Unbind of an LDAP service ({}) is complete."", getPort());<line16>          LOG.info(""Sending notice of disconnect to existing clients sessions."");<line33>      LOG.warn(""Failed to sent NoD."", e);<line35>    LOG.info(""Ldap service stopped."");"
"<line5>    LOGGER.debug(""Trying to acquire token for table: {}"", tableName);"
"<line55>              log.warn(""KBFolderPersistenceImpl.fetchByG_P_UT(long, long, String, boolean) with""+ "" parameters (""+ StringUtil.merge(finderArgs)+ "") yields a result set with more than 1 result. This violates the logical""+ "" unique restriction. There is no order guarantee on which result is""+ "" returned by this finder."");"
"<line4>        log.debug(""message received is of ObjectMessage type."");<line6>        log.debug(""found action "" + userActionRequest.getActionKey());<line8>        log.debug(""message received is not of ObjectMessage type."");<line10>      log.error(e);"
<line3>      logger.debug(objToLog);
"<line17>          LOGGER.debug(""Executed clear datasource SQL statement: {}"", sql);"
"<line5>    log.info(Color.GREEN + ""Stop_2_3 : missing column stop_lat"" + Color.NORMAL);"
"<line19>                          LOG.info(""Received: {}"", destinationInfo);"
"<line5>      log.debug(""Received activation response [requestId=""+ msg.getRequestId()+ "", nodeId=""+ nodeId+ ""]"");"
"<line7>          log.debug(""IOException: "" + e.getMessage());<line13>        log.error(""Cannot find Maven application directory. Either specify 'maven.home' system property,""+ "" or M2_HOME environment variable."");"
"<line2>      logger.trace(""Executing refresh job"");"
"<line7>        log.error(""Error updating aggregate host stats for SpawnBalancer: {}"", e.getMessage(), e);<line8>        log.error(""Error updating aggregate host stats for SpawnBalancer: {}"", e.getMessage());"
"<line2>    LOG.debug(""Providing password for ssh authentication of user '{}'"", config.getUsername());"
"<line5>        logger.error(""Error in closing data source "" + sourceHolder, e);"
"<line2>    logger.info(""Called committee processor"");<line11>      logger.info(""Processing ""+ chamber+ ""committees for s""+ sessionYear+ "" y""+ year+ ""\t""+ legDataFragment.getPublishedDateTime());"
"<line3>      ActiveMQRALogger.LOGGER.trace(""getMapNames()"");"
"<line5>        LOG.debug(""Failed to close ledger {} : "", ledgerId, re);"
"<line1>    log.debug(""attaching clean StgNZielobjekt instance"");<line3>      log.debug(""attach successful"");<line4>      log.error(""attach failed"", re);"
"<line35>      log.error(""query table error,{}"", e);"
"<line22>            logger.warn(""{} while creating {} instance: {}"",e.getClass().getSimpleName(),state.getClass().getSimpleName(),e.getMessage());<line28>        logger.debug(""Received update of a not accepted type ({}) for item {}"",newState.getClass().getSimpleName(),itemName);<line30>      logger.debug(""Received update for non-existing item: {}"", e.getMessage());"
"<line11>      logger.debug(""Cannot send command '{}' - socket channel was closed"", command);<line12>      logger.debug(""Sending Command: '{}'"", command);"
"<line5>    log.info(Color.GREEN + ""Route_8 : route_long_name includes route_short_name"" + Color.NORMAL);"
<line4>      logger.warn(e.getMessage());
"<line7>      logger.debug(""Got response mana type from player: "" + getId());"
"<line2>    log.info(""Reloading configuration"");"
"<line2>    logger.debug(""deleteSubscriptionResponse started ..."");<line17>      logger.debug(ex.getMessage(), ex);"
"<line21>      log.error(String.format(""%s. Script inum: %s"", e.getMessage(), customScript.getInum()), e);<line33>        log.warn("" Update the script's init method to init(self, customScript, configurationAttributes)"",customScript.getName());<line35>      log.error(""Failed to initialize custom script: '{}'"", ex, customScript.getName());"
"<line10>      LOG.trace(""[{}][{}] render (AJAX) - was already rendered in this cycle - skipping"",getMarkupId(),vis.getMarkupId());<line25>      LOG.trace(""[{}][{}] render (AJAX) - deferred rendering will trigger - skipping"",getMarkupId(),vis.getMarkupId());<line27>    LOG.trace(""[{}][{}] render (AJAX)"", getMarkupId(), vis.getMarkupId());<line52>      LOG.error(""Unable to load data"", e);"
"<line5>      LOGGER.error(""Could not read skin from input: "", e);"
"<line3>      log.debug(""writeBuffer({}) writing {} bytes"", this, buffer.available());"
"<line50>      log.error(""Error while fetching column by id {}, Caused by {}."", pKeyColumnValue, e);"
"<line2>    LOG.info(""Starting Server using kerberos credential"");"
"<line41>            log.warn(Messages.getString(""ProcessorUtilities.nullProcess""));"
"<line2>    LOG.info().$(""started maxConnections"").$();<line69>                LOG.info().$(""closed [fd="").$(fd).$(']').$();"
<line10>      log.error(exception, exception);
"<line3>    LOG.debug(""Creating '{}'..."", FileUtils.fileName(file));<line13>      LOG.error(""Failed to initialize structural index: {}"", e.getMessage(), e);"
"<line18>        logger.error(String.format(""Error while adding key=%s to As list=%s"", key, Arrays.toString(asList)));"
"<line43>          LOG.error(""impossible to create a java object from Bin:""+ field.getName()+ "" and type:""+ field.getType()+ "" and value:""+ t+ ""; recordReceived:""+ currentBin);"
"<line7>        logger.debug(""Number of Mbob deleted: {}"", deletedCount);<line8>        logger.debug(""Nothing to delete"");"
"<line11>    log.info(""Join query returned: ""+ sampleListener.getLastEvent().get(""key1"")+ "" and ""+ sampleListener.getLastEvent().get(""key2""));"
"<line5>          LOGGER.trace(""filter = {} matches {}"", importPAXWicketAPI);<line7>          LOGGER.trace(""filter = {} not matches {}"", importPAXWicketAPI);<line9>        LOGGER.warn(""can't parse filter expression: {}"", filterString);"
<line12>        logger.info(query);
<line15>      log.error(systemException, systemException);
"<line1>    LOG.debug(""setPrimaryFieldPos()"");"
"<line27>      LOGGER.debug(""Temporary file deleted: {}"", tempDir.delete());"
"<line13>        LOG.error(""Error deleting user "" + username, e);"
"<line47>        log.debug(""latencyMilis={} -- lastLocalColor={} -- lastRemoteColor={} -- ""+ ""lastLocalColorChangeTime={} -- lastRemoteColorChangeTime={} -- ""+ ""lastLocalColorChangeTimeAbsolute={} -- lastRemoteColorChangeTimeAbsolute={}"",latencyMilis,lastLocalColor,lastRemoteColor,formater.format(lastLocalColorChangeTime),formater.format(lastRemoteColorChangeTime),parsedLocaltime,parsedRemotetime);<line48>          log.debug(""--> Latency adquired ({} ms)"", latencyMilis);<line66>              log.warn(latencyException.getMessage());<line75>      log.debug(""Finished LatencyController thread due to Interrupted Exception"");"
"<line2>    logger.info(""SchedulerManager:schedule: Started scheduler job for cache refresh with ttl in sec =""+ TTL);"
"<line8>      LOGGER.trace(""Change arrived for decoding {}"", change);<line12>      LOGGER.trace(""Empty change arrived"");"
"<line9>    logger.trace(""none"");<line11>    logger.trace(HELLO);"
<line26>            log.warn(sb.toString());
"<line25>          log.info(""closed the moved namespace info, namespace is {}, old zkClusterKey is {}"",namespace,zkClusterKey);"
"<line2>    log.info(""Start supplying threads"");<line3>      log.info(""ThreadStarter will finish, because target concurrency less than 0."");<line9>        log.debug(""Concurrency factual/expected: ""+ concurrTG.getConcurrency()+ ""/""+ getPlannedConcurrency(isDebugEnabled));<line16>    log.info(""Done supplying threads"");"
"<line5>      logger.error(""Error while loading content vo : id {}"", id, t);"
"<line8>          LOG.info(""Listener: ""+ listener+ "" on path: ""+ propertyKey.getPath()+ "" already exists. skip add"");<line21>      LOG.info(""Added listener: ""+ listener+ "" for type: ""+ type+ "" to path: ""+ newHandler.getPath());"
"<line14>      LOGGER.info(""Caught exception while initializing caches"", e);"
"<line13>      log.warn(""[ClusterStatusHolder-{}] receive the expired heartbeat from {}, serverTime: {},""+ "" heartTime: {}"",appName,heartbeat.getWorkerAddress(),System.currentTimeMillis(),heartbeat.getHeartbeatTime());"
"<line7>      logger.debug(""Source of NiFi flow has opted to yield for {} milliseconds. Will pause dataflow until""+ "" that time period has elapsed."",yieldMillis);<line13>    logger.debug(""Triggering dataflow"");<line18>      logger.warn(""Dataflow timed out after waiting {} milliseconds. Will cancel the execution."",timeoutMillis);<line23>      logger.error(""Dataflow {} failed to execute properly"",dataflowName,triggerResult.getFailureCause().orElse(null));<line45>    logger.debug(""Returning {} records from poll() method (took {} nanos to run dataflow)"",sourceRecords.size(),nanos);"
"<line10>        log.debug(""Metric=[%s] has not been configured to be emitted to opentsdb"",((ServiceMetricEvent) event).getMetric());"
"<line9>      logger.debug(""Unable to remove shutdown hook for {}, shutdown already in progress"", serviceName, e);<line10>      logger.warn(""Exception while un-registering {}'s shutdown hook."", serviceName, t);"
"<line7>      log.debug(String.format(""Login attempt succeeded for remote IP: %s"", authDetails.getIp()));<line16>      log.debug(String.format(""OIDC login attempt succeeded for remote IP: %s"", remoteAddress));"
<line6>      log.debug(e.getMessage(), e);<line8>      log.error(e.getMessage(), e);
<line9>          log.debug(noSuchFolderException, noSuchFolderException);
<line24>    LOGGER.info(ArrayConverter.bytesToHexString(message.getRandom().getValue()));
<line6>        logger.error(e, e);<line8>      CompletionAdaptor.logger.error(e, e);
"<line5>    LOGGER.info(""Rack object returned to client : "" + updatedRack.toJsonString());"
"<line18>        LOG.trace(""Found {} in cache for {}, row='{}', locateType={}, replicaId={}"",loc,tableName,Bytes.toStringBinary(row),RegionLocateType.BEFORE,replicaId);"
"<line3>      LOGGER.info(""Using JAVA_OPTS from conf file (jvm.args)"");<line6>        LOGGER.info(""Using JAVA_OPTS from environment"");<line7>        LOGGER.info(""Using default JAVA_OPTS"");<line21>    LOGGER.info(""Setting JAVA_OPTS to "" + jvmargs);"
"<line1>    LOG.debug(""Start new backup exclusive operation"");"
"<line5>      log.debug("".sendEventJson Processing event "" + json);"
"<line6>    LOGGER.debug(""Created user."");<line8>    LOGGER.debug(""Created admin."");<line10>    LOGGER.debug(""Created user"");<line12>    LOGGER.debug(""Created admin"");<line14>    LOGGER.debug(""Created permission view for role user"");<line16>    LOGGER.debug(""Created permission user:* for role admin"");<line18>    LOGGER.debug(""Assigned user role user"");<line20>    LOGGER.debug(""Assigned admin role admin"");"
"<line5>      log.debug(""storing blob "" + digest + "" to S3"");<line10>        log.debug(""blob "" + digest + "" is already in S3"");<line43>          log.debug(""stored blob "" + digest + "" to S3 in "" + dtms + ""ms"");"
"<line2>    LOGGER.debug(""Serializing RSAClientKeyExchangeMessage"");"
"<line5>        log.error(""OffHeapNamespaceExtractionCacheManager.disposeCache() was not called, disposed""+ "" resources by the JVM"");<line7>          log.error(t, ""Error while deleting key %s from MapDb"", mapDbKey);"
<line21>      log.error(systemException, systemException);
"<line2>    Log.debug(""Test"");"
"<line1>    LOG.info(""Task '"" + taskid.getTaskID().toString() + ""' has failed."");"
"<line6>      log.warn(""Unexpected authorizable or definition for property rep:impersonators"");"
<line15>      log.error(exception, exception);
"<line16>        log.error(""JanusGraphManager should be instantiated on this server, but it is not. ""+ ""Please restart with proper server settings. ""+ ""As a result, we could not evict graph {} from the cache."",graphName);<line24>          log.debug(""Graph {} has been removed from the JanusGraphManager graph cache."", graphName);<line27>          log.debug(""Sent {}: evictionID={} originID={}"",MgmtLogType.CACHED_TYPE_EVICTION_ACK,evictionId,originId);<line28>          log.warn(""System log has already shut down. Did not sent {}: evictionID={} originID={}"",MgmtLogType.CACHED_TYPE_EVICTION_ACK,evictionId,originId);<line32>        log.error(""Evicted [{}] from cache but waiting too long for transactions to close. Stale""+ "" transaction alert on: {}"",getId(),openTx);<line37>        log.error(""Interrupted eviction ack thread for "" + getId(), e);"
<line4>      LOG.debug(validationResult.get());
<line7>      logger.error(e, true);
"<line7>      LOG.debug(""Updated layout ID for table {} to {}."", tableURI, layoutID);"
"<line9>            logger.debug(""Current position saved: "" + pos);"
"<line9>                    logger.debug(""Reliability loss with policy of reconnect and membership thread doing""+ "" reconnect"");<line21>                    logger.fatal(""Unexpected exception:"", e);<line39>      logger.fatal(""Unexpected exception:"", e);"
"<line12>      logger.debug(""Pushing documents is disabled: integrator.send.documents.disabled = false"");<line14>    logger.debug(""pushing demographicDocuments edited after ""+ lastDataUpdated+ "" for facilityId:""+ facility.getId()+ "", demographicId:""+ demographicId);"
"<line6>      LOGGER.warn(""{} Error exporting {} \t{}"",Thread.currentThread().getName(),evidence.getPath(),e.toString());"
"<line2>    logger.debug(""getAuthorizationIntraCloudEntries started..."");<line15>      logger.debug(ex.getMessage(), ex);"
"<line26>      log.info(""PageMemory tracking enabled by system property."");"
"<line3>      logger.info(String.format(""get management IP[%s] from Java property[management.server.ip]"", ip));<line7>      logger.info(String.format(""get management IP[%s] from environment variable[ZSTACK_MANAGEMENT_SERVER_IP]"", ip));<line42>    logger.info(String.format(""get management IP[%s] from default route[/sbin/ip route]"", ip));"
"<line2>      logger.debug(String.format(""execUpdate: %s"", query));<line7>        logger.debug(String.format(""%s"", LogUtil.getStackTrace(e)));"
"<line12>      LOG.error(""ProviderException requesting version history for "" + pageName, e);"
"<line1>    logger.debug(""build"");"
"<line14>        logger.error(""Failed to perform tasks when enumerator was finished"", e);"
"<line11>        log.warn(""Unexpected uiSource type '"" + child.getClass().getName() + ""'."");"
"<line3>          LOG.warn(""Failover triggered by {}"", user.getId());"
"<line6>    LOGGER.info(""CreateDatasetVersion Response : \n"" + datasetVersion2);<line24>    LOGGER.info(""CreateDatasetVersion Response : \n"" + datasetVersion);<line32>    LOGGER.info(""DeleteDatasetVersion deleted successfully"");<line32>    LOGGER.info(deleteDatasetVersionResponse.toString());"
"<line2>    logger.debug(""delete MoodleUserProviderInstance for pid="" + pid);<line9>        logger.warn(""Unable to unregister mbean for pid='{}': {}"", pid, e.getMessage());"
"<line6>      log.warn(""Error occurred while sleeping"", e);"
"<line4>      LOGGER.warn(MessageFormat.format(this.getActionExecution().getAction().getType()+ "" does not accept {0} as type for sourceFileName"",sourceFileName.getClass()));"
"<line5>      log.info(""The user for OTP authN can not be found: "" + subject, e);<line14>        log.info(""Code provided by {} is invalid"", subject);<line23>      log.warn(""Error during TOTP verification for "" + subject, e);"
"<line6>      LOG.info(""No bulk extract support for : {}"", edOrgId);<line10>        LOG.info(""No bulk extract file found for : {}"", edOrgId);"
"<line8>      LOG.error(""exception while shutting down ldap"", e);"
"<line9>        log.debug(""Created a folder ["" + targetPath.toString() + ""]"");<line13>        log.debug(""Folder ["" + targetPath.toString() + ""] already exists"");<line16>          log.debug(""Will remove the folder ["" + targetPath.toString() + ""]"");<line20>          log.debug(""Recreated folder ["" + targetPath.toString() + ""]"");"
"<line14>      log.error(""Error handling url: "" + urlString, e);"
"<line23>    LOG.info(""Command to launch container for PS is : "" + mergedCommand);"
"<line13>        logger.info(String.format(""upgrading %s repository to version %s"", repositoryName, version));<line16>            logger.info(String.format(""Running %s"", file.getName()));"
"<line12>    LOG.error(""submitAndSchedule is not supported on Server.Please run your operation on Prism "");"
"<line5>      log.debug(""Person "" + ldapPerson.getAccountId() + "" was updated - updating cache"");<line10>      log.debug(""Person ""+ ldapPerson.getAccountId()+ "" display name was updated - updating display name everywhere"");"
"<line1>    log.debug(""persisting StgG20Scl instance"");<line3>      log.debug(""persist successful"");<line4>      log.error(""persist failed"", re);"
"<line7>      LOG.warn(""Interrupted while setting up mocks"", e);"
"<line7>      LOG.error(""Error while removing object"", t);"
"<line6>      logger.debug(""Path: "" + path + "" is already relative path."");"
"<line15>          LOG.error(""While generating new OIDC JWKS"", ge);<line17>        LOG.error(""While reading OIDC JWKS"", e);"
<line2>    LOG.info()
"<line23>            logger.trace(LogMarker.BRIDGE_SERVER_VERBOSE, ""{}: Adding message to queue: {}"", this, object);<line27>          logger.debug(""{}: This queue has already seen this event.  The highest sequence number in the""+ "" queue for {} is {}, but this event's sequence number is {}"",this.regionName,ti,dace.lastDispatchedSequenceId,sequenceID);<line44>          logger.trace(LogMarker.BRIDGE_SERVER_VERBOSE, ""{}: Adding message to queue: {}"", this, object);"
"<line2>    LOG.info(""Trying to start the FHIR container"");<line4>    LOG.info(""FHIR instance running at {}"", getServiceBaseURL());"
"<line48>      LOGGER.error(""Error accessing database"", sqle);"
"<line1>    LOG.debug(""Trying to start ad-hoc backup for Veeam job: "" + jobId);<line5>      LOG.error(""Failed to list Veeam jobs due to:"", e);"
"<line25>      logger.trace(""Id of new replica set {} is {}."", rs, newReplicaSetID);<line32>      logger.error(""Error when create replicaSet "" + rs, e);"
"<line15>      logger.debug(""New application orgName {} orgAppName {} id {} "",orgName,name,applicationId.toString());"
"<line2>    LOG.warn(""Exception raised when handling request to {}"", url, cause);"
"<line5>    log.info(""{}: Deactivating ConsumeBenchWorker."", id);"
<line6>      LOGGER.error(ex.getMessage(), ex);
"<line2>    log.error(""Error during report execution"", t);"
"<line6>    LOG.info(""Configuring CommitProcessor with {} worker threads."",numWorkerThreads > 0 ? numWorkerThreads : ""no"");"
"<line6>      logger.debug(MessageFormat.format(""Added message: {0}"", auditMessage));"
"<line2>      logger.debug(""Removing execution id: {}"", event.getExecId());"
"<line6>      logger.debug(""Executing SQL batch update of {} statements"", sql.length);"
"<line14>      log.trace(""IOException while creating jmxremote.access file:"", e);"
"<line46>        LOG.debug(ignore, ""Cannot close columns result set"");"
"<line1>    LOGGER.debug(""Updating Expansion Properties."");"
<line38>      logger.warn(errMsg, e);
"<line2>    LOG.info(""{} - {} (base)"",formatter.print(baseSlice.getStart()),formatter.print(baseSlice.getEnd()));<line3>      LOG.info(""{} - {}"", formatter.print(slice.getStart()), formatter.print(slice.getEnd()));"
"<line4>      log.trace(""Executing python 'getLogouExternalUrl' authenticator method"");<line10>      log.error(ex.getMessage(), ex);"
"<line7>        LOGGER.info(""User group with uri "" + uri + "" not found."");"
"<line16>        logger.warn(""Query {} interrupted"", sourceInfo);"
"<line10>    log.info(""test entity child group id gives: "" + result);"
"<line7>      LOG.error(""Some of the fields in the query {} are not one of the valid fields {}."",fields,gdqt.getFields().getFields());"
"<line3>      LOGGER.debug(""Implicit declaration of property {}."", key);"
"<line16>      logger.warn(getLogPrefix() + backlogDroppedMsg);<line29>        logger.info(getLogPrefix()+ ""backlog is dropped for all targets, no packets will held in the backlog until""+ "" some of the targets will recover"");"
"<line3>    log.info(""Role  {} dropped successfully "", roleName);"
"<line25>      LOGGER.error(""An exception occurred publishing metrics to Parfait"", ex);"
"<line2>    LOG.info(""Getting the latest statistics timeline map"");<line14>    LOG.info(""Latest statistics timeline map is set"");"
"<line1>    logger.debug(""Create copy of track {}"", track);"
"<line13>    logger.debug(""oxAuth redirection Url: '{}'"", redirectionUrl);"
"<line4>      logger.info(""Transformed bundle {} with location {} to be handled by the DX protocol handler under""+ "" new location {}"",getDisplayName(bundle),bundle.getLocation(),newLocation);<line9>      logger.error(msg, e);"
"<line4>      logger.debug(""Could not load the TAE peptide parameter data file"");<line29>    logger.debug(""Loaded "" + taeParams.size() + "" TAE parameters for amino acids"");"
"<line21>      LOG.error(""Unable to retrieve results"", e);"
"<line4>      LOGGER.error(""Exception creating JAXBContext for templates: {}"", e.getLocalizedMessage(), e);"
"<line7>    logger.debug(""processing request to /settings"");<line28>        logger.warn(uri + "" atom uri problem"", e);"
"<line9>                  LOG.trace(""build image callback {}"", item);"
<line13>      logger.error(e.getMessage(), e);
"<line2>      logger.debug(""Permit join to {} invalid period of {} seconds."", destination, duration);<line4>    logger.debug(""Permit join to {} for {} seconds."", destination, duration);"
<line4>    logger.warn(format, param);
<line6>      log.warn(ade.getMessage());
"<line13>      LOGGER.debug(""Successfully tested connection for {} with user '{}'"",OracleConnection.connectionString(config),connection.username());<line14>      LOGGER.info(""Failed testing connection for {} with user '{}'"",config.withMaskedPasswords(),userValue,e);"
"<line16>      LOG.info(""Native decoding is enabled for ""+ inboundName+ "". Inbound deserialization done at the broker."");<line17>      LOG.info(""Native decoding is disabled for ""+ inboundName+ "". Inbound message conversion done by Spring Cloud Stream."");"
"<line14>          LOG.warn(""invalidate {} connection and {} sessions due to '{}'"",m_impl != null ? 1 : 0,m_sessionWrappers.size(),e.getMessage());<line17>            LOG.info(""invalidating session {} of {}"", i, n);<line22>              LOG.info(""invalidating connection"");<line23>              LOG.info(""connection invalidated"");"
"<line2>      log.warn(""There is no Asterisk configured"");<line8>        log.debug(""{}"", r);<line11>      log.error(""Error while executing ManagerAction: {}"", action, e);"
"<line19>      logger.debug(""An exception occurred while posting data : '{}'"", e.getMessage());<line21>      logger.debug(""The request '{}' yields '{}'"", requestData, responseData);<line26>          logger.debug(""A remote exception occurred: '{}'"", error.getAsString());<line36>          logger.debug(""A remote exception occurred: '{}':'{}':'{}'"", code, message, data);<line37>          logger.debug(""An unknown remote exception occurred: '{}'"", error.toString());"
"<line15>      logger.debug(""received this ConnectionMessage FromExternal:\n{}"",RdfUtils.toString(Prefixer.setPrefixes(wonMessage.getCompleteDataset())));<line16>        logger.debug(""This message contains the forwarded message(s) {}"",wonMessage.getForwardedMessageURIs());"
"<line5>        LOGGER.warn(""Operation ""+ operationModel.getName()+ "" from extension ""+ extensionModel.getName()+ "" threw exception while executing the onSuccess FlowListener"",e);<line12>        LOGGER.warn(""Operation ""+ operationModel.getName()+ "" from extension ""+ extensionModel.getName()+ "" threw exception while executing the onError FlowListener"",e);"
"<line14>        LOG.info(""Plugin \""{}\"" receives command [{}] with parameters '{''{'{}'}''}'"",new Object[] {this.getName(), command.getName(), command.getProperties()});<line34>      LOG.error(ex.getLocalizedMessage());"
"<line3>        LOG.trace(""Request {} is already done"", requestContext.hashCode());<line9>        LOG.trace(""Request {} reached timeout of {}, current calculation {}"",requestContext.hashCode(),requestContext.getTimeout(),calculateTimeout);<line16>          LOG.trace(""No target address for request {}, retry after {}."",requestContext.hashCode(),RETRY_DELAY);<line20>          LOG.trace(""No target address for request {}, will fail request."", requestContext.hashCode());<line29>      LOG.trace(""Send request {} to {} with topic {}"",requestContext.hashCode(),requestContext.getNodeAddress(),requestContext.getTopicName());"
"<line7>      LOG.trace(""failed to activate method: {}#{}"", className, methodName, e);"
"<line2>    logger.debug(""Activating admin group loader"");<line9>      logger.warn(""\n""+ ""######################################################\n""+ ""#                                                    #\n""+ ""# WARNING: Opencast still uses the default admin     #\n""+ ""#          credentials. Never do this in production. #\n""+ ""#                                                    #\n""+ ""#          To change the password, edit the key      #\n""+ ""#          org.opencastproject.security.admin.pass   #\n""+ ""#          in custom.properties.                     #\n""+ ""#                                                    #\n""+ ""######################################################"");"
"<line9>        logger.info(""Read-only property on {}: {}"",new Object[] {sourceNodeType, typeResource.getRawType()});<line51>          logger.warn(""Unable to execute {}.{}: {}"",entityType.getSimpleName(),methodName,t.getMessage());"
"<line7>          logger.debug(""Removing publish in progress marker from updated node, because update was not""+ "" successful. (node=""+ nodeRef.toString()+ "")"");"
"<line25>    LOG.info(""Caught script exception"", scriptException);"
"<line1>    logger.info(""Selected startup strategy {}"", startupStrategy);<line7>    logger.info(""Configured '{}' server state repository"", this.repository.getClass().getSimpleName());<line15>      logger.trace(""{} processing"", extension);<line22>          logger.info(""{} has been successfully registered as server extension"", extension);<line23>          logger.warn(""{} has not been registered as server extension"", extension);<line32>        logger.error(""Error when initializing server extension of type {}"", extension, e);"
"<line5>      LOGGER.warn(String.format(""Property boolean query max clause count doesn't exists, value is set to %d"",BooleanQuery.getMaxClauseCount()));<line6>      LOGGER.warn(String.format(""Property service is null, boolean query max clause count value is set to %d"",BooleanQuery.getMaxClauseCount()));"
"<line3>    LOGGER.info(""    Retrieving "" + url);"
"<line5>        log.debug(""There has not been found info of dependencies for module called "" + moduleName);<line12>      log.debug(""Module'""+ moduleName+ ""' had dependencies but it id not contain information of other modules"");"
<line27>      log.error(exception, exception);
"<line3>    log.debug(""Loading values..."");"
"<line8>        LOG.info(""Caught ObjectNotActive exception: ""+ ona+ "" during deactivate_object() call on POA: ""+ bindingPOA);"
"<line2>      ActiveMQRALogger.LOGGER.trace(""isEnable1xPrefixes()"");"
"<line11>      log.error(basicMsg + "" while processing Metric '"" + tmpl + ""'"", exc);<line12>      log.error(basicMsg + "": "" + msg);<line13>        log.error(""Stack trace follows:"", exc);"
"<line3>      log.info(""...Setting queryLogLevelInfo: "" + queryLogLevelInfo);"
"<line2>    log.info(""Starting an EBInMemoryRegistry on UUID {0}"", registryUuid);"
"<line6>    LOG.info(""Pushing changes: {}"", commitComment);"
"<line21>      logger.debug(""Node to bucketId map: {}"", ret);"
"<line28>      log.debug(""Validating attachment contentType: ""+ receivedAttachment.getContentType()+ ""='""+ controlAttachment.getContentType()+ ""': OK."");"
<line15>      log.error(systemException, systemException);
<line1>    Logger.trace(id, message, error);
"<line27>        logger.info(""Unable to update WebPush Variant '{}'"", webPushVariant.getVariantID());<line27>        logger.debug(""Details: {}"", cve);<line30>      logger.trace(""Updating WebPush Variant '{}'"", webPushID);"
"<line2>    log.info(context.getVariable(""index""));"
"<line2>    log.trace(""{} Polling for new data"");<line12>          log.trace(""Waiting {} ms to poll {} next"", nextUpdate - now, querier.toString());<line18>        log.debug(""Checking for next block of results from {}"", querier.toString());<line29>          log.trace(""No updates for {}"", querier.toString());<line31>            log.trace(""More than ""+ CONSECUTIVE_EMPTY_RESULTS_BEFORE_RETURN+ "" consecutive empty results for all queriers, returning"");<line38>        log.debug(""Returning {} records for {}"", results.size(), querier.toString());<line40>        log.error(""Failed to run query for table {}: {}"", querier.toString(), sqle);"
"<line2>    log.trace(MessageFormat.format(""Deleting script {0}"", scriptKey.toString()));"
<line7>      logger.debug(e.getMessage(), e);
"<line18>        LOG.warn(""Unable to parse response payload, continuing without conversion: "", e.getMessage());<line18>        LOG.debug(""Unable to parse `{}` to JSON"", body, e);"
<line16>      LOG.error(msg, e);
"<line44>      logger.debug(""Expected "" + expectAuthRefs + "" authority references, found "" + numAuthRefsFound);<line45>    logger.info(this.toString());<line49>        logger.debug(testName+ "": list-item[""+ i+ ""] Field:""+ item.getSourceField()+ "" =""+ "" item display name = ""+ item.getAuthDisplayName()+ "" auth display name = ""+ item.getItemDisplayName());<line49>        logger.debug(testName + "": list-item["" + i + ""] refName="" + item.getRefName());<line49>        logger.debug(testName + "": list-item["" + i + ""] URI="" + item.getUri());"
"<line7>      logger.error(""Exception thrown from crucial Journal Transport: '"" + transportName + ""'"", e);"
<line22>        log.debug(portalException, portalException);
<line14>        log.warn(sb.toString());<line15>        log.info(sb.toString());<line16>        log.error(sb.toString());
"<line1>    logger.debug(""initOwnCloudInfo started..."");<line6>      logger.warn(ex.getMessage());"
"<line12>            logger.info(""Unsupported encoding"", e);"
"<line2>    logger.debug(""Stopping USB-Serial background discovery"");<line6>        logger.debug(""Stopped USB-serial background discovery"");"
"<line37>      log.info(""Verifying that docker containers are reachable"");<line44>                log.info(""Probing: {}:{}"", DOCKER_HOST.address(), probePort);"
"<line9>    log.info(""Making a POST request to the {} endpoint with no connector started and no signature""+ "" header; expecting 400 error response"",connectorTasksEndpoint);<line12>    log.info(""Making a POST request to the {} endpoint with no connector started and an invalid""+ "" signature header; expecting 403 error response"",connectorTasksEndpoint);<line21>    log.info(""Starting the {} connector"", CONNECTOR_NAME);<line24>    log.info(""Making a POST request to the {} endpoint with the connector started and no signature""+ "" header; expecting 400 error response"",connectorTasksEndpoint);<line27>    log.info(""Making a POST request to the {} endpoint with the connector started and an invalid""+ "" signature header; expecting 403 error response"",connectorTasksEndpoint);"
"<line38>    logger.info(""Update fixed_date to {} on ci [{}:{}]"", fixedDate, ciTypeId, guid);"
"<line9>        LOGGER.error(""Failed {} processing {} consecutive times. Abort. Mail is saved in {}"",mail.getName(),failureCount,configuration.getErrorRepositoryURL().asString());<line11>        LOGGER.error(""Failed {} processing {} consecutive times. Mail is requeued with increased failure""+ "" count."",mail.getName(),failureCount,processingException);<line14>      LOGGER.error(""Could not apply standard error handling for {}, defaulting to nack"",mail.getName(),nestedE);"
"<line2>      LOGGER.info(""Creating VirtualBox port forward for "" + port);"
"<line5>      log.info(""Test zero duration on update, key: "" + key);"
"<line14>            logger.trace(""{} is WAN, adding its dns servers: {}"", netInterfaceConfig.getName(), servers);<line16>            logger.error(""Error adding dns servers for {}"", netInterfaceConfig.getName(), e);"
"<line3>      log.info(""setting up class"");"
"<line9>      LOGGER.error(""Could not download result: {}"", e.getMessage(), e);"
"<line3>        logger.debug(""Fail to update the ""+ _containerName+ "" container XML file for ""+ spaceName+ "" space; the DOM element for spaces is null"");"
"<line6>      logger.error(""BulkMigrationUser:encryptData:error occurred while encrypting data"", e);"
"<line2>    log.debug(""Connection established with sessionId: "" + session.getSessionId());"
"<line14>      log.error(""Unable to get blogs portlet instance configuration"", configurationException);"
"<line22>      LOG.debug(""SQL : "" + query.toString());<line22>      LOG.debug(""SQL.param.system : "" + system);<line22>      LOG.debug(""SQL.param.test : "" + test);<line22>      LOG.debug(""SQL.param.testcase : "" + testcase);<line71>          LOG.error(""Unable to execute query : "" + exception.toString());<line75>        LOG.error(""Unable to execute query : "" + exception.toString());<line79>      LOG.error(""Unable to execute query : "" + exception.toString());<line85>        LOG.warn(""Exception Closing the connection : "" + e.toString());"
"<line14>        logger.debug(""intersect contract m = "" + m);"
"<line7>      logger.error(""Error in Kafka iterator wrapper"", e);"
"<line3>    logger.debug(""Converting  response to "" + clazz);"
"<line13>      this.logger.warn(""Unexpected buffer underflow. Resetting and compacting buffer."", ex);"
"<line7>      log.fatal(""Unable to process audit message "" + auditMessage, exception);"
<line11>        log.debug(exception, exception);
"<line26>      log.warn(""Error while initializing cug importer"", e);"
<line13>    log.warn(sb.toString());
"<line4>      logger.debug("">> destroying virtualMachine(%s) job(%s)"", virtualMachineId, destroyVirtualMachine);<line6>      logger.trace(""<< virtualMachine(%s) not found"", virtualMachineId);"
<line5>      logger.error(e);
"<line3>    logger.debug(""createServiceDefinitionResponse started..."");"
"<line13>          logger.info(String.format(""Recalculated computed location for %d of %d cataloging records."",processed, recordsToProcess));<line18>              logger.trace(""Skipping soft-deleted CollectionObject record with CSID ""+ collectionObjectCsid);<line46>      logger.error(errMsg);<line50>    logger.info(""Updated computedCurrentLocation values in "" + numUpdated + "" CollectionObject record(s)."");"
"<line2>    LOGGER.info(""CredentialService: received kapua event from {}, operation {}"",kapuaEvent.getService(),kapuaEvent.getOperation());"
"<line6>      LOGGER.error(""error printing message"", ioe);"
<line12>        log.debug(sb.toString());
"<line3>    log.info(text);<line3>    log.info(repeat(""#"", 100));<line3>    log.info("""");"
"<line10>      log.debug(""Forwarding request to master {"" + master.toString() + ""}"");<line31>      log.warn(""Request to be proxied is already read"");<line35>              e -> log.error(""Could not forward request to Mesh: {}"", e, e.getMessage()))"
"<line7>          log.debug(""Found cloud offer with info:"" + offer.toString());<line10>        log.warn(""not found information for cloud offer name '"" + offerName + ""'"");"
"<line3>      log.debug(""Transforming frame using Word2Vec algorithm: {} "", model);<line18>      log.error(e.getMessage());"
"<line2>      LOG.info(""MiniHBaseCluster stopped"");"
"<line27>              LOG.error(""Failed to update permssions on user <{}>"", user.getName(), e);<line28>            LOG.info(""Migrating entity <{}> permissions <{}> to <{}> grant for user <{}>"",targetGRN,permissions,capability,user.getName());<line29>            LOG.info(""Skipping non-migratable entity <{}>. Permissions <{}> cannot be converted to a""+ "" grant capability"",entityID,permissions);"
<line21>    logger.info(sizeInfo);
"<line1>    Log.warn(""Autocreating jiveID row for type '"" + type + ""'"");"
"<line3>      log.info(""Optimization success status [{0}]"", success);<line4>      log.warn(""OptimizerRunner: The optimizer is null. Could not optimize table."");"
"<line36>      LOGGER.error(""Error while mapping to MysqlBinLogEvent . Exception : ""+ e.getMessage()+ "" Cause: ""+ e.getCause(),e);"
<line4>      log.error(exception, exception);
"<line32>        LOG.info(""preprocess: ignored entities={}; pruned entities={}. topic-offset={}, partition={}"",ignoredEntities,prunedEntities,context.getKafkaMessageOffset(),context.getKafkaPartition());"
"<line4>      logger.debug(""Endpoint missing. Component is disabled."");"
"<line2>    log.info(""Beginning to generate Stream Discovery lists for all users."");<line2>    log.info(""Regenerating weekday count temp data for ""+ numberOfDaysOfWeekdayCountDataToGenerate+ "" days"");<line4>    log.info(""Generating the list of featured streams"");<line5>    log.info(""Generating the list of most active streams"");<line6>    log.info(""Generating the list of most viewed streams."");<line7>    log.info(""Generating the list of most followed streams."");<line8>    log.info(""Generating the list of most recent streams."");<line9>    log.info(""Finished generating Stream Discovery lists for all users."");"
<line8>          log.debug(_NO_SUCH_ENTITY_WITH_PRIMARY_KEY + primaryKey);
<line21>      log.error(systemException, systemException);
<line18>      log.error(systemException, systemException);
<line11>    logger.debug(dbInfo.toString());
"<line4>    logger.warn(""Unable to write trace to disk"", e);"
"<line21>        LOG.warn(""Cannot create Heartbeat Message because node's identifier is not known at this time"");<line26>      LOG.debug(""Generated heartbeat"");<line28>      LOG.warn(""Failed to create heartbeat due to: "" + ex, ex);"
"<line1>    log.debug(""persisting TmpItvSel instance"");<line3>      log.debug(""persist successful"");<line4>      log.error(""persist failed"", re);"
"<line33>      logger.info(""File "" + fileName + "" created successfully"");<line34>      logger.error(""writeToFile: "" + e.getMessage(), e);<line39>          logger.error(""writeToFile: "" + e.getMessage(), e);"
"<line5>      LOG.info(""The request has a token with subject: "" + getToken().getSubject());<line10>      LOG.warn(""Client principal name is null."");<line26>      LOG.warn(""Can't get the client entry."");"
"<line5>      LOG.debug(""Exception as expected when testing sshable "" + machineConfig);"
"<line3>      LOGGER.trace(getClass().getSimpleName() + ""::read"");<line8>      LOGGER.error(e.getMessage(), e);"
"<line10>            LOG.debug(""[epPolicyTemplateProvider] harvestAll succeeded: {}"", amount);<line14>            LOG.debug(""[epPolicyTemplateProvider] harvestAll FAILED: {}"", t.getMessage());"
"<line4>      logger.debug(""Manufacturer matched: search: {}, device value: {}."", MANUFACTURER, manufacturer);<line6>        logger.debug(""Device type matched: search: {}, device value: {}."", UPNP_DEVICE_TYPE, type);"
"<line7>    LOGGER.info(""Cluster: "" + getName() + "" locationType: "" + locationType + "" value:"" + value);"
"<line2>      logger.trace(""Liste Filme lesen von: {}"", source);<line10>      logger.warn(ex);"
"<line1>    logger.trace(""Setting occupancy groups list"");"
"<line6>        logger.debug(""Failure handling failure message for Event ""+ event+ "" (""+ eventType+ "") and Request ""+ request,e);"
"<line9>      logger.warn(""Target tag doesn't exist"");"
<line9>      LOGGER.error(e);
"<line5>      LOGGER.info(""Cannot retrieve space: "" + id);"
<line10>        LOGGER.error(e);
"<line17>            LOG.debug(""Creating new category anchor, category = {}, glossary = {}"",storeObject.getGuid(),updatedCategory.getAnchor().getGlossaryGuid());<line29>            LOG.debug(""Updating category anchor, currAnchor = {}, newAnchor = {} and category = {}"",storeObject.getAnchor().getGlossaryGuid(),updatedCategory.getAnchor().getGlossaryGuid(),storeObject.getGuid());<line35>            LOG.debug(""Derived qualifiedName = {}"", storeObject.getQualifiedName());<line46>            LOG.debug(""Deleting category anchor"");"
"<line6>      logger.info(""total: count: {}; time: {}; average: {}; period: count: {}; time: {}; average: {}"",totalCount,endTime - totalBeginTime,totalCount * 1000 / (endTime - totalBeginTime),tupleCount,endTime - beginTime,tupleCount * 1000 / (endTime - beginTime));"
"<line9>        logger.error(""count down is null, returning generic countdown vo!"");"
"<line4>        log.info(""Key is locked [key="" + key + "", node="" + ignite.name() + ']');"
<line9>      LOG.error(ex.toString(), ex);
"<line5>        LOGGER.warn(""Unable to check partial result of scanner context"", e);"
"<line7>    LOGGER.info(""DayProfileTablePassive to set is: {}"", this.dlmsHelper.getDebugInfo(dayArray));"
"<line13>      LOG.debug(""Inventory node connector key is null. Data can't be written to topology termination""+ "" point"");"
"<line18>        log.warn(""Can't handle a "" + nci.getClass() + ""; must be a NormalizedFieldAndValue."");<line26>          log.debug(""Not creating index mutations for ""+ term+ "" as we've already created mutations for it."");"
"<line9>      logger.error(""**** Error in UserTokenDAO:"", e);"
<line48>          log.warn(exception, exception);
"<line35>                  LOG.error(""Received unexpected message:"" + incoming);<line38>                LOG.error(""Exception in listener"", e);"
"<line15>                            logger.warn(""Can not find request controller to handle request {} with""+ "" pkRangeId {}"",request.getActivityId(),this.getResolvedPartitionKeyRangeId(request));"
"<line44>            log.info(String.format(""Existing Export params for FileShare id: %1$s,  SecurityType: %2$s, ""+ ""Permissions: %3$s, Root user mapping: %4$s, "",id,fileExport.getSecurityType(),fileExport.getPermissions(),fileExport.getRootUserMapping()));<line44>            log.info(String.format(""Recieved Export params for FileShare id: %1$s,  SecurityType: %2$s, ""+ ""Permissions: %3$s, Root user mapping: %4$s, "",id, securityType, permissions, rootUserMapping));"
"<line12>      logger.error(""Error while updating isMicromapperEnabled flag for crisis: "" + code, e);"
"<line8>        logger.error(""Exception initializing JdbcAsyncWriter"", ex);"
"<line3>      getLog().info(""\nSkipping create goal.\n"");<line8>      log.info(MessageFormat.format(messages.getString(""info.install.type.preexisting""), """"));<line18>      log.info(MessageFormat.format(messages.getString(""info.server.start.create""), serverName));<line23>      log.info(MessageFormat.format(messages.getString(""info.server.create.created""),serverName,serverDirectory.getCanonicalPath()));"
"<line6>      logger.warn(""{}: No preset found at id: {}"", handler.getDeviceName(), command.intValue());"
"<line2>    Log.debug(""Test"");"
"<line4>    logger.debug(""setFirewallPortForwardingConfiguration() :: Deleting port forward rules"");<line8>      logger.debug(""setFirewallPortForwardingConfiguration() :: Adding port forward rule for: {}"",portForwardEntry.getInPort());<line13>          logger.info(e.getMessage(), e);"
"<line2>    LOGGER.info(""Member removed: "" + membershipEvent.getMember());<line2>    LOGGER.info(""list of members now :"" + membershipEvent.getMembers());"
"<line6>          LOG.info(""Dropping bootstrap index. Deleting file : "" + indexPath);"
"<line12>              log.debug(""Requested QR not found. {}"", token);<line17>            log.error(""Error cloning project."", e);"
"<line14>        LOGGER.debug(""Azure tables not supported with Active directory authentication"");<line17>      LOGGER.error(e.getLocalizedMessage());"
"<line6>    LOG.info(""Property (set to default) {} = {}"", keyValueDefault.getKey(), keyValueDefault.getValue());"
"<line3>      LOG.debug(""["" + id_ + ""] setGlobalAlpha("" + globalAlpha + "")"");"
"<line9>      this.logger.error(""Error loading server handler jar"", e);"
"<line6>      LOG.error(""While checking threads"", t);<line7>      LOG.trace(""Finished checking threads after {}"", JavaUtils.duration(start));"
"<line13>    LOGGER.info(""add rows in %.3fs"", (endTime - startTime) / 1000);"
"<line10>      log.error(""Unable to authenticate client against Kerberos"", e);"
"<line14>    LOG.warn(policy.toString() + "" is not supported by ByChar comparison"");"
"<line6>          log.debug(""Attempting to read ConfigMap "" + configMapName);<line23>            log.debug(""Done reading ConfigMap "" + configMap.getMetadata().getName());"
"<line14>            LOG.debug(""Found event body: {}"", body);<line23>        LOG.error(""Never found event body: {}"", body);"
"<line1>    logger.trace(""Creating SQL indices"");<line11>      logger.error(ex);<line12>    logger.trace(""Finished creating SQL indices"");"
"<line6>      LOG.error(""Failed to parse IP address {}"", e);"
"<line8>          BPELRESTLightElement.LOG.error(""Internal Error: Stored URI was invalid"", e);"
"<line14>        LOGGER.debug(""Got response from operator 0 after: {}"", (System.currentTimeMillis() - startTime));<line21>            LOGGER.debug(""Got response from operator {} after: {}"",mergedBlocksNumber,(System.currentTimeMillis() - startTime));<line22>            LOGGER.debug(""Merged response from operator {} after: {}"",mergedBlocksNumber,(System.currentTimeMillis() - startTime));"
"<line6>      logger.debug(""Can not handle timeout event"", e);"
"<line13>      logger.debug(""Error detecting the EZSP frame type"", e);<line30>      logger.debug(""Error creating instance of EzspFrame"", e);"
"<line10>                LOGGER.debug(""Failed getting {} resource bundle, using default \""{}\"""",INTRIGUE_BASE_NAME,DEFAULT_MESSAGE_FORMAT);"
"<line14>      logger.debug(""invalid syntax"", e);"
"<line5>        logger.debug(""MembershipViewReplyProcessor received {}"", view);"
"<line4>        LOG.debug(""Setting custom LocaleResolver of type "" + localeResolver.getClass().getName());"
"<line1>    log.info(""controller.policyFlags()"");"
"<line23>      logger.error(""Error while loading username for control uuid: "" + uuid, t);"
"<line12>    LOG.info(""Starting {} Program '{}' in workflow"", prettyProgramType, programName);<line13>    LOG.info(""{} Program '{}' in workflow completed"", prettyProgramType, programName);"
"<line7>      log.debug(""finished"");"
"<line8>      LOG.debug(""Obtaining active resource claims, will NOT consider {} because its parent is not the""+ "" current section"",file);"
"<line3>      LOGGER.error(""Unable to delete the following file: "" + filename);"
